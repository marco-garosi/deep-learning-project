{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class RefCocoG_Dataset(Dataset):\n",
    "    full_annotations = None\n",
    "\n",
    "    def __init__(self, root_dir, annotations_f, instances_f, split='train', transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations_f = annotations_f\n",
    "        self.instances_f = instances_f\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.get_annotations()\n",
    "        self.image_names = list([\n",
    "            self.annotations[id]['image']['actual_file_name']\n",
    "            for id in self.annotations\n",
    "        ])\n",
    "\n",
    "    def get_annotations(self):\n",
    "        if RefCocoG_Dataset.full_annotations:\n",
    "            self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, RefCocoG_Dataset.full_annotations.items()))\n",
    "            return\n",
    "\n",
    "        # Load pickle data\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.annotations_f), 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "\n",
    "        # Load instances\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.instances_f), 'rb') as file:\n",
    "            self.instances = json.load(file)\n",
    "\n",
    "        # Match data between the two files and build the actual dataset\n",
    "        self.annotations = {}\n",
    "\n",
    "        images_actual_file_names = {}\n",
    "        for image in self.instances['images']:\n",
    "            images_actual_file_names[image['id']] = image['file_name']\n",
    "\n",
    "        for image in self.data:\n",
    "            if image['ann_id'] not in self.annotations:\n",
    "                self.annotations[image['ann_id']] = {}\n",
    "\n",
    "            self.annotations[image['ann_id']]['image'] = image\n",
    "            self.annotations[image['ann_id']]['image']['actual_file_name'] = images_actual_file_names[image['image_id']]\n",
    "\n",
    "        for annotation in self.instances['annotations']:\n",
    "            if annotation['id'] not in self.annotations:\n",
    "                continue\n",
    "\n",
    "            self.annotations[annotation['id']]['annotation'] = annotation\n",
    "\n",
    "        # Keep only samples from the given split\n",
    "        RefCocoG_Dataset.full_annotations = self.annotations\n",
    "        self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, self.annotations.items()))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def corner_size_to_corners(self, bounding_box):\n",
    "        \"\"\"\n",
    "        Transform (top_left_x, top_left_y, width, height) bounding box representation\n",
    "        into (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            bounding_box[0],\n",
    "            bounding_box[1],\n",
    "            bounding_box[0] + bounding_box[2],\n",
    "            bounding_box[1] + bounding_box[3]\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image name at the given index\n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # Load the image file as a PIL image\n",
    "        image = Image.open(os.path.join(self.root_dir, 'images', image_name)).convert('RGB')\n",
    "        # image = read_image(os.path.join(self.root_dir, 'images', image_name), ImageReadMode.RGB)\n",
    "        \n",
    "        image_id = list(self.annotations)[idx]\n",
    "\n",
    "        # print(image_id)\n",
    "\n",
    "        # Get the caption for the image\n",
    "        prompts = [\n",
    "            prompt['sent'] for prompt in self.annotations[image_id]['image']['sentences']\n",
    "        ]\n",
    "\n",
    "        # Get the bounding box for the prompts for the image\n",
    "        bounding_box = self.corner_size_to_corners(self.annotations[image_id]['annotation']['bbox'])\n",
    "\n",
    "        # Apply the transform if given\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = [\n",
    "            image,\n",
    "            bounding_box,\n",
    "            prompts,\n",
    "        ]\n",
    "\n",
    "        # Return the sample as a list\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='train')\n",
    "dataset_val = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='val')\n",
    "dataset_test = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='test')\n",
    "\n",
    "dataset_splits = [\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RefCocoG_Dataset.full_annotations), len(dataset_train.annotations), len(dataset_val.annotations), len(dataset_test.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_differently_sized_prompts(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    bboxes = [item[1] for item in batch]\n",
    "    prompts = [item[2] for item in batch]\n",
    "    \n",
    "    return list(images), list(bboxes), list(prompts)\n",
    "\n",
    "def get_data(dataset_splits, batch_size=64, test_batch_size=256, num_workers=0):\n",
    "    training_data = dataset_splits[0]\n",
    "    validation_data = dataset_splits[1]\n",
    "    test_data = dataset_splits[2]\n",
    "\n",
    "    # Change shuffle to True for train\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data(dataset_splits, batch_size=128, test_batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # First GPU\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "else:\n",
    "    yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "models, preprocesses = [], []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        model, preprocess = clip.load(\"RN50x16\", device=f'cuda:{i}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preprocesses.append(preprocess)\n",
    "else:\n",
    "    model, preprocess = clip.load(\"RN50x16\", device=device)\n",
    "    models.append(model)\n",
    "    preprocesses.append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(models[0].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Cosine Similarity\n",
    "\n",
    "    Normalizes both tensors a and b. Returns <b, a.T> (inner product).\n",
    "    \"\"\"\n",
    "\n",
    "    a_norm = a / a.norm(dim=-1, keepdim=True)\n",
    "    b_norm = b / b.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (b_norm @ a_norm.T)\n",
    "\n",
    "    return similarity.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualise_scores(scores: torch.Tensor, images, texts: list[str]):\n",
    "    for t_idx, text in enumerate(texts):\n",
    "        for i_idx, image in enumerate(images):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f'Score: {scores[t_idx, i_idx]} / Prompt: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, device=None, models=None, preprocesses=None, yolo_models=None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        if not models or not preprocesses:\n",
    "            raise ValueError('Models and preprocesses for CLIP model should be provided')\n",
    "\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "        \n",
    "        if not yolo_models:\n",
    "            raise ValueError('Models for YOLO should be provided')\n",
    "        self.yolo_models = yolo_models\n",
    "\n",
    "        self.transform_to_tensor = T.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def forward(self, indices, images, prompts_list):\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        # -- Getting the right data and moving it to the correct device --\n",
    "\n",
    "        # Images remain on the CPU because they are PIL Images, not Tensors\n",
    "        # Converting to Tensors leads to errors with YOLO\n",
    "        images = [images[i] for i in indices]\n",
    "\n",
    "        prompts_list = [prompts_list[i] for i in indices]\n",
    "        prompts_tensor = [clip.tokenize(prompt_list).to(self.device) for prompt_list in prompts_list]\n",
    "\n",
    "        # -- Actual processing --\n",
    "\n",
    "        bounding_boxes = self.get_bounding_boxes(images)\n",
    "\n",
    "        # It contains the predicted bounding box for each image for each prompt\n",
    "        # Then, it is a list of length len(images) and for each entry there is a\n",
    "        # list with len(prompts[i]), where i is the i-th image \n",
    "        overall_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, prompts_tensor_for_sample in enumerate(prompts_tensor):\n",
    "                # Image crops\n",
    "                image_crops = self.get_cropped_bounding_boxes(images[idx], bounding_boxes.xyxy[idx])\n",
    "\n",
    "                preprocessed_image_crops = torch.stack([self.preprocesses[self.device_index](image).to(self.device) for image in image_crops])\n",
    "\n",
    "                crop_features = self.models[self.device_index].encode_image(preprocessed_image_crops)\n",
    "                crop_features /= crop_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                # Scaling is not required as cosine_similarity already scales.\n",
    "                # This is to avoid redundant computations and speed up runtime\n",
    "                text_features = self.models[self.device_index].encode_text(prompts_tensor_for_sample)\n",
    "\n",
    "                similarity = cosine_similarity(crop_features, text_features).float()\n",
    "                texts_p = (100 * similarity).softmax(dim=-1)\n",
    "\n",
    "                # To return the cosine similarity between the best crops and the prompts\n",
    "                # max_cos_sim_values, _ = similarity.max(dim=-1)\n",
    "                # for max_value in max_cos_sim_values:\n",
    "                #     overall_outputs.append(max_value.to(self.device))\n",
    "                # continue\n",
    "\n",
    "                _, max_indices = texts_p.max(dim=1)\n",
    "                try:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor(bounding_boxes.xyxy[idx][max_idx, 0:4]).to(self.device)\n",
    "                        )\n",
    "                except:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor((0, 0, 0, 0)).to(self.device)\n",
    "                        )\n",
    "\n",
    "        return torch.stack(overall_outputs)\n",
    "\n",
    "    def get_prompts(self, sample):\n",
    "        return [prompt['sent'] for prompt in sample['image']['sentences']]\n",
    "\n",
    "    def get_bounding_boxes(self, pil_images):\n",
    "        bounding_boxes = self.yolo_models[self.device_index](pil_images)\n",
    "        return bounding_boxes\n",
    "    \n",
    "    def get_cropped_bounding_boxes(self, image, bounding_boxes):\n",
    "        cropped_bounding_boxes = []\n",
    "        \n",
    "        for bounding_box in bounding_boxes:\n",
    "            cropped_img = image.crop((bounding_box[0].item(), bounding_box[1].item(), bounding_box[2].item(), bounding_box[3].item()))\n",
    "            cropped_bounding_boxes.append(cropped_img)\n",
    "\n",
    "        if len(cropped_bounding_boxes) == 0:\n",
    "            cropped_bounding_boxes.append(image)\n",
    "                \n",
    "        return cropped_bounding_boxes\n",
    "\n",
    "baseline_model = BaselineModel(models=models, preprocesses=preprocesses, yolo_models=yolo_models)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    baseline_model = torch.nn.DataParallel(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def iou_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Localization Accuracy Metric\n",
    "\n",
    "    Intersection over Union (IoU) is a common metric measure for localization accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).unsqueeze(0).to(device)\n",
    "\n",
    "    return box_iou(bounding_boxes, ground_truth_bounding_boxes)\n",
    "\n",
    "def cosine_similarity_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Cosine Similarity Metric\n",
    "\n",
    "    Cosine similarity is a common metric measure for semantic similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).to(device)\n",
    "    \n",
    "    return cosine_similarity(bounding_boxes, ground_truth_bounding_boxes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute average cosine similarity between embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_outputs = []\n",
    "\n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(test_loader):\n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    prompts_tensor = [clip.tokenize(prompt_list) for prompt_list in prompts]\n",
    "    \n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    outputs = baseline_model(indices, images, prompts)\n",
    "\n",
    "    overall_outputs.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_cpu = []\n",
    "for out in overall_outputs:\n",
    "    for cos_sim_val in out:\n",
    "        cos_sim_cpu.append(cos_sim_val.item())\n",
    "cos_sim_cpu = np.array(cos_sim_cpu)\n",
    "np.nanmean(cos_sim_cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import boxes as box_ops\n",
    "\n",
    "IoUs = []\n",
    "cosine_similarities = []\n",
    "  \n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(test_loader):\n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    prompts_tensor = [clip.tokenize(prompt_list) for prompt_list in prompts]\n",
    "    \n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    outputs = baseline_model(indices, images, prompts)\n",
    "\n",
    "    outputs_grouped_by_sample = []\n",
    "    outputs_idx = 0\n",
    "    prompts_idx = 0\n",
    "    while True:\n",
    "        if not prompts_idx < len(images):\n",
    "            break\n",
    "\n",
    "        outputs_grouped_by_sample.append(\n",
    "            outputs[outputs_idx : outputs_idx + len(prompts[prompts_idx])]\n",
    "        )\n",
    "\n",
    "        outputs_idx += len(prompts[prompts_idx])\n",
    "        prompts_idx += 1\n",
    "\n",
    "    for output_bboxes, gt_bboxes in zip(outputs_grouped_by_sample, gt_bounding_boxes):\n",
    "        \"\"\"\n",
    "        There is one output bounding box for each prompt given in input.\n",
    "        Note that each prompt for a given input is actually a list of prompts,\n",
    "        therefore it can contain an arbitrary number of promps. Hence, there is\n",
    "        a bounding box for each one of them.\n",
    "        \"\"\"\n",
    "\n",
    "        result_ious = iou_metric(output_bboxes, gt_bboxes)\n",
    "        result_cosine_similarity = cosine_similarity_metric(output_bboxes, gt_bboxes)\n",
    "\n",
    "        for iou in result_ious:\n",
    "            IoUs.append(iou)\n",
    "\n",
    "        for cs in result_cosine_similarity:\n",
    "            cosine_similarities.append(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoUs_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in IoUs])\n",
    "mIoU = np.nanmean(IoUs_to_cpu)\n",
    "\n",
    "cosine_similarities_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in cosine_similarities])\n",
    "m_cos_sim = np.nanmean(cosine_similarities_to_cpu)\n",
    "\n",
    "print('--- Metrics ---')\n",
    "print(f'Mean Intersection over Union (mIoU): {mIoU}')\n",
    "print(f'Mean Cosine Similarity: {m_cos_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('outcomes/iou_baseline_with_RN50x16.csv', IoUs_to_cpu, delimiter=',')\n",
    "np.savetxt('outcomes/cossim_baseline_with_RN50x16.csv', cosine_similarities_to_cpu, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "counter_threshold = 0\n",
    "for iou in IoUs:\n",
    "    if iou == 0:\n",
    "        counter += 1\n",
    "    # if iou < 0.5:\n",
    "    #     counter_threshold += 1\n",
    "    if iou >= 0.5:\n",
    "        counter_threshold += 1\n",
    "counter, counter_threshold, len(IoUs), counter_threshold / len(IoUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "output_idx = 0\n",
    "\n",
    "# Loading the image\n",
    "img = images[output_idx]\n",
    "\n",
    "#Â Preparing the output\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "# Create a Rectangle patch\n",
    "for bbox, color in zip(outputs_grouped_by_sample[output_idx][1:2], colors):\n",
    "    bounding_box_coordinates = bbox.cpu()\n",
    "    top_left_x, top_left_y = bounding_box_coordinates[0], bounding_box_coordinates[1]\n",
    "    width, height = bounding_box_coordinates[2]- top_left_x, bounding_box_coordinates[3] - top_left_y\n",
    "\n",
    "    # Parameters: (x, y), width, height\n",
    "    rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=1, edgecolor=color, facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.set_title(prompts[output_idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
