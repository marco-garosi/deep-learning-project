{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class RefCocoG_Dataset(Dataset):\n",
    "    full_annotations = None\n",
    "\n",
    "    def __init__(self, root_dir, annotations_f, instances_f, split='train', transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations_f = annotations_f\n",
    "        self.instances_f = instances_f\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if self.transform is None:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(size=224, interpolation=T.InterpolationMode.BICUBIC, max_size=None, antialias='warn'),\n",
    "                T.CenterCrop(size=(224, 224))\n",
    "            ])\n",
    "\n",
    "        self.get_annotations()\n",
    "        self.image_names = list([\n",
    "            self.annotations[id]['image']['actual_file_name']\n",
    "            for id in self.annotations\n",
    "        ])\n",
    "\n",
    "    def get_annotations(self):\n",
    "        if RefCocoG_Dataset.full_annotations:\n",
    "            self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, RefCocoG_Dataset.full_annotations.items()))\n",
    "            return\n",
    "\n",
    "        # Load pickle data\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.annotations_f), 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "\n",
    "        # Load instances\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.instances_f), 'rb') as file:\n",
    "            self.instances = json.load(file)\n",
    "\n",
    "        # Match data between the two files and build the actual dataset\n",
    "        self.annotations = {}\n",
    "\n",
    "        images_actual_file_names = {}\n",
    "        for image in self.instances['images']:\n",
    "            images_actual_file_names[image['id']] = image['file_name']\n",
    "\n",
    "        for image in self.data:\n",
    "            if image['ann_id'] not in self.annotations:\n",
    "                self.annotations[image['ann_id']] = {}\n",
    "\n",
    "            self.annotations[image['ann_id']]['image'] = image\n",
    "            self.annotations[image['ann_id']]['image']['actual_file_name'] = images_actual_file_names[image['image_id']]\n",
    "\n",
    "        for annotation in self.instances['annotations']:\n",
    "            if annotation['id'] not in self.annotations:\n",
    "                continue\n",
    "\n",
    "            self.annotations[annotation['id']]['annotation'] = annotation\n",
    "\n",
    "        # Keep only samples from the given split\n",
    "        RefCocoG_Dataset.full_annotations = self.annotations\n",
    "        self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, self.annotations.items()))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def corner_size_to_corners(self, bounding_box):\n",
    "        \"\"\"\n",
    "        Transform (top_left_x, top_left_y, width, height) bounding box representation\n",
    "        into (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            bounding_box[0],\n",
    "            bounding_box[1],\n",
    "            bounding_box[0] + bounding_box[2],\n",
    "            bounding_box[1] + bounding_box[3]\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image name at the given index\n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # Load the image file as a PIL image\n",
    "        # image = Image.open(os.path.join(self.root_dir, 'images', image_name))\n",
    "        image = read_image(os.path.join(self.root_dir, 'images', image_name), ImageReadMode.RGB)\n",
    "        \n",
    "        image_id = list(self.annotations)[idx]\n",
    "\n",
    "        # print(image_id)\n",
    "\n",
    "        # Get the caption for the image\n",
    "        prompts = [\n",
    "            prompt['sent'] for prompt in self.annotations[image_id]['image']['sentences']\n",
    "        ]\n",
    "\n",
    "        # Get the bounding box for the prompts for the image\n",
    "        bounding_box = self.corner_size_to_corners(self.annotations[image_id]['annotation']['bbox'])\n",
    "\n",
    "        # Apply the transform if given\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = [\n",
    "            image,\n",
    "            bounding_box,\n",
    "            prompts,\n",
    "        ]\n",
    "\n",
    "        # Return the sample as a list\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='train')\n",
    "dataset_val = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='val')\n",
    "dataset_test = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='test')\n",
    "\n",
    "dataset_splits = [\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49820, 42224, 2573, 5023)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RefCocoG_Dataset.full_annotations), len(dataset_train.annotations), len(dataset_val.annotations), len(dataset_test.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_differently_sized_prompts(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    bboxes = [item[1] for item in batch]\n",
    "    prompts = [item[2] for item in batch]\n",
    "\n",
    "    return torch.stack(images, dim=0), list(bboxes), list(prompts)\n",
    "\n",
    "def get_data(dataset_splits, batch_size=64, test_batch_size=256):\n",
    "    training_data = dataset_splits[0]\n",
    "    validation_data = dataset_splits[1]\n",
    "    test_data = dataset_splits[2]\n",
    "\n",
    "    # Change shuffle to True for train\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True, collate_fn=collate_differently_sized_prompts, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data(dataset_splits, batch_size=128, test_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 0 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 1 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 2 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 3 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 4 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 5 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 6 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 7 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 8 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 9 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 10 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 11 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 12 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 13 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 14 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 15 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 16 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 17 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 18 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 19 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 20 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 21 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 22 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 23 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 24 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 25 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 26 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 27 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 28 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 29 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 30 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 31 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 32 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 33 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 34 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 35 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 36 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 37 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 38 --\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "128\n",
      "-- Batch index: 39 --\n",
      "torch.Size([31, 3, 224, 224])\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (image, bounding_box, prompts) in enumerate(test_loader):\n",
    "    \n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    print(image.shape)\n",
    "    print(len(bounding_box))\n",
    "    print(len(prompts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
