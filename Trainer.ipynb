{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class RefCocoG_Dataset(Dataset):\n",
    "    full_annotations = None\n",
    "\n",
    "    def __init__(self, root_dir, annotations_f, instances_f, split='train', transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations_f = annotations_f\n",
    "        self.instances_f = instances_f\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.get_annotations()\n",
    "        self.image_names = list([\n",
    "            self.annotations[id]['image']['actual_file_name']\n",
    "            for id in self.annotations\n",
    "        ])\n",
    "\n",
    "    def get_annotations(self):\n",
    "        if RefCocoG_Dataset.full_annotations:\n",
    "            self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, RefCocoG_Dataset.full_annotations.items()))\n",
    "            return\n",
    "\n",
    "        # Load pickle data\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.annotations_f), 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "\n",
    "        # Load instances\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.instances_f), 'rb') as file:\n",
    "            self.instances = json.load(file)\n",
    "\n",
    "        # Match data between the two files and build the actual dataset\n",
    "        self.annotations = {}\n",
    "\n",
    "        images_actual_file_names = {}\n",
    "        for image in self.instances['images']:\n",
    "            images_actual_file_names[image['id']] = image['file_name']\n",
    "\n",
    "        for image in self.data:\n",
    "            if image['ann_id'] not in self.annotations:\n",
    "                self.annotations[image['ann_id']] = {}\n",
    "\n",
    "            self.annotations[image['ann_id']]['image'] = image\n",
    "            self.annotations[image['ann_id']]['image']['actual_file_name'] = images_actual_file_names[image['image_id']]\n",
    "\n",
    "        for annotation in self.instances['annotations']:\n",
    "            if annotation['id'] not in self.annotations:\n",
    "                continue\n",
    "\n",
    "            self.annotations[annotation['id']]['annotation'] = annotation\n",
    "\n",
    "        # Keep only samples from the given split\n",
    "        RefCocoG_Dataset.full_annotations = self.annotations\n",
    "        self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, self.annotations.items()))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def corner_size_to_corners(self, bounding_box):\n",
    "        \"\"\"\n",
    "        Transform (top_left_x, top_left_y, width, height) bounding box representation\n",
    "        into (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            bounding_box[0],\n",
    "            bounding_box[1],\n",
    "            bounding_box[0] + bounding_box[2],\n",
    "            bounding_box[1] + bounding_box[3]\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image name at the given index\n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # Load the image file as a PIL image\n",
    "        image = Image.open(os.path.join(self.root_dir, 'images', image_name)).convert('RGB')\n",
    "        # image = read_image(os.path.join(self.root_dir, 'images', image_name), ImageReadMode.RGB)\n",
    "        \n",
    "        image_id = list(self.annotations)[idx]\n",
    "\n",
    "        # print(image_id)\n",
    "\n",
    "        # Get the caption for the image\n",
    "        prompts = [\n",
    "            prompt['sent'] for prompt in self.annotations[image_id]['image']['sentences']\n",
    "        ]\n",
    "\n",
    "        # Get the bounding box for the prompts for the image\n",
    "        bounding_box = self.corner_size_to_corners(self.annotations[image_id]['annotation']['bbox'])\n",
    "\n",
    "        # Apply the transform if given\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = [\n",
    "            image,\n",
    "            bounding_box,\n",
    "            prompts,\n",
    "        ]\n",
    "\n",
    "        # Return the sample as a list\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='train')\n",
    "dataset_val = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='val')\n",
    "dataset_test = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='test')\n",
    "\n",
    "dataset_splits = [\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the man in yellow coat', 'skiier in red pants'],\n",
       " ['there is red colored truck in between the other trucks',\n",
       "  'a shiny red vintage pickup truck'],\n",
       " ['a apple desktop computer',\n",
       "  'the white imac computer that is also turned on'],\n",
       " ['a girl wearing glasses and a pink shirt',\n",
       "  'an asian girl with a pink shirt eating at the table']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset_test[i][2] for i in [0, 1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49820, 42224, 2573, 5023)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RefCocoG_Dataset.full_annotations), len(dataset_train.annotations), len(dataset_val.annotations), len(dataset_test.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_differently_sized_prompts(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    bboxes = [item[1] for item in batch]\n",
    "    prompts = [item[2] for item in batch]\n",
    "    \n",
    "    return list(images), list(bboxes), list(prompts)\n",
    "\n",
    "def get_data(dataset_splits, batch_size=64, test_batch_size=256, num_workers=0):\n",
    "    training_data = dataset_splits[0]\n",
    "    validation_data = dataset_splits[1]\n",
    "    test_data = dataset_splits[2]\n",
    "\n",
    "    # Change shuffle to True for train\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data(dataset_splits, batch_size=128, test_batch_size=256, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # First GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "# else:\n",
    "#     yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "models, preprocesses = [], []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        model, preprocess = clip.load(\"ViT-B/32\", device=f'cuda:{i}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preprocesses.append(preprocess)\n",
    "else:\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    models.append(model)\n",
    "    preprocesses.append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(models[0].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Cosine Similarity\n",
    "\n",
    "    Normalizes both tensors a and b. Returns <b, a.T> (inner product).\n",
    "    \"\"\"\n",
    "\n",
    "    a_norm = a / a.norm(dim=-1, keepdim=True)\n",
    "    b_norm = b / b.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (b_norm @ a_norm.T)\n",
    "\n",
    "    return similarity.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualise_scores(scores: torch.Tensor, images, texts: list[str]):\n",
    "    for t_idx, text in enumerate(texts):\n",
    "        for i_idx, image in enumerate(images):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f'Score: {scores[t_idx, i_idx]} / Prompt: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_out_features = 512\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * self.clip_out_features, 4 * self.clip_out_features)\n",
    "        self.fc2 = nn.Linear(4 * self.clip_out_features, 6 * self.clip_out_features)\n",
    "        self.fc3 = nn.Linear(6 * self.clip_out_features, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiNetModel(nn.Module):\n",
    "    def __init__(self, models, preprocesses):# model=None, preprocess=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # if model is None or preprocess is None:\n",
    "        #     self.model, self.preprocess = clip.load(\"ViT-B/32\")\n",
    "        # else:\n",
    "        #     self.model, self.preprocess = model, preprocess\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "\n",
    "        self.clip_out_features = 512\n",
    "\n",
    "        # self.regressor = nn.Sequential(\n",
    "        #     nn.Linear(2 * self.clip_out_features, 4 * self.clip_out_features),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(4 * self.clip_out_features, 6 * self.clip_out_features),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(6 * self.clip_out_features, 4),\n",
    "        # ).to(device)\n",
    "        self.regressor = RegressorModel().half().to(device)\n",
    "\n",
    "    def forward(self, indices: torch.Tensor, images: torch.Tensor, prompts) -> torch.Tensor:\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        model, preprocess = self.models[self.device_index], self.preprocesses[self.device_index]\n",
    "\n",
    "        images = [images[i] for i in indices]\n",
    "        prompts = [prompts[i] for i in indices]\n",
    "\n",
    "        preprocessed_images = torch.stack([\n",
    "            preprocess(image) for image in images\n",
    "        ]).to(self.device)\n",
    "        preprocessed_prompts = torch.cat([\n",
    "            tokenized for tokenized in\n",
    "            [clip.tokenize(prompt_list) for prompt_list in prompts]\n",
    "        ]).to(self.device)\n",
    "\n",
    "        # Storing the index for each prompt so as to easily retreive its encoding\n",
    "        prompts_indices_for_image = []\n",
    "        start_index = 0\n",
    "        for prompt_list in prompts:\n",
    "            prompts_indices_for_image.append(\n",
    "                torch.tensor(list(range(0, len(prompt_list)))).to(self.device) + start_index\n",
    "            )\n",
    "            start_index += len(prompt_list)\n",
    "\n",
    "        # tot = 0\n",
    "        # for prompt_list in prompts:\n",
    "        #     tot += len(prompt_list)\n",
    "        # # print(f'there are {tot} prompts, {preprocessed_prompts.shape}')\n",
    "        # # print(f'{prompts_indices_for_image[-1][-1] + 1} prompts on {prompts_indices_for_image[-1].device}. Given {tot} prompts')\n",
    "        # return None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images_features = model.encode_image(preprocessed_images)\n",
    "            # print(f'{images_features.shape}, {images_features.device}')\n",
    "            texts_features = model.encode_text(preprocessed_prompts)\n",
    "            # print(f'{texts_features.shape}, {texts_features.device}')\n",
    "\n",
    "            # texts_features = []\n",
    "            # for idx, preprocessed_prompts_for_sample in enumerate(preprocessed_prompts):\n",
    "            #     text_features = model.encode_text(preprocessed_prompts_for_sample)\n",
    "            #     texts_features.append(text_features)\n",
    "\n",
    "        text_features_by_image = [\n",
    "            texts_features[indices]\n",
    "            for indices in prompts_indices_for_image\n",
    "        ]\n",
    "\n",
    "        bboxes = []\n",
    "        for image_features, text_features_for_image in zip(images_features, text_features_by_image):\n",
    "            # print(f'shapes: {image_features.shape}, {text_features_for_image.shape}')\n",
    "            for text_features in text_features_for_image:\n",
    "                x = torch.cat([image_features, text_features], dim=0).to(torch.float16)\n",
    "                bbox = self.regressor(x)\n",
    "                bboxes.append(bbox)\n",
    "\n",
    "        # Compute the cosine similarity between an image and its prompts\n",
    "        # similarities = []\n",
    "        # for idx, text_features_for_image in enumerate(text_features_by_image):\n",
    "        #     similarities.append(cosine_similarity(images_features[idx], text_features_for_image))\n",
    "\n",
    "        \n",
    "\n",
    "        # print(f'{images.device}, {images.shape}, {len(prompts)}')\n",
    "        # print(f'images: {images}, prompts: {prompts}')\n",
    "        # print(next(self.models[self.device_index].parameters()).device)\n",
    "        \n",
    "        # return images_features, texts_features\n",
    "        return torch.stack(bboxes)\n",
    "\n",
    "multinet_model = MultiNetModel(models, preprocesses)#model=model, preprocess=preprocess)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multinet_model = torch.nn.DataParallel(multinet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoundingBoxModel(nn.Module):\n",
    "    def __init__(self, models, preprocess):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "\n",
    "        # self.fc1 = nn.Linear(1024, 4096)\n",
    "        # self.relu1 = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(4096, 512 * 6)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        # self.fc3 = nn.Linear(6 * 512, 4)\n",
    "\n",
    "    def forward(self, indices: torch.Tensor, images: torch.Tensor, prompts) -> torch.Tensor:\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        model, preprocess = self.models[self.device_index], self.preprocesses[self.device_index]\n",
    "\n",
    "        images = [images[i] for i in indices]\n",
    "        prompts = [prompts[i] for i in indices]\n",
    "\n",
    "        preprocessed_images = torch.stack([\n",
    "            preprocess(image) for image in images\n",
    "        ]).to(self.device)\n",
    "        preprocessed_prompts = torch.cat([\n",
    "            clip.tokenize(prompt) for prompt in prompts\n",
    "        ]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images_features = model.encode_image(preprocessed_images).float()\n",
    "            texts_features = model.encode_text(preprocessed_prompts).float()\n",
    "\n",
    "        # print(f'{images_features.shape}, {texts_features.shape}')\n",
    "\n",
    "        features = torch.cat([images_features, texts_features], dim=1)\n",
    "        # x = self.fc1(features)\n",
    "        # x = self.relu1(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.relu2(x)\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        # return x\n",
    "        return features\n",
    "\n",
    "boundingbox_model = BoundingBoxModel(models, preprocesses).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    boundingbox_model = torch.nn.DataParallel(boundingbox_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(boundingbox_model.module.models[0].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def iou_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Localization Accuracy Metric\n",
    "\n",
    "    Intersection over Union (IoU) is a common metric measure for localization accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).unsqueeze(0).to(device)\n",
    "    # ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).to(device)\n",
    "\n",
    "    # print(bounding_boxes.shape, ground_truth_bounding_boxes.shape)\n",
    "\n",
    "    return box_iou(bounding_boxes, ground_truth_bounding_boxes)\n",
    "\n",
    "def cosine_similarity_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Cosine Similarity Metric\n",
    "\n",
    "    Cosine similarity is a common metric measure for semantic similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).to(device)\n",
    "    \n",
    "    return cosine_similarity(bounding_boxes, ground_truth_bounding_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum):\n",
    "  try:\n",
    "    actual_model = model.module\n",
    "  except AttributeError:\n",
    "    actual_model = model\n",
    "\n",
    "  optimizer = torch.optim.SGD([\n",
    "      {'params': actual_model.parameters(), 'lr': lr}\n",
    "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
    "  \n",
    "  return optimizer\n",
    "\n",
    "def get_cost_function():\n",
    "  # cost_function = torch.nn.MSELoss()\n",
    "  cost_function = torchvision.ops.distance_box_iou_loss\n",
    "  return cost_function\n",
    "\n",
    "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  # set the network to training mode\n",
    "  net.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # iterate over the training set\n",
    "  for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "    if batch_idx % 25 == 0:\n",
    "      print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    prompts = [prompt_list[0] for prompt_list in prompts]\n",
    "      \n",
    "    # forward pass\n",
    "    outputs = net(indices, images, prompts)\n",
    "\n",
    "    # full_gt_bounding_boxes = []\n",
    "    # for sample_idx, prompt_list in enumerate(prompts):\n",
    "    #     for _ in range(len(prompt_list)):\n",
    "    #         full_gt_bounding_boxes.append(torch.tensor(gt_bounding_boxes[sample_idx]))\n",
    "    # full_gt_bounding_boxes = torch.stack(full_gt_bounding_boxes).to(device)\n",
    "    gt_bounding_boxes = torch.tensor(gt_bounding_boxes).to(device)\n",
    "\n",
    "    # pred_bounding_boxes_by_image = [[] for _ in range(len(prompts))]\n",
    "    # for sample_idx, prompt_list in enumerate(prompts):\n",
    "    #     for _ in range(len(prompt_list)):\n",
    "    #         pred_bounding_boxes_by_image[sample_idx].append(outputs[sample_idx])\n",
    "    #     pred_bounding_boxes_by_image[sample_idx] = torch.stack(pred_bounding_boxes_by_image[sample_idx])\n",
    "\n",
    "    # loss computation\n",
    "    loss = cost_function(outputs, gt_bounding_boxes, reduction='sum')\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # parameters update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # gradients reset\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # fetch prediction and loss value\n",
    "    samples += float(outputs.shape[0])\n",
    "    cumulative_loss += float(loss.item())\n",
    "\n",
    "    # compute accuracy\n",
    "    for output_bbox, gt_bbox in zip(outputs, gt_bounding_boxes):\n",
    "          cumulative_accuracy += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(output_bbox.unsqueeze(0), gt_bbox)]))\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy / samples\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  # set the network to evaluation mode\n",
    "  net.eval() \n",
    "\n",
    "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "  with torch.no_grad():\n",
    "    # iterate over the test set\n",
    "    for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "        if batch_idx % 25 == 0:\n",
    "          print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "        indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "        prompts = [prompt_list[0] for prompt_list in prompts]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = net(indices, images, prompts)\n",
    "        \n",
    "        # full_gt_bounding_boxes = []\n",
    "        # for sample_idx, prompt_list in enumerate(prompts):\n",
    "        #     for _ in range(len(prompt_list)):\n",
    "        #         full_gt_bounding_boxes.append(torch.tensor(gt_bounding_boxes[sample_idx]))\n",
    "        # full_gt_bounding_boxes = torch.stack(full_gt_bounding_boxes).to(device)\n",
    "        gt_bounding_boxes = torch.tensor(gt_bounding_boxes).to(device)\n",
    "\n",
    "        # pred_bounding_boxes_by_image = [[] for _ in range(len(prompts))]\n",
    "        # for sample_idx, prompt_list in enumerate(prompts):\n",
    "        #     for _ in range(len(prompt_list)):\n",
    "        #         pred_bounding_boxes_by_image[sample_idx].append(outputs[sample_idx])\n",
    "        #     pred_bounding_boxes_by_image[sample_idx] = torch.stack(pred_bounding_boxes_by_image[sample_idx])\n",
    "\n",
    "        # loss computation\n",
    "        loss = cost_function(outputs, gt_bounding_boxes, reduction='sum')\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        samples += float(outputs.shape[0])\n",
    "        cumulative_loss += float(loss.item())\n",
    "\n",
    "        # compute accuracy\n",
    "        # print(iou_metric(outputs, full_gt_bounding_boxes).shape)\n",
    "        # for gt_bbox, pred_bbox in zip(full_gt_bounding_boxes, outputs):\n",
    "        # for sample_idx, pred_bounding_boxes_for_image in enumerate(pred_bounding_boxes_by_image):\n",
    "        #   cumulative_accuracy += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(pred_bounding_boxes_for_image, gt_bounding_boxes[sample_idx])]))\n",
    "        for output_bbox, gt_bbox in zip(outputs, gt_bounding_boxes):\n",
    "          cumulative_accuracy += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(output_bbox.unsqueeze(0), gt_bbox)]))\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "# main funcition\n",
    "def train(\n",
    "      # dataset_name=\"cifar10\",\n",
    "      # batch_size=128,\n",
    "      # num_classes=10,\n",
    "      model,\n",
    "      train_loader, val_loader, test_loader,\n",
    "      device='cuda:0',\n",
    "      learning_rate=0.01,\n",
    "      weight_decay=0.000001,\n",
    "      momentum=0.9,\n",
    "      epochs=10,\n",
    "    ):\n",
    "  # create a logger for the experiment\n",
    "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "  # get dataloaders\n",
    "  # train_loader, val_loader, test_loader = get_data(\n",
    "  #   dataset_name, transform=preprocess, batch_size=batch_size,\n",
    "  # )\n",
    "  \n",
    "  # instantiate the network and move it to the chosen device (GPU)\n",
    "  net = model\n",
    "  \n",
    "  # instantiate the optimizer\n",
    "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "  \n",
    "  # define the cost function\n",
    "  cost_function = get_cost_function()\n",
    "\n",
    "  # computes evaluation results before training\n",
    "  print('Before training:')\n",
    "  # train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "  # val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "  # test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "  # log to TensorBoard\n",
    "  # log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "  # log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "  # log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "  # print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  # print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  # print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "\n",
    "  print('\\n-- Starting training --')\n",
    "\n",
    "  # for each epoch, train the network and then compute evaluation results\n",
    "  for e in range(epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    # val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "    # logs to TensorBoard\n",
    "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "    print('Epoch: {:d}'.format(e+1))\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "  # compute final evaluation results\n",
    "  print('After training:')\n",
    "  train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "  val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "  test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "  # log to TensorBoard\n",
    "  log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "  log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "  log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "\n",
    "  # closes the logger\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "-----------------------------------------------------\n",
      "\n",
      "-- Starting training --\n",
      "-- Batch index: 0 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\operatore\\AppData\\Local\\Temp\\ipykernel_10732\\1452670118.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 25 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 100 --\n",
      "-- Batch index: 125 --\n",
      "-- Batch index: 150 --\n",
      "-- Batch index: 175 --\n",
      "-- Batch index: 200 --\n",
      "-- Batch index: 225 --\n",
      "-- Batch index: 250 --\n",
      "-- Batch index: 275 --\n",
      "-- Batch index: 300 --\n",
      "-- Batch index: 325 --\n",
      "Epoch: 1\n",
      "\tTraining loss 0.93159, Training accuracy 0.17\n",
      "\tValidation loss 0.00000, Validation accuracy 0.00\n",
      "-----------------------------------------------------\n",
      "-- Batch index: 0 --\n",
      "-- Batch index: 25 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 100 --\n",
      "-- Batch index: 125 --\n",
      "-- Batch index: 150 --\n",
      "-- Batch index: 175 --\n",
      "-- Batch index: 200 --\n",
      "-- Batch index: 225 --\n",
      "-- Batch index: 250 --\n",
      "-- Batch index: 275 --\n",
      "-- Batch index: 300 --\n",
      "-- Batch index: 325 --\n",
      "Epoch: 2\n",
      "\tTraining loss 0.85095, Training accuracy 0.21\n",
      "\tValidation loss 0.00000, Validation accuracy 0.00\n",
      "-----------------------------------------------------\n",
      "-- Batch index: 0 --\n",
      "-- Batch index: 25 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 100 --\n",
      "-- Batch index: 125 --\n",
      "-- Batch index: 150 --\n",
      "-- Batch index: 175 --\n",
      "-- Batch index: 200 --\n",
      "-- Batch index: 225 --\n",
      "-- Batch index: 250 --\n",
      "-- Batch index: 275 --\n",
      "-- Batch index: 300 --\n",
      "-- Batch index: 325 --\n",
      "Epoch: 3\n",
      "\tTraining loss 0.84517, Training accuracy 0.22\n",
      "\tValidation loss 0.00000, Validation accuracy 0.00\n",
      "-----------------------------------------------------\n",
      "-- Batch index: 0 --\n",
      "-- Batch index: 25 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 100 --\n",
      "-- Batch index: 125 --\n",
      "-- Batch index: 150 --\n",
      "-- Batch index: 175 --\n",
      "-- Batch index: 200 --\n",
      "-- Batch index: 225 --\n",
      "-- Batch index: 250 --\n",
      "-- Batch index: 275 --\n",
      "-- Batch index: 300 --\n",
      "-- Batch index: 325 --\n",
      "Epoch: 4\n",
      "\tTraining loss 0.83756, Training accuracy 0.22\n",
      "\tValidation loss 0.00000, Validation accuracy 0.00\n",
      "-----------------------------------------------------\n",
      "-- Batch index: 0 --\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(boundingbox_model, train_loader, val_loader, test_loader)\n",
      "Cell \u001b[1;32mIn[32], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, test_loader, device, learning_rate, weight_decay, momentum, epochs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# for each epoch, train the network and then compute evaluation results\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 59\u001b[0m   train_loss, train_accuracy \u001b[39m=\u001b[39m training_step(net, train_loader, optimizer, cost_function)\n\u001b[0;32m     60\u001b[0m   val_loss, val_accuracy \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m     61\u001b[0m   \u001b[39m# val_loss, val_accuracy = test_step(net, val_loader, cost_function)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m   \u001b[39m# logs to TensorBoard\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 38\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m prompts \u001b[39m=\u001b[39m [prompt_list[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m prompt_list \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m     37\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m outputs \u001b[39m=\u001b[39m net(indices, images, prompts)\n\u001b[0;32m     40\u001b[0m \u001b[39m# full_gt_bounding_boxes = []\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# for sample_idx, prompt_list in enumerate(prompts):\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m#     for _ in range(len(prompt_list)):\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m#         full_gt_bounding_boxes.append(torch.tensor(gt_bounding_boxes[sample_idx]))\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m# full_gt_bounding_boxes = torch.stack(full_gt_bounding_boxes).to(device)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m gt_bounding_boxes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(gt_bounding_boxes)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[1;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[1;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[0;32m     79\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[1;32m---> 81\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m], streams[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(boundingbox_model, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute average cosine similarity between embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 0 --\n",
      "-- Batch index: 1 --\n",
      "-- Batch index: 2 --\n",
      "-- Batch index: 3 --\n",
      "-- Batch index: 4 --\n",
      "-- Batch index: 5 --\n",
      "-- Batch index: 6 --\n",
      "-- Batch index: 7 --\n",
      "-- Batch index: 8 --\n",
      "-- Batch index: 9 --\n",
      "-- Batch index: 10 --\n",
      "-- Batch index: 11 --\n",
      "-- Batch index: 12 --\n",
      "-- Batch index: 13 --\n",
      "-- Batch index: 14 --\n",
      "-- Batch index: 15 --\n",
      "-- Batch index: 16 --\n",
      "-- Batch index: 17 --\n",
      "-- Batch index: 18 --\n",
      "-- Batch index: 19 --\n",
      "-- Batch index: 20 --\n",
      "-- Batch index: 21 --\n",
      "-- Batch index: 22 --\n",
      "-- Batch index: 23 --\n",
      "-- Batch index: 24 --\n",
      "-- Batch index: 25 --\n",
      "-- Batch index: 26 --\n",
      "-- Batch index: 27 --\n",
      "-- Batch index: 28 --\n",
      "-- Batch index: 29 --\n",
      "-- Batch index: 30 --\n",
      "-- Batch index: 31 --\n",
      "-- Batch index: 32 --\n",
      "-- Batch index: 33 --\n",
      "-- Batch index: 34 --\n",
      "-- Batch index: 35 --\n",
      "-- Batch index: 36 --\n",
      "-- Batch index: 37 --\n",
      "-- Batch index: 38 --\n",
      "-- Batch index: 39 --\n",
      "-- Batch index: 40 --\n",
      "-- Batch index: 41 --\n",
      "-- Batch index: 42 --\n",
      "-- Batch index: 43 --\n",
      "-- Batch index: 44 --\n",
      "-- Batch index: 45 --\n",
      "-- Batch index: 46 --\n",
      "-- Batch index: 47 --\n",
      "-- Batch index: 48 --\n",
      "-- Batch index: 49 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 51 --\n",
      "-- Batch index: 52 --\n",
      "-- Batch index: 53 --\n",
      "-- Batch index: 54 --\n",
      "-- Batch index: 55 --\n",
      "-- Batch index: 56 --\n",
      "-- Batch index: 57 --\n",
      "-- Batch index: 58 --\n",
      "-- Batch index: 59 --\n",
      "-- Batch index: 60 --\n",
      "-- Batch index: 61 --\n",
      "-- Batch index: 62 --\n",
      "-- Batch index: 63 --\n",
      "-- Batch index: 64 --\n",
      "-- Batch index: 65 --\n",
      "-- Batch index: 66 --\n",
      "-- Batch index: 67 --\n",
      "-- Batch index: 68 --\n",
      "-- Batch index: 69 --\n",
      "-- Batch index: 70 --\n",
      "-- Batch index: 71 --\n",
      "-- Batch index: 72 --\n",
      "-- Batch index: 73 --\n",
      "-- Batch index: 74 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 76 --\n",
      "-- Batch index: 77 --\n",
      "-- Batch index: 78 --\n",
      "-- Batch index: 79 --\n",
      "-- Batch index: 80 --\n",
      "-- Batch index: 81 --\n",
      "-- Batch index: 82 --\n",
      "-- Batch index: 83 --\n",
      "-- Batch index: 84 --\n",
      "-- Batch index: 85 --\n",
      "-- Batch index: 86 --\n",
      "-- Batch index: 87 --\n",
      "-- Batch index: 88 --\n",
      "-- Batch index: 89 --\n",
      "-- Batch index: 90 --\n",
      "-- Batch index: 91 --\n",
      "-- Batch index: 92 --\n",
      "-- Batch index: 93 --\n",
      "-- Batch index: 94 --\n",
      "-- Batch index: 95 --\n",
      "-- Batch index: 96 --\n",
      "-- Batch index: 97 --\n",
      "-- Batch index: 98 --\n",
      "-- Batch index: 99 --\n",
      "-- Batch index: 100 --\n",
      "-- Batch index: 101 --\n",
      "-- Batch index: 102 --\n",
      "-- Batch index: 103 --\n",
      "-- Batch index: 104 --\n",
      "-- Batch index: 105 --\n",
      "-- Batch index: 106 --\n",
      "-- Batch index: 107 --\n",
      "-- Batch index: 108 --\n",
      "-- Batch index: 109 --\n",
      "-- Batch index: 110 --\n",
      "-- Batch index: 111 --\n",
      "-- Batch index: 112 --\n",
      "-- Batch index: 113 --\n",
      "-- Batch index: 114 --\n",
      "-- Batch index: 115 --\n",
      "-- Batch index: 116 --\n",
      "-- Batch index: 117 --\n",
      "-- Batch index: 118 --\n",
      "-- Batch index: 119 --\n",
      "-- Batch index: 120 --\n",
      "-- Batch index: 121 --\n",
      "-- Batch index: 122 --\n",
      "-- Batch index: 123 --\n",
      "-- Batch index: 124 --\n",
      "-- Batch index: 125 --\n",
      "-- Batch index: 126 --\n",
      "-- Batch index: 127 --\n",
      "-- Batch index: 128 --\n",
      "-- Batch index: 129 --\n",
      "-- Batch index: 130 --\n",
      "-- Batch index: 131 --\n",
      "-- Batch index: 132 --\n",
      "-- Batch index: 133 --\n",
      "-- Batch index: 134 --\n",
      "-- Batch index: 135 --\n",
      "-- Batch index: 136 --\n",
      "-- Batch index: 137 --\n",
      "-- Batch index: 138 --\n",
      "-- Batch index: 139 --\n",
      "-- Batch index: 140 --\n",
      "-- Batch index: 141 --\n",
      "-- Batch index: 142 --\n",
      "-- Batch index: 143 --\n",
      "-- Batch index: 144 --\n",
      "-- Batch index: 145 --\n",
      "-- Batch index: 146 --\n",
      "-- Batch index: 147 --\n",
      "-- Batch index: 148 --\n",
      "-- Batch index: 149 --\n",
      "-- Batch index: 150 --\n",
      "-- Batch index: 151 --\n",
      "-- Batch index: 152 --\n",
      "-- Batch index: 153 --\n",
      "-- Batch index: 154 --\n",
      "-- Batch index: 155 --\n",
      "-- Batch index: 156 --\n",
      "-- Batch index: 157 --\n",
      "-- Batch index: 158 --\n",
      "-- Batch index: 159 --\n",
      "-- Batch index: 160 --\n",
      "-- Batch index: 161 --\n",
      "-- Batch index: 162 --\n",
      "-- Batch index: 163 --\n",
      "-- Batch index: 164 --\n",
      "-- Batch index: 165 --\n",
      "-- Batch index: 166 --\n",
      "-- Batch index: 167 --\n",
      "-- Batch index: 168 --\n",
      "-- Batch index: 169 --\n",
      "-- Batch index: 170 --\n",
      "-- Batch index: 171 --\n",
      "-- Batch index: 172 --\n",
      "-- Batch index: 173 --\n",
      "-- Batch index: 174 --\n",
      "-- Batch index: 175 --\n",
      "-- Batch index: 176 --\n",
      "-- Batch index: 177 --\n",
      "-- Batch index: 178 --\n",
      "-- Batch index: 179 --\n",
      "-- Batch index: 180 --\n",
      "-- Batch index: 181 --\n",
      "-- Batch index: 182 --\n",
      "-- Batch index: 183 --\n",
      "-- Batch index: 184 --\n",
      "-- Batch index: 185 --\n",
      "-- Batch index: 186 --\n",
      "-- Batch index: 187 --\n",
      "-- Batch index: 188 --\n",
      "-- Batch index: 189 --\n",
      "-- Batch index: 190 --\n",
      "-- Batch index: 191 --\n",
      "-- Batch index: 192 --\n",
      "-- Batch index: 193 --\n",
      "-- Batch index: 194 --\n",
      "-- Batch index: 195 --\n",
      "-- Batch index: 196 --\n",
      "-- Batch index: 197 --\n",
      "-- Batch index: 198 --\n",
      "-- Batch index: 199 --\n",
      "-- Batch index: 200 --\n",
      "-- Batch index: 201 --\n",
      "-- Batch index: 202 --\n",
      "-- Batch index: 203 --\n",
      "-- Batch index: 204 --\n",
      "-- Batch index: 205 --\n",
      "-- Batch index: 206 --\n",
      "-- Batch index: 207 --\n",
      "-- Batch index: 208 --\n",
      "-- Batch index: 209 --\n",
      "-- Batch index: 210 --\n",
      "-- Batch index: 211 --\n",
      "-- Batch index: 212 --\n",
      "-- Batch index: 213 --\n",
      "-- Batch index: 214 --\n",
      "-- Batch index: 215 --\n",
      "-- Batch index: 216 --\n",
      "-- Batch index: 217 --\n",
      "-- Batch index: 218 --\n",
      "-- Batch index: 219 --\n",
      "-- Batch index: 220 --\n",
      "-- Batch index: 221 --\n",
      "-- Batch index: 222 --\n",
      "-- Batch index: 223 --\n",
      "-- Batch index: 224 --\n",
      "-- Batch index: 225 --\n",
      "-- Batch index: 226 --\n",
      "-- Batch index: 227 --\n",
      "-- Batch index: 228 --\n",
      "-- Batch index: 229 --\n",
      "-- Batch index: 230 --\n",
      "-- Batch index: 231 --\n",
      "-- Batch index: 232 --\n",
      "-- Batch index: 233 --\n",
      "-- Batch index: 234 --\n",
      "-- Batch index: 235 --\n",
      "-- Batch index: 236 --\n",
      "-- Batch index: 237 --\n",
      "-- Batch index: 238 --\n",
      "-- Batch index: 239 --\n",
      "-- Batch index: 240 --\n",
      "-- Batch index: 241 --\n",
      "-- Batch index: 242 --\n",
      "-- Batch index: 243 --\n",
      "-- Batch index: 244 --\n",
      "-- Batch index: 245 --\n",
      "-- Batch index: 246 --\n",
      "-- Batch index: 247 --\n",
      "-- Batch index: 248 --\n",
      "-- Batch index: 249 --\n",
      "-- Batch index: 250 --\n",
      "-- Batch index: 251 --\n",
      "-- Batch index: 252 --\n",
      "-- Batch index: 253 --\n",
      "-- Batch index: 254 --\n",
      "-- Batch index: 255 --\n",
      "-- Batch index: 256 --\n",
      "-- Batch index: 257 --\n",
      "-- Batch index: 258 --\n",
      "-- Batch index: 259 --\n",
      "-- Batch index: 260 --\n",
      "-- Batch index: 261 --\n",
      "-- Batch index: 262 --\n",
      "-- Batch index: 263 --\n",
      "-- Batch index: 264 --\n",
      "-- Batch index: 265 --\n",
      "-- Batch index: 266 --\n",
      "-- Batch index: 267 --\n",
      "-- Batch index: 268 --\n",
      "-- Batch index: 269 --\n",
      "-- Batch index: 270 --\n",
      "-- Batch index: 271 --\n",
      "-- Batch index: 272 --\n",
      "-- Batch index: 273 --\n",
      "-- Batch index: 274 --\n",
      "-- Batch index: 275 --\n",
      "-- Batch index: 276 --\n",
      "-- Batch index: 277 --\n",
      "-- Batch index: 278 --\n",
      "-- Batch index: 279 --\n",
      "-- Batch index: 280 --\n",
      "-- Batch index: 281 --\n",
      "-- Batch index: 282 --\n",
      "-- Batch index: 283 --\n",
      "-- Batch index: 284 --\n",
      "-- Batch index: 285 --\n",
      "-- Batch index: 286 --\n",
      "-- Batch index: 287 --\n",
      "-- Batch index: 288 --\n",
      "-- Batch index: 289 --\n",
      "-- Batch index: 290 --\n",
      "-- Batch index: 291 --\n",
      "-- Batch index: 292 --\n",
      "-- Batch index: 293 --\n",
      "-- Batch index: 294 --\n",
      "-- Batch index: 295 --\n",
      "-- Batch index: 296 --\n",
      "-- Batch index: 297 --\n",
      "-- Batch index: 298 --\n",
      "-- Batch index: 299 --\n",
      "-- Batch index: 300 --\n",
      "-- Batch index: 301 --\n",
      "-- Batch index: 302 --\n",
      "-- Batch index: 303 --\n",
      "-- Batch index: 304 --\n",
      "-- Batch index: 305 --\n",
      "-- Batch index: 306 --\n",
      "-- Batch index: 307 --\n",
      "-- Batch index: 308 --\n",
      "-- Batch index: 309 --\n",
      "-- Batch index: 310 --\n",
      "-- Batch index: 311 --\n",
      "-- Batch index: 312 --\n",
      "-- Batch index: 313 --\n",
      "-- Batch index: 314 --\n",
      "-- Batch index: 315 --\n",
      "-- Batch index: 316 --\n",
      "-- Batch index: 317 --\n",
      "-- Batch index: 318 --\n",
      "-- Batch index: 319 --\n",
      "-- Batch index: 320 --\n",
      "-- Batch index: 321 --\n",
      "-- Batch index: 322 --\n",
      "-- Batch index: 323 --\n",
      "-- Batch index: 324 --\n",
      "-- Batch index: 325 --\n",
      "-- Batch index: 326 --\n",
      "-- Batch index: 327 --\n",
      "-- Batch index: 328 --\n"
     ]
    }
   ],
   "source": [
    "overall_outputs = []\n",
    "overall_gt_bboxes = []\n",
    "\n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(train_loader):\n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "    \n",
    "    # images_tensor = torch.stack([transform(image) for image in images]).to(device)\n",
    "    \n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    prompts = [prompt_list[0] for prompt_list in prompts]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = boundingbox_model(indices, images, prompts)\n",
    "\n",
    "    overall_outputs.append(outputs)\n",
    "    overall_gt_bboxes.append(torch.tensor(gt_bounding_boxes).to(device))\n",
    "\n",
    "    # if batch_idx == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(sublist_outputs) for sublist_outputs in overall_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([42112, 1024]), torch.Size([42112, 4]), torch.Size([42112, 1028]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_outputs_tensor = torch.cat(overall_outputs).to(device)\n",
    "overall_gt_bboxes_tensor = torch.cat(overall_gt_bboxes).to(device)\n",
    "\n",
    "\"\"\"\n",
    "Then overall_data contains a row for each (image, prompt). Each row contains\n",
    "the embeddings of both text and image (first 1024 columns) and the ground\n",
    "truth bounding box (last 4 columns)\n",
    "\"\"\"\n",
    "overall_data = torch.cat([overall_outputs_tensor, overall_gt_bboxes_tensor], dim=1)\n",
    "\n",
    "overall_outputs_tensor.shape, overall_gt_bboxes_tensor.shape, overall_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "col_names = [f'emb_{i}' for i in range(1024)]\n",
    "col_names.append('x_min')\n",
    "col_names.append('y_min')\n",
    "col_names.append('x_max')\n",
    "col_names.append('y_max')\n",
    "\n",
    "\n",
    "np.savetxt('train_data.csv', overall_data.cpu().numpy(), delimiter=',', header=','.join(col_names), comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2933678304222916"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_cpu = []\n",
    "for out in overall_outputs:\n",
    "    for cos_sim_val in out:\n",
    "        cos_sim_cpu.append(cos_sim_val.item())\n",
    "cos_sim_cpu = np.array(cos_sim_cpu)\n",
    "np.nanmean(cos_sim_cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 0 --\n",
      "-- Batch index: 1 --\n",
      "-- Batch index: 2 --\n",
      "-- Batch index: 3 --\n",
      "-- Batch index: 4 --\n",
      "-- Batch index: 5 --\n",
      "-- Batch index: 6 --\n",
      "-- Batch index: 7 --\n",
      "-- Batch index: 8 --\n",
      "-- Batch index: 9 --\n",
      "-- Batch index: 10 --\n",
      "-- Batch index: 11 --\n",
      "-- Batch index: 12 --\n",
      "-- Batch index: 13 --\n",
      "-- Batch index: 14 --\n",
      "-- Batch index: 15 --\n",
      "-- Batch index: 16 --\n",
      "-- Batch index: 17 --\n",
      "-- Batch index: 18 --\n",
      "-- Batch index: 19 --\n",
      "-- Batch index: 20 --\n",
      "-- Batch index: 21 --\n",
      "-- Batch index: 22 --\n",
      "-- Batch index: 23 --\n",
      "-- Batch index: 24 --\n",
      "-- Batch index: 25 --\n",
      "-- Batch index: 26 --\n",
      "-- Batch index: 27 --\n",
      "-- Batch index: 28 --\n",
      "-- Batch index: 29 --\n",
      "-- Batch index: 30 --\n",
      "-- Batch index: 31 --\n",
      "-- Batch index: 32 --\n",
      "-- Batch index: 33 --\n",
      "-- Batch index: 34 --\n",
      "-- Batch index: 35 --\n",
      "-- Batch index: 36 --\n",
      "-- Batch index: 37 --\n",
      "-- Batch index: 38 --\n",
      "-- Batch index: 39 --\n",
      "-- Batch index: 40 --\n",
      "-- Batch index: 41 --\n",
      "-- Batch index: 42 --\n",
      "-- Batch index: 43 --\n",
      "-- Batch index: 44 --\n",
      "-- Batch index: 45 --\n",
      "-- Batch index: 46 --\n",
      "-- Batch index: 47 --\n",
      "-- Batch index: 48 --\n",
      "-- Batch index: 49 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 51 --\n",
      "-- Batch index: 52 --\n",
      "-- Batch index: 53 --\n",
      "-- Batch index: 54 --\n",
      "-- Batch index: 55 --\n",
      "-- Batch index: 56 --\n",
      "-- Batch index: 57 --\n",
      "-- Batch index: 58 --\n",
      "-- Batch index: 59 --\n",
      "-- Batch index: 60 --\n",
      "-- Batch index: 61 --\n",
      "-- Batch index: 62 --\n",
      "-- Batch index: 63 --\n",
      "-- Batch index: 64 --\n",
      "-- Batch index: 65 --\n",
      "-- Batch index: 66 --\n",
      "-- Batch index: 67 --\n",
      "-- Batch index: 68 --\n",
      "-- Batch index: 69 --\n",
      "-- Batch index: 70 --\n",
      "-- Batch index: 71 --\n",
      "-- Batch index: 72 --\n",
      "-- Batch index: 73 --\n",
      "-- Batch index: 74 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 76 --\n",
      "-- Batch index: 77 --\n",
      "-- Batch index: 78 --\n"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import boxes as box_ops\n",
    "\n",
    "IoUs = []\n",
    "cosine_similarities = []\n",
    "  \n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(test_loader):\n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    prompts_tensor = [clip.tokenize(prompt_list) for prompt_list in prompts]\n",
    "    \n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    outputs = baseline_model(indices, images, prompts)\n",
    "\n",
    "    outputs_grouped_by_sample = []\n",
    "    outputs_idx = 0\n",
    "    prompts_idx = 0\n",
    "    while True:\n",
    "        if not prompts_idx < len(images):\n",
    "            break\n",
    "\n",
    "        outputs_grouped_by_sample.append(\n",
    "            outputs[outputs_idx : outputs_idx + len(prompts[prompts_idx])]\n",
    "        )\n",
    "\n",
    "        outputs_idx += len(prompts[prompts_idx])\n",
    "        prompts_idx += 1\n",
    "\n",
    "    for output_bboxes, gt_bboxes in zip(outputs_grouped_by_sample, gt_bounding_boxes):\n",
    "        \"\"\"\n",
    "        There is one output bounding box for each prompt given in input.\n",
    "        Note that each prompt for a given input is actually a list of prompts,\n",
    "        therefore it can contain an arbitrary number of promps. Hence, there is\n",
    "        a bounding box for each one of them.\n",
    "        \"\"\"\n",
    "\n",
    "        result_ious = iou_metric(output_bboxes, gt_bboxes)\n",
    "        result_cosine_similarity = cosine_similarity_metric(output_bboxes, gt_bboxes)\n",
    "\n",
    "        for iou in result_ious:\n",
    "            IoUs.append(iou)\n",
    "\n",
    "        for cs in result_cosine_similarity:\n",
    "            cosine_similarities.append(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metrics ---\n",
      "Mean Intersection over Union (mIoU): 0.5163747769828632\n",
      "Mean Cosine Similarity: 0.9557872927090548\n"
     ]
    }
   ],
   "source": [
    "IoUs_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in IoUs])\n",
    "mIoU = np.nanmean(IoUs_to_cpu)\n",
    "\n",
    "cosine_similarities_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in cosine_similarities])\n",
    "m_cos_sim = np.nanmean(cosine_similarities_to_cpu)\n",
    "\n",
    "print('--- Metrics ---')\n",
    "print(f'Mean Intersection over Union (mIoU): {mIoU}')\n",
    "print(f'Mean Cosine Similarity: {m_cos_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m     12\u001b[0m \u001b[39m# Display the image\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m ax\u001b[39m.\u001b[39;49mimshow(img)\n\u001b[0;32m     15\u001b[0m colors \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[39m# Create a Rectangle patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:1459\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1458\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1459\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1461\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1462\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1463\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5658\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[0;32m   5659\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[0;32m   5660\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5661\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5662\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5663\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5665\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5666\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5667\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5668\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:701\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[0;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m--> 701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    705\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "output_idx = 0\n",
    "\n",
    "# Loading the image\n",
    "img = images[output_idx]\n",
    "\n",
    "#Preparing the output\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "# Create a Rectangle patch\n",
    "for bbox, color in zip(outputs_grouped_by_sample[output_idx][1:2], colors):\n",
    "    bounding_box_coordinates = bbox.cpu()\n",
    "    top_left_x, top_left_y = bounding_box_coordinates[0], bounding_box_coordinates[1]\n",
    "    width, height = bounding_box_coordinates[2]- top_left_x, bounding_box_coordinates[3] - top_left_y\n",
    "\n",
    "    # Parameters: (x, y), width, height\n",
    "    rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=1, edgecolor=color, facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.set_title(prompts[output_idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x13d947f9060>, <torch.cuda.device at 0x13d947fb9a0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
