{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./asset/UniTn-Logo.svg\" style=\"width: 7em;\"/></div>\n",
    "<h1 align=\"center\" style=\"font-size: 3.2em;\">University of Trento<br/>Machine Learning - Deep Learning Module</h1>\n",
    "<h2 align=\"center\" style=\"font-size: 1.5em;\">\n",
    "    Marco Garosi<br/>\n",
    "    Matteo Minardi<br/>\n",
    "    Eric Suardi<br/>\n",
    "    A.Y. 2022-2023\n",
    "</h2>\n",
    "<h2 align=\"center\" style=\"font-size: 1.2em;\">Dr. Alessandro Conti<br/>Prof. Elisa Ricci</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* Introduction\n",
    "* How to run this Notebook\n",
    "* Steps\n",
    "* Metrics\n",
    "* \"Boilerplate\" Code\n",
    "* Baseline\n",
    "    * Results\n",
    "    * Issues\n",
    "* Core Ideas\n",
    "    1. Adding a bottleneck on top of CLIP\n",
    "        * Results\n",
    "    2. Adding a Fully-Connected Neural Network on top of CLIP\n",
    "    3. GradCam and ClipSeg\n",
    "    4. Natural Language Understanding\n",
    "    5. Graphs\n",
    "    6. Incorporating more information into the inputs\n",
    "        * Encoding the position using colors\n",
    "            * Encoding position in the background\n",
    "            * Encoding position in the whole image\n",
    "        * [Our Final Proposal] Deviating CLIP's \"attention\" to relevant parts\n",
    "* Results\n",
    "* Notes\n",
    "* References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Deep Learning is becoming increasingly relevant both in the research and industrial worlds. Thanks to its exceptional ability in solving problems and tasks, Deep Learning is being actively researched and developed to discover and implement new exciting architectures.\n",
    "\n",
    "One of the most cutting-edge techniques are the so-called ***multimodal models***, which are models that are able to perform on a mixture of data types, for instance text and visual inputs.\n",
    "\n",
    "The task assigned to this project is based on this latter type of multimodal models. By leveraging the capabilities of the ***Contrastive Language-Image Pre-training***, or **CLIP** for short, model by OpenAI the objective was to perform *visual grounding* on the *RefCOCOg* benchmark dataset.\n",
    "\n",
    "CLIP, which comprises two encoders - one for the textual information, one for the visual one -, comes with a variety of *backbone* models which lead to different performance. The following backbones are available:\n",
    "* ResNet50\n",
    "* ResNet101\n",
    "* ResNet50x4\n",
    "* ResNet50x16\n",
    "* ResNet50x64\n",
    "* ViT-B/32\n",
    "* ViT-B/16\n",
    "* ViT-L/14\n",
    "* ViT-L/14@336px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this Notebook\n",
    "\n",
    "This Notebook contains: the baseline implementation that we developed; **our model/architecture proposal, at the very end** (\"Deviating CLIP's \"attention\" to relevant parts\" section); all the ideas that we worked on, in between the baseline and our final proposal.\n",
    "\n",
    "We decided to include the intermediate ideas as they were part of our process and we think some of them may be of interest.\n",
    "\n",
    "To run this Notebook, we suggest to run the first part up to the baseline. Then, our results can be checked by jumping to the respective section, as mentioned earlier, and running that.\n",
    "\n",
    "It is not necessary to run the code in the cells in between, since it is just implementations of some ideas we are proposing.\n",
    "\n",
    "All the results can be found in the tables at the end of every section, therefore it is not strictly necessary to run the Notebook other than to check it.\n",
    "\n",
    "There is quite some code. However, we did our best to keep it as simple and understandable as possible. Comments will guide the readers into the details, whereas Markdown cells will drive them across the macro-blocks by providing a more general overview of what is going on in each part of this Notebook.\n",
    "Some code may be repeated for clarity and to ensure that it is consistent across all the sections of this comprehensive Notebook.\n",
    "\n",
    "___\n",
    "### Package installation\n",
    "\n",
    "To run this Notebook, some additional packages are required. Running the next cell will install them using PIP. We are assuming that PyTorch is already installed and properly configured to work on the machine (since installation may vary depending on hardware and software configurations).\n",
    "\n",
    "___\n",
    "### Dataset download\n",
    "\n",
    "The dataset will be downloaded from Google Drive, using the shared folder ID and the `gdown` command. However, it may happen that Google Drive does not allow to download the `.tar` folder. This can happen if several downloads of the same file occurred recently and there is nothing we can do to avoid this: it is a Google Drive's policy.\n",
    "\n",
    "If it does not work, we kindly ask to manually load the dataset from Google Drive rather than downloading it in the environment. While this solves the issue, the implementation would depend on the path of the dataset in one's Google Drive storage, so it is not possible, for us, to handle this directly.\n",
    "\n",
    "The dataset is then extracted in the `refcocog` folder, which is the one in which the code will look for to load all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two cells only if executing on Google Colab. They will download the dataset and extract it into the current environment, which will then be mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!tar -xvzf \"drive/MyDrive/Deep Learning Project/refcocog.tar.gz\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this setup section, please remember that CLIP models will be downloaded if they are not found in the cache. Therefore, this may slow down the running time.\n",
    "\n",
    "In addition, some parts of this Notebook use different CLIP backbones, therefore multiple downloads may occur."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "In order to develop our project, we followed the following steps:\n",
    "1. Reading\n",
    "    * We have read the foundational papers for this project\n",
    "1. Dataset exploration\n",
    "    * We have thoroughly explored the dataset to understand its structure, how to link information together and what to expect from it\n",
    "1. Baseline implementation\n",
    "    * We have implemented the baseline to understand how it performs and what issues it brings in\n",
    "1. Results examination\n",
    "    * We have looked at several outputs one-by-one to better understand the problems that the baseline was unable to solve\n",
    "1. Brain storming, looking for novel ideas and a lot of reading\n",
    "1. Exploration\n",
    "    * We have implemented and tested many different architectures and pipelines in order to understand what could work and what could not\n",
    "1. Improvement\n",
    "    * Based on the results of the previous step, we have decided to focus on a couple of architectures and did our best, both in terms of reading new papers and creativity, to push them forwards and improve the baseline's results\n",
    "1. Results examination and conclusions\n",
    "\n",
    "Some of these steps were, of course, iterated multiple times as we had to gather knowledge about the problems of each solution we had thought of."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Before presenting our work, it is crucial to explain the metrics we utilized to measure the performance of our models.\n",
    "\n",
    "We implemented and employed the following metrics:\n",
    "* Intersection over Union (IoU), average\n",
    "* Accuracy, computed as the fraction of samples having an Intersection over Union $\\ge 0.5$\n",
    "* Cosine similarity between the embedding of the image cropped on the estimated bounding box and the embedding of the image cropped on the golden/ground truth bounding box, average\n",
    "\n",
    "Other metrics can be computed (e.g. top k accuracy, which looks for a correct result in the top k produced - that is, the k bounding boxes with the highest confidence). However, these can be derived or are strongly correlated to the three we have decided to use for the overall assignment, therefore we chose to compute the three aforementioned metrics on the whole set of examples used for testing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Boilerplate\" Code\n",
    "\n",
    "In the following sections we will present our code and our findings. In this section we will instead present and briefly explain, through comments, the \"boilerplate\" code that is necessary to run our architecture and assess its performance.\n",
    "\n",
    "> Some pieces of code are repeated across different proposals. This happens especially for loading CLIP model(s). We decided to repeat that code across different proposals to ensure the correct CLIP models are loaded at each spot and to make it easier to refer to the relevant part of the architectures (and CLIP backbones are one of those)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCocoG_Dataset(Dataset):\n",
    "    full_annotations = None\n",
    "\n",
    "    def __init__(self, root_dir, annotations_f, instances_f, split='train', transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations_f = annotations_f\n",
    "        self.instances_f = instances_f\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.get_annotations()\n",
    "        self.image_names = list([\n",
    "            self.annotations[id]['image']['actual_file_name']\n",
    "            for id in self.annotations\n",
    "        ])\n",
    "\n",
    "    def get_annotations(self):\n",
    "        if RefCocoG_Dataset.full_annotations:\n",
    "            self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, RefCocoG_Dataset.full_annotations.items()))\n",
    "            return\n",
    "\n",
    "        # Load pickle data\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.annotations_f), 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "\n",
    "        # Load instances\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.instances_f), 'rb') as file:\n",
    "            self.instances = json.load(file)\n",
    "\n",
    "        # Match data between the two files and build the actual dataset\n",
    "        self.annotations = {}\n",
    "\n",
    "        images_actual_file_names = {}\n",
    "        for image in self.instances['images']:\n",
    "            images_actual_file_names[image['id']] = image['file_name']\n",
    "\n",
    "        for image in self.data:\n",
    "            if image['ann_id'] not in self.annotations:\n",
    "                self.annotations[image['ann_id']] = {}\n",
    "\n",
    "            self.annotations[image['ann_id']]['image'] = image\n",
    "            self.annotations[image['ann_id']]['image']['actual_file_name'] = images_actual_file_names[image['image_id']]\n",
    "\n",
    "        for annotation in self.instances['annotations']:\n",
    "            if annotation['id'] not in self.annotations:\n",
    "                continue\n",
    "\n",
    "            self.annotations[annotation['id']]['annotation'] = annotation\n",
    "\n",
    "        # Keep only samples from the given split\n",
    "        RefCocoG_Dataset.full_annotations = self.annotations\n",
    "        self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, self.annotations.items()))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def corner_size_to_corners(self, bounding_box):\n",
    "        \"\"\"\n",
    "        Transform (top_left_x, top_left_y, width, height) bounding box representation\n",
    "        into (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            bounding_box[0],\n",
    "            bounding_box[1],\n",
    "            bounding_box[0] + bounding_box[2],\n",
    "            bounding_box[1] + bounding_box[3]\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image name at the given index\n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # Load the image file as a PIL image\n",
    "        image = Image.open(os.path.join(self.root_dir, 'images', image_name)).convert('RGB')\n",
    "        \n",
    "        image_id = list(self.annotations)[idx]\n",
    "\n",
    "        # Get the caption for the image\n",
    "        prompts = [\n",
    "            prompt['sent'] for prompt in self.annotations[image_id]['image']['sentences']\n",
    "        ]\n",
    "\n",
    "        # Get the bounding box for the prompts for the image\n",
    "        bounding_box = self.corner_size_to_corners(self.annotations[image_id]['annotation']['bbox'])\n",
    "\n",
    "        # Apply the transform if given\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = [\n",
    "            image,\n",
    "            bounding_box,\n",
    "            prompts,\n",
    "        ]\n",
    "\n",
    "        # Return the sample as a list\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with the three splits\n",
    "\n",
    "dataset_train = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='train')\n",
    "dataset_val = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='val')\n",
    "dataset_test = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='test')\n",
    "\n",
    "dataset_splits = [\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49820, 42224, 2573, 5023)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display how many samples there are per split to check whether\n",
    "# it was loaded correctly\n",
    "len(RefCocoG_Dataset.full_annotations), len(dataset_train.annotations), len(dataset_val.annotations), len(dataset_test.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be able to move lists of objects around (especially lists of\n",
    "# PIL Images, rather than tensors), we need a custom collation function.\n",
    "# This ensures we can feed the original images to the pipeline, rather\n",
    "# than tensor-transformed (with scaling, cropping, etc.) versions.\n",
    "\n",
    "def collate_differently_sized_prompts(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    bboxes = [item[1] for item in batch]\n",
    "    prompts = [item[2] for item in batch]\n",
    "    \n",
    "    return list(images), list(bboxes), list(prompts)\n",
    "\n",
    "def get_data(dataset_splits, batch_size=64, test_batch_size=256, num_workers=0):\n",
    "    training_data = dataset_splits[0]\n",
    "    validation_data = dataset_splits[1]\n",
    "    test_data = dataset_splits[2]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, collate_fn=collate_differently_sized_prompts, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data(dataset_splits, batch_size=64, test_batch_size=64, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the correct device to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # First GPU\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor, keep_on_same_device=False):\n",
    "    \"\"\"\n",
    "    Cosine Similarity\n",
    "\n",
    "    Normalizes both tensors a and b. Returns <b, a.T> (inner product).\n",
    "    \"\"\"\n",
    "\n",
    "    a_norm = a / a.norm(dim=-1, keepdim=True)\n",
    "    b_norm = b / b.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (b_norm @ a_norm.T)\n",
    "\n",
    "    if keep_on_same_device:\n",
    "        return similarity\n",
    "    \n",
    "    return similarity.cpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And metrics-related code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Evaluator(nn.Module):\n",
    "    def __init__(self, device=None, models=None, preprocesses=None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        if not models or not preprocesses:\n",
    "            raise ValueError('Models and preprocesses for CLIP model should be provided')\n",
    "\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "\n",
    "        self.clip_backbones = list(self.models.keys())\n",
    "\n",
    "    def forward(self, indices, images, gt_bounding_boxes, pred_bounding_boxes):\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        # -- Getting the right data and moving it to the correct device --\n",
    "\n",
    "        # Images remain on the CPU because they are PIL Images, not Tensors\n",
    "        # Converting to Tensors leads to errors with YOLO\n",
    "        images = [images[i] for i in indices]\n",
    "\n",
    "        gt_bounding_boxes = [torch.tensor(gt_bounding_boxes[i]).unsqueeze(0) for i in indices]\n",
    "        pred_bounding_boxes = [torch.tensor(pred_bounding_boxes[i]) for i in indices]\n",
    "        \n",
    "        pred_crops = [self.get_cropped_bounding_boxes(image, bbox_list) for image, bbox_list in zip(images, pred_bounding_boxes)]\n",
    "        gt_crops = [self.get_cropped_bounding_boxes(image, bbox, len(pred_bboxes)) for image, bbox, pred_bboxes in zip(images, gt_bounding_boxes, pred_bounding_boxes)]\n",
    "\n",
    "        # Store overall results across all the provided backbones\n",
    "        overall_results = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for backbone in self.clip_backbones:\n",
    "                preprocessed_gt_crops = torch.stack([self.preprocesses[backbone][self.device_index](image) for sample in gt_crops for image in sample]).to(self.device)\n",
    "                preprocessed_pred_crops = torch.stack([self.preprocesses[backbone][self.device_index](image) for sample in pred_crops for image in sample]).to(self.device)\n",
    "\n",
    "                gt_crop_features = self.models[backbone][self.device_index].encode_image(preprocessed_gt_crops)\n",
    "                pred_crop_features = self.models[backbone][self.device_index].encode_image(preprocessed_pred_crops)\n",
    "\n",
    "                result = cosine_similarity(gt_crop_features, pred_crop_features, keep_on_same_device=True)\n",
    "                result = torch.diagonal(result)\n",
    "                overall_results.append(result)\n",
    "        \n",
    "        # Computing an average for each column\n",
    "        overall_results = torch.stack(overall_results).mean(dim=0)\n",
    "\n",
    "        return overall_results\n",
    "\n",
    "    def get_cropped_bounding_boxes(self, image, bounding_boxes, repeat=1):\n",
    "        cropped_bounding_boxes = []\n",
    "        \n",
    "        if bounding_boxes is None:\n",
    "            return [image] * repeat\n",
    "\n",
    "        for bounding_box in bounding_boxes:\n",
    "            cropped_img = image.crop((bounding_box[0].item(), bounding_box[1].item(), bounding_box[2].item(), bounding_box[3].item()))\n",
    "            cropped_bounding_boxes += [cropped_img] * repeat\n",
    "        \n",
    "        if len(cropped_bounding_boxes) == 0:\n",
    "            return [image] * repeat\n",
    "\n",
    "        return cropped_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def iou_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Localization Accuracy Metric\n",
    "\n",
    "    Intersection over Union (IoU) is a common metric measure for localization accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(ground_truth_bounding_boxes):\n",
    "        ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes)\n",
    "    ground_truth_bounding_boxes = ground_truth_bounding_boxes.unsqueeze(0).to(device)\n",
    "\n",
    "    return box_iou(bounding_boxes, ground_truth_bounding_boxes)\n",
    "\n",
    "def cosine_similarity_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Cosine Similarity Metric\n",
    "\n",
    "    Cosine similarity is a common metric measure for semantic similarity.\n",
    "    This measures the cosine similarity between the bounding boxes considered\n",
    "    as sets of points.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(ground_truth_bounding_boxes):\n",
    "        ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes)\n",
    "    ground_truth_bounding_boxes = ground_truth_bounding_boxes.to(device)\n",
    "    \n",
    "    return cosine_similarity(bounding_boxes, ground_truth_bounding_boxes)\n",
    "\n",
    "def selected_area_cosine_similarity_metric(images, ground_truth_bounding_boxes, pred_bounding_boxes, evaluator_model):\n",
    "    \"\"\"\n",
    "    Cosime Similarity between the embedding of the crops Metric\n",
    "\n",
    "    Cosine similarity is a common metric measure for semantic similarity.\n",
    "    This measures the cosine similarity between the embeddings of the areas\n",
    "    as cropped by the bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_bounding_boxes = [bbox.cpu().numpy() for bbox in pred_bounding_boxes]\n",
    "    indices = torch.tensor(list(range(len(pred_bounding_boxes)))).to(device)\n",
    "    \n",
    "    return evaluator_model(indices, images, ground_truth_bounding_boxes, pred_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_iou_threshold = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, define a function to test the model on a given split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data_loader, model, evaluator_model, device, verbose=False):\n",
    "    IoUs = []\n",
    "    cosine_similarities = []\n",
    "    \n",
    "    for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "        if verbose:\n",
    "            print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "        # Run the model\n",
    "        indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "        outputs = model(indices, images, prompts)\n",
    "\n",
    "        # Group the outputs by sample, since each sample may have multiple\n",
    "        # prompts which were treated independently of one another, thus\n",
    "        # generating multiple bounding boxes.\n",
    "\n",
    "        outputs_grouped_by_sample = []\n",
    "        outputs_idx = 0\n",
    "        prompts_idx = 0\n",
    "        while True:\n",
    "            if not prompts_idx < len(images):\n",
    "                break\n",
    "\n",
    "            outputs_grouped_by_sample.append(\n",
    "                outputs[outputs_idx : outputs_idx + len(prompts[prompts_idx])]\n",
    "            )\n",
    "\n",
    "            outputs_idx += len(prompts[prompts_idx])\n",
    "            prompts_idx += 1\n",
    "\n",
    "        cosine_similarities += selected_area_cosine_similarity_metric(images, gt_bounding_boxes, outputs_grouped_by_sample, evaluator_model)\n",
    "        \n",
    "        for output_bboxes, gt_bboxes in zip(outputs_grouped_by_sample, gt_bounding_boxes):\n",
    "            \"\"\"\n",
    "            There is one output bounding box for each prompt given in input.\n",
    "            Note that each prompt for a given input is actually a list of prompts,\n",
    "            therefore it can contain an arbitrary number of promps. Hence, there is\n",
    "            a bounding box for each one of them.\n",
    "            \"\"\"\n",
    "            \n",
    "            result_ious = iou_metric(output_bboxes, gt_bboxes)\n",
    "            for iou in result_ious:\n",
    "                IoUs.append(iou)\n",
    "\n",
    "    IoUs_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in IoUs])\n",
    "    mIoU = np.nanmean(IoUs_to_cpu)\n",
    "\n",
    "    counter = np.sum([1 if iou >= accuracy_iou_threshold else 0 for iou in IoUs_to_cpu])\n",
    "    accuracy = counter / len(IoUs)\n",
    "\n",
    "    cosine_similarities_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in cosine_similarities])\n",
    "    m_cos_sim = np.nanmean(cosine_similarities_to_cpu)\n",
    "\n",
    "    print('--- Metrics ---')\n",
    "    print(f'Mean Intersection over Union (mIoU): {mIoU}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Mean Cosine Similarity: {m_cos_sim}')\n",
    "\n",
    "    return mIoU, accuracy, m_cos_sim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "In this section, we present our implementation of the baseline as proposed in the assignment.\n",
    "\n",
    "<div align=\"center\"><img src=\"./asset/baseline-architecture.png\"/></div>\n",
    "\n",
    "More precisely, the region proposals are dealt with as follows:\n",
    "\n",
    "<div align=\"center\"><img src=\"./asset/baseline-detail.png\"/></div>\n",
    "\n",
    "As the image shows, region/object proposals are extracted for every image using the YOLOv5 model. The image is then cropped for every bounding box found by YOLO and these crops are then fed to the CLIP Image Encoder. The prompt(s) given for the image are also fed to CLIP's Textual Encoder.\n",
    "\n",
    "A dot product (cosine similarity) is then computed among all object proposals/prompts pairs, thus creating a matrix with size $n \\times m$, where $n$ is the number of prompts and $m$ is the number of object proposals.\n",
    "\n",
    "To ensure that a bounding box is computed for each prompt, rows are treated independently. The maximum value of the cosine similarity is therefore taken for each row, thus allowing for finding the best crop matching the $i^{\\textit{th}}$ prompt.\n",
    "\n",
    "The code implementation is presented below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to load YOLOv5, small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\operatore/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-28 Python-3.10.11 torch-2.0.0+cu117 CUDA:0 (Quadro P2000, 5120MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\operatore\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in C:\\Users\\operatore/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-28 Python-3.10.11 torch-2.0.0+cu117 CUDA:0 (Quadro P2000, 5120MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\operatore\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "else:\n",
    "    yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have to load CLIP. For the baseline, we use ResNet50x16. Note that, as for YOLO, we load a model for each GPU available: this will allow us to exploit multiple-GPU systems to speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip_backbones = ['RN50x16']\n",
    "\n",
    "models, preprocesses = {}, {}\n",
    "\n",
    "for clip_backbone in clip_backbones:\n",
    "    models[clip_backbone] = []\n",
    "    preprocesses[clip_backbone] = []\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            model, preprocess = clip.load(clip_backbone, device=f'cuda:{i}')\n",
    "            \n",
    "            models[clip_backbone].append(model)\n",
    "            preprocesses[clip_backbone].append(preprocess)\n",
    "    else:\n",
    "        model, preprocess = clip.load(clip_backbone, device=device)\n",
    "        models[clip_backbone].append(model)\n",
    "        preprocesses[clip_backbone].append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use_for_baseline = 'RN50x16'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the actual model. It extends `torch.nn.Module` so that it can be utilized as any other models and it can be used in multi-GPU settings more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, device=None, models=None, preprocesses=None, yolo_models=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a BaselineModel.\n",
    "\n",
    "        CLIP models have to be given, along with the preprocessors.\n",
    "        YOLO models are expected as well.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        if not models or not preprocesses:\n",
    "            raise ValueError('Models and preprocesses for CLIP model should be provided')\n",
    "\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "        \n",
    "        if not yolo_models:\n",
    "            raise ValueError('Models for YOLO should be provided')\n",
    "        self.yolo_models = yolo_models\n",
    "\n",
    "    def forward(self, indices: torch.Tensor, images, prompts_list):\n",
    "        \"\"\"\n",
    "        Forward call\n",
    "\n",
    "        `indices` represents a list of indices for the images and prompts lists.\n",
    "        `indices` is automatically split in a multi-GPU setting, therefore it allows\n",
    "        for extracting only the inputs that should be processed by the single GPU.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list \n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        # -- Getting the right data and moving it to the correct device --\n",
    "\n",
    "        # Images remain on the CPU because they are PIL Images, not Tensors\n",
    "        images = [images[i] for i in indices]\n",
    "\n",
    "        prompts_list = [prompts_list[i] for i in indices]\n",
    "        prompts_tensor = [clip.tokenize(prompt_list).to(self.device) for prompt_list in prompts_list]\n",
    "\n",
    "        # -- Actual processing --\n",
    "\n",
    "        bounding_boxes = self.get_bounding_boxes(images)\n",
    "\n",
    "        # It contains the predicted bounding box for each image for each prompt\n",
    "        # Then, it is a list of length len(images) and for each entry there is a\n",
    "        # list with len(prompts[i]), where i is the i-th image \n",
    "        overall_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, prompts_tensor_for_sample in enumerate(prompts_tensor):\n",
    "                # Image crops\n",
    "                image_crops = self.get_cropped_bounding_boxes(images[idx], bounding_boxes.xyxy[idx])\n",
    "\n",
    "                preprocessed_image_crops = torch.stack([self.preprocesses[self.device_index](image).to(self.device) for image in image_crops])\n",
    "\n",
    "                crop_features = self.models[self.device_index].encode_image(preprocessed_image_crops)\n",
    "                crop_features /= crop_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                text_features = self.models[self.device_index].encode_text(prompts_tensor_for_sample)\n",
    "\n",
    "                similarity = cosine_similarity(crop_features, text_features).float()\n",
    "                texts_p = (100 * similarity).softmax(dim=-1)\n",
    "\n",
    "                # Get the best-matching region proposals for the given prompt\n",
    "                _, max_indices = texts_p.max(dim=1)\n",
    "                try:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor(bounding_boxes.xyxy[idx][max_idx, 0:4]).to(self.device)\n",
    "                        )\n",
    "                except:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor((0, 0, images[idx].size[0], images[idx].size[1])).to(self.device)\n",
    "                        )\n",
    "\n",
    "        return torch.stack(overall_outputs)\n",
    "\n",
    "    def get_bounding_boxes(self, pil_images):\n",
    "        \"\"\"\n",
    "        Extract bounding boxes (region proposals) using YOLOv5\n",
    "        \"\"\"\n",
    "\n",
    "        bounding_boxes = self.yolo_models[self.device_index](pil_images)\n",
    "        return bounding_boxes\n",
    "    \n",
    "    def get_cropped_bounding_boxes(self, image, bounding_boxes):\n",
    "        \"\"\"\n",
    "        Crop the input image on each object found by YOLO.\n",
    "        `bounding_boxes` is a list of boxes, where each box is represented\n",
    "        as (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "        \"\"\"\n",
    "\n",
    "        cropped_bounding_boxes = []\n",
    "        \n",
    "        for bounding_box in bounding_boxes:\n",
    "            cropped_img = image.crop((bounding_box[0].item(), bounding_box[1].item(), bounding_box[2].item(), bounding_box[3].item()))\n",
    "            cropped_bounding_boxes.append(cropped_img)\n",
    "\n",
    "        if len(cropped_bounding_boxes) == 0:\n",
    "            cropped_bounding_boxes.append(image)\n",
    "                \n",
    "        return cropped_bounding_boxes\n",
    "\n",
    "# Instantiate the model\n",
    "baseline_model = BaselineModel(models=models[model_to_use_for_baseline], preprocesses=preprocesses[model_to_use_for_baseline], yolo_models=yolo_models)\n",
    "\n",
    "# And make it work with multiple GPUs if they are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    baseline_model = torch.nn.DataParallel(baseline_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the evaluator with the same CLIP backbones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_model = Evaluator(models=models, preprocesses=preprocesses)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    evaluator_model = torch.nn.DataParallel(evaluator_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on the validation and the test set to measure the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 0 --\n",
      "--- Metrics ---\n",
      "Mean Intersection over Union (mIoU): 0.5377648168453767\n",
      "Accuracy: 0.5564516129032258\n",
      "Mean Cosine Similarity: 0.879729240171371\n",
      "-- Batch index: 0 --\n",
      "--- Metrics ---\n",
      "Mean Intersection over Union (mIoU): 0.5657518891818916\n",
      "Accuracy: 0.559322033898305\n",
      "Mean Cosine Similarity: 0.8941795219809322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5657518891818916, 0.559322033898305, 0.8941795219809322)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-- Test Set --')\n",
    "test_model(test_loader, baseline_model, evaluator_model, device, verbose=True)\n",
    "\n",
    "print('-- Validation Set --')\n",
    "test_model(val_loader, baseline_model, evaluator_model, device, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In order to test the baseline thoroughly, we have run it using different CLIP backbone models. The following table summarizes the results we found for each metric.\n",
    "\n",
    "| Backbone | Mean Intersection over Union | Accuracy | Mean Cosine Similarity |\n",
    "| -------- | ---------------------------- | -------- | ----------------- |\n",
    "| RN50x16  | 0.504                        | 0.517    | 0.877             |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "\n",
    "While the baseline performs decently, we noticed that it had severe issues in dealing with spatial relationships. We found that it performed rather well on those samples which do not have spatial information in the referring expression (e.g. \"The banana\" does not contain spatial information, \"The banana on the right\" does).\n",
    "\n",
    "We strongly believe that this issue derives from the fact that cropping the proposed regions deletes the spatial information in the visual input, thus making it impossible (even for a human being!) to actually discern among a set of visually similar objects whose only distinctive feature is position in the image.\n",
    "\n",
    "While the overall architecture may have other problems, we found this one to be the most \"annoying\". Therefore, we decided to focus on this one issue to improve the baseline on. Hence, we focused on improving the mean Intersection over Union (mIoU) metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Ideas\n",
    "\n",
    "We tried two main ways towards solving this issue:\n",
    "* fine-tuning the CLIP model;\n",
    "* leveraging CLIP's strong zero-shot and multi-modal reasoning capabilities.\n",
    "\n",
    "The idea which led to the best results is the second one. However, we will briefly present all the paths we tried for completeness.\n",
    "\n",
    "In addition, there are some ideas which we though about but which we only partially implemented due to lack of resources and/or because we were already focusing on our final candidate. We would like to briefly present them anyway, as we think they might be of interest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding a bottleneck on top of CLIP\n",
    "\n",
    "We tried adding a bottleneck on top of CLIP with the following architecture:\n",
    "\n",
    "<div align=\"center\"><img src=\"./asset/regressor-model.png\"/></div>\n",
    "\n",
    "It takes as input the concatenation of the text and image embeddings produced by CLIP. It produces as output a set of coordinates, which represent the top-left and bottom-right points of the bounding box.\n",
    "\n",
    "We were expecting for it not to produce good results, but we decided to accurately measure its performance - this is why we implemented it. It was not able to improve on the baseline and we noticed that it usually produces much larger bounding boxes than the ground truth ones.\n",
    "\n",
    "On the other hand, it does not include any region-proposal algorithm, thus being faster in estimating a bounding box.\n",
    "\n",
    "We trained on the full training set for 30 epochs. We utilized the Adam optimizer, with a learning rate of $0.01$ (the Stochastic Gradient Descent (SGD) optimizer tended to diverge, thus being unstable). We tried both the IoU loss and the Mean Squared Error (MSE) loss. In either case, we were not able to get satisfactory results.\n",
    "\n",
    "We chose this architecture as a test to understand whether it was possible to solve the task in this simple way. The architecture is a basic bottleneck, which reduces the dimensionality of the input to the first layer at each layer, thus producing a four-value output as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the architecture's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "backbone = 'ViT-B/32'\n",
    "\n",
    "models, preprocesses = [], []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        model, preprocess = clip.load(backbone, device=f'cuda:{i}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preprocesses.append(preprocess)\n",
    "else:\n",
    "    model, preprocess = clip.load(backbone, device=device)\n",
    "    models.append(model)\n",
    "    preprocesses.append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiNetModel(nn.Module):\n",
    "    def __init__(self, models, preprocesses, downstream_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "\n",
    "        self.clip_out_features = 512\n",
    "\n",
    "        self.regressor = downstream_model\n",
    "\n",
    "    @torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16)\n",
    "    @torch.autocast(device_type=\"cuda\")\n",
    "    def forward(self, indices: torch.Tensor, images, prompts) -> torch.Tensor:\n",
    "        # Get the indices of the samples to work with in the bach for the\n",
    "        # current computing device\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        model, preprocess = self.models[self.device_index], self.preprocesses[self.device_index]\n",
    "        \n",
    "        images = [images[i] for i in indices]\n",
    "        prompts = [prompts[i] for i in indices]\n",
    "\n",
    "        preprocessed_images = torch.stack([\n",
    "            preprocess(image) for image in images\n",
    "        ]).to(self.device)\n",
    "        preprocessed_prompts = torch.cat([\n",
    "            tokenized for tokenized in\n",
    "            [clip.tokenize(prompt_list) for prompt_list in prompts]\n",
    "        ]).to(self.device)\n",
    "\n",
    "        # Storing the index for each prompt so as to easily retreive its encoding\n",
    "        prompts_indices_for_image = []\n",
    "        start_index = 0\n",
    "        for prompt_list in prompts:\n",
    "            prompts_indices_for_image.append(\n",
    "                torch.tensor(list(range(0, len(prompt_list)))).to(self.device) + start_index\n",
    "            )\n",
    "            start_index += len(prompt_list)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images_features = model.encode_image(preprocessed_images)\n",
    "            texts_features = model.encode_text(preprocessed_prompts)\n",
    "\n",
    "        # Retreive the given prompts for each image\n",
    "        text_features_by_image = [\n",
    "            texts_features[indices]\n",
    "            for indices in prompts_indices_for_image\n",
    "        ]\n",
    "\n",
    "        # Feed each pair (image, prompt) pair to the FFNN and get\n",
    "        # predictions in output\n",
    "        bboxes = []\n",
    "        for image_features, text_features_for_image in zip(images_features, text_features_by_image):\n",
    "            for text_features in text_features_for_image:\n",
    "                x = torch.cat([image_features, text_features], dim=0)#.to(torch.float16)\n",
    "                bbox = self.regressor(x)\n",
    "                bboxes.append(bbox)\n",
    "        \n",
    "        return torch.stack(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "    def __init__(self, clip_out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_out_features = clip_out_features\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * self.clip_out_features, self.clip_out_features)\n",
    "        self.fc2 = nn.Linear(self.clip_out_features, self.clip_out_features // 2)\n",
    "        self.fc3 = nn.Linear(self.clip_out_features // 2, 128)\n",
    "        self.fc4 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a tensor of features representing CLIP's image features and\n",
    "        CLIP's text features concatenated into a tensor with length 1024 (512 + 512)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "regressor_model = RegressorModel(512).to(device)\n",
    "multinet_model = MultiNetModel(models, preprocesses, regressor_model)\n",
    "\n",
    "# And make it work with multiple GPUs if they are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multinet_model = torch.nn.DataParallel(multinet_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define training and testing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an Adam optimizer\n",
    "def get_optimizer(model, lr, wd=None, momentum=None):\n",
    "    try:\n",
    "        actual_model = model.module\n",
    "    except AttributeError:\n",
    "        actual_model = model\n",
    "  \n",
    "    optimizer = torch.optim.Adam(actual_model.parameters(), lr=lr)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_loss(pred_bboxes, gt_bboxes):\n",
    "    x1 = torch.max(pred_bboxes[:, 0], gt_bboxes[:, 0])\n",
    "    y1 = torch.max(pred_bboxes[:, 1], gt_bboxes[:, 1])\n",
    "    x2 = torch.max(pred_bboxes[:, 2], gt_bboxes[:, 2])\n",
    "    y2 = torch.max(pred_bboxes[:, 3], gt_bboxes[:, 3])\n",
    "\n",
    "    intersection_area = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "    predicted_area = (pred_bboxes[:, 2] - pred_bboxes[:, 0]) * (pred_bboxes[:, 3] - pred_bboxes[:, 1])\n",
    "    target_area = (gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (gt_bboxes[:, 3] - gt_bboxes[:, 1])\n",
    "\n",
    "    union_area = predicted_area + target_area - intersection_area\n",
    "\n",
    "    # Adding 1e-6 to the denominator for numerical stability\n",
    "    # print(f'IoU is: {(intersection_area / (union_area + 1e-6))}, with intersection {intersection_area} and union {union_area}')\n",
    "    iou_loss = torch.clamp(1.0 - (intersection_area / (union_area + 1e-6)), min=0.0, max=1)\n",
    "    \n",
    "    # print(f'iou loss: {iou_loss}, with mean {iou_loss.nanmean()}')\n",
    "\n",
    "    mean = iou_loss.nanmean()\n",
    "\n",
    "    # Ensure not to return NaN\n",
    "    if torch.isnan(mean):\n",
    "        # print('returning 1')\n",
    "        return torch.tensor(1.0)\n",
    "    # print(f'returning {mean}')\n",
    "    return mean\n",
    "\n",
    "def get_cost_function_multinet_model():\n",
    "    cost_function = torch.nn.MSELoss()\n",
    "    # cost_function = iou_loss\n",
    "    return cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining both training and testing steps\n",
    "\n",
    "@torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16)\n",
    "@torch.autocast(device_type=\"cuda\")\n",
    "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_iou = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Iterate over the training set\n",
    "    for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "        # print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "        indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "        prompts = [prompt_list[0:1] for prompt_list in prompts]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(indices, images, prompts)\n",
    "\n",
    "        # Ground truth\n",
    "        gt_bounding_boxes = torch.tensor(gt_bounding_boxes).to(device)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = cost_function(outputs, gt_bounding_boxes)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameters update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update statistics\n",
    "        samples += float(outputs.shape[0])\n",
    "        cumulative_loss += float(loss.item())\n",
    "\n",
    "        # Compute IoU\n",
    "        for output_bbox, gt_bbox in zip(outputs, gt_bounding_boxes):\n",
    "            cumulative_iou += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(output_bbox.unsqueeze(0), gt_bbox)]))\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_iou / samples\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device='cuda'):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_iou = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval() \n",
    "\n",
    "    # Disable gradient computation (only testing!)\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "            # print(f'-- Batch index: {batch_idx} --')\n",
    "            \n",
    "            indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "            prompts = [prompt_list[0:1] for prompt_list in prompts]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = net(indices, images, prompts)\n",
    "\n",
    "            # Ground truth\n",
    "            gt_bounding_boxes = torch.tensor(gt_bounding_boxes).to(device)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, gt_bounding_boxes)\n",
    "\n",
    "            # Update statistics\n",
    "            samples += float(outputs.shape[0])\n",
    "            cumulative_loss += float(loss.item())\n",
    "\n",
    "            # Compute IoU\n",
    "            for output_bbox, gt_bbox in zip(outputs, gt_bounding_boxes):\n",
    "                cumulative_iou += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(output_bbox.unsqueeze(0), gt_bbox)]))\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_iou / samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the function that actually handles and interleaves training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f'{prefix}/loss', loss, step)\n",
    "    writer.add_scalar(f'{prefix}/accuracy', accuracy, step)\n",
    "\n",
    "def final_tests(net, train_loader, val_loader, test_loader, cost_function, device, writer=None, epochs=None):\n",
    "    # Compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function, device=device)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function, device=device)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device=device)\n",
    "\n",
    "    if writer is not None and epochs is not None:\n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader, val_loader, test_loader,\n",
    "    cost_function,\n",
    "    device='cuda:0',\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.000001,\n",
    "    momentum=0.9,\n",
    "    epochs=10,\n",
    "    skip_initial_train_test=True,\n",
    "    skip_initial_val_test=False,\n",
    "    skip_initial_test_test=False,\n",
    "    ):\n",
    "\n",
    "    # Create a logger\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    # Store the network\n",
    "    net = model\n",
    "    \n",
    "    # Instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "\n",
    "    # Evaluate before training and log to TensorBoard\n",
    "    print('Before training:')\n",
    "    if not skip_initial_train_test:\n",
    "        train_loss, train_accuracy = test_step(net, train_loader, cost_function, device=device)\n",
    "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "        print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "\n",
    "    if not skip_initial_val_test:\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function, device=device)\n",
    "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "        print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "        \n",
    "    if not skip_initial_test_test:\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function, device=device)\n",
    "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "        print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    print('\\n-- Starting training --')\n",
    "\n",
    "    # Train for `epochs` steps\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device=device)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function, device=device)\n",
    "        \n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e+1))\n",
    "        print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "        print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    final_tests(net, train_loader, val_loader, test_loader, cost_function, device, writer, epochs)\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "-----------------------------------------------------\n",
      "\n",
      "-- Starting training --\n",
      "torch.Size([64, 4]) torch.Size([64, 4])\n",
      "tensor([[0.0000e+00, 3.2748e+02, 4.5900e+02, 6.3169e+02],\n",
      "        [3.3342e+02, 3.4340e+01, 5.7159e+02, 4.0771e+02],\n",
      "        [3.1641e+02, 3.1116e+02, 4.6317e+02, 4.8000e+02],\n",
      "        [4.0402e+02, 1.3467e+02, 4.7775e+02, 3.5787e+02],\n",
      "        [4.3619e+02, 2.1093e+02, 5.9039e+02, 4.2631e+02],\n",
      "        [3.9568e+02, 1.4919e+02, 4.8865e+02, 4.1730e+02],\n",
      "        [5.2679e+02, 9.6910e+01, 6.4000e+02, 2.6676e+02],\n",
      "        [8.7950e+01, 1.8550e+01, 4.2692e+02, 5.6745e+02],\n",
      "        [4.1077e+02, 1.4659e+02, 5.3907e+02, 3.6777e+02],\n",
      "        [3.6672e+02, 1.5425e+02, 6.1567e+02, 4.0607e+02],\n",
      "        [3.2939e+02, 9.2370e+01, 5.7968e+02, 4.5210e+02],\n",
      "        [1.3315e+02, 1.3172e+02, 4.6067e+02, 3.2931e+02],\n",
      "        [2.1573e+02, 1.2809e+02, 3.8539e+02, 2.4494e+02],\n",
      "        [3.4899e+02, 0.0000e+00, 4.9742e+02, 2.5417e+02],\n",
      "        [1.1968e+02, 1.3650e+01, 3.2043e+02, 4.7800e+02],\n",
      "        [3.2400e+00, 8.3780e+01, 3.2108e+02, 3.9081e+02],\n",
      "        [3.7676e+02, 3.4384e+02, 4.9272e+02, 4.7811e+02],\n",
      "        [4.1178e+02, 1.3578e+02, 5.5092e+02, 2.8867e+02],\n",
      "        [3.3798e+02, 1.3951e+02, 4.4100e+02, 4.4009e+02],\n",
      "        [4.1439e+02, 1.2500e+00, 6.4000e+02, 1.0141e+02],\n",
      "        [4.4428e+02, 3.0440e+01, 6.4000e+02, 3.4744e+02],\n",
      "        [3.0092e+02, 9.3430e+01, 4.0613e+02, 2.6209e+02],\n",
      "        [3.0128e+02, 7.8470e+01, 4.1632e+02, 3.2621e+02],\n",
      "        [2.8368e+02, 7.4770e+01, 6.2794e+02, 4.2095e+02],\n",
      "        [2.9124e+02, 9.3910e+01, 6.4000e+02, 4.7252e+02],\n",
      "        [2.5439e+02, 3.3758e+02, 3.7043e+02, 4.8733e+02],\n",
      "        [2.7733e+02, 1.9003e+02, 4.7450e+02, 4.0562e+02],\n",
      "        [4.4586e+02, 6.3710e+01, 5.4549e+02, 3.1626e+02],\n",
      "        [7.3510e+01, 5.1890e+01, 3.8198e+02, 5.6072e+02],\n",
      "        [5.5260e+01, 1.3084e+02, 3.1964e+02, 4.7323e+02],\n",
      "        [2.4831e+02, 6.7870e+01, 4.1163e+02, 4.1551e+02],\n",
      "        [7.3470e+01, 2.4422e+02, 2.0618e+02, 5.1835e+02],\n",
      "        [1.8057e+02, 1.3559e+02, 3.0104e+02, 2.2432e+02],\n",
      "        [1.7980e+01, 1.7790e+01, 3.3258e+02, 4.7734e+02],\n",
      "        [4.4850e+01, 1.9624e+02, 1.7493e+02, 4.9900e+02],\n",
      "        [1.1341e+02, 1.5840e+01, 1.8759e+02, 1.4886e+02],\n",
      "        [0.0000e+00, 2.0478e+02, 2.1671e+02, 4.2241e+02],\n",
      "        [1.7004e+02, 2.4070e+01, 5.2600e+02, 4.1885e+02],\n",
      "        [1.1515e+02, 1.9200e+00, 3.5216e+02, 3.7039e+02],\n",
      "        [4.4000e-01, 4.5724e+02, 4.2600e+02, 6.4000e+02],\n",
      "        [5.0810e+01, 3.4432e+02, 6.4000e+02, 4.7514e+02],\n",
      "        [2.5446e+02, 3.3450e+01, 4.0599e+02, 3.6510e+02],\n",
      "        [3.6919e+02, 1.3294e+02, 5.1552e+02, 2.7093e+02],\n",
      "        [2.9013e+02, 2.0026e+02, 4.1259e+02, 4.2269e+02],\n",
      "        [1.8318e+02, 4.5360e+01, 4.6323e+02, 5.9889e+02],\n",
      "        [1.0712e+02, 3.0245e+02, 3.0498e+02, 4.8090e+02],\n",
      "        [8.5800e+00, 4.6690e+01, 5.8598e+02, 4.1733e+02],\n",
      "        [3.4113e+02, 2.5425e+02, 5.7512e+02, 3.8955e+02],\n",
      "        [0.0000e+00, 9.4550e+01, 3.4955e+02, 4.0494e+02],\n",
      "        [3.1526e+02, 9.4420e+01, 6.4000e+02, 3.0072e+02],\n",
      "        [6.2760e+01, 1.5635e+02, 4.8557e+02, 3.8978e+02],\n",
      "        [0.0000e+00, 1.3434e+02, 2.9362e+02, 3.7806e+02],\n",
      "        [5.0400e+00, 2.6233e+02, 2.3138e+02, 3.6690e+02],\n",
      "        [2.8874e+02, 1.6037e+02, 4.6324e+02, 2.9243e+02],\n",
      "        [7.2000e-01, 2.7300e+00, 2.2922e+02, 1.4921e+02],\n",
      "        [2.4908e+02, 2.3640e+01, 4.0706e+02, 5.0681e+02],\n",
      "        [1.2895e+02, 1.6652e+02, 2.9201e+02, 3.1362e+02],\n",
      "        [3.3690e+01, 1.2986e+02, 2.5159e+02, 3.5505e+02],\n",
      "        [7.1910e+01, 6.7600e+01, 3.0202e+02, 4.6742e+02],\n",
      "        [3.6516e+02, 2.8540e+01, 6.3978e+02, 4.3209e+02],\n",
      "        [5.1616e+02, 3.7600e+00, 6.3714e+02, 3.4723e+02],\n",
      "        [3.3793e+02, 3.0549e+02, 4.4705e+02, 5.9490e+02],\n",
      "        [2.6574e+02, 4.2917e+02, 3.4601e+02, 6.2442e+02],\n",
      "        [1.7510e+01, 3.3985e+02, 3.7363e+02, 4.8000e+02]], device='cuda:0') tensor([[-0.0573,  0.0315, -0.0041, -0.0375],\n",
      "        [-0.0541,  0.0249,  0.0157, -0.0084],\n",
      "        [-0.0746,  0.0171, -0.0090, -0.0288],\n",
      "        [-0.0602,  0.0242,  0.0198, -0.0345],\n",
      "        [-0.0653,  0.0247, -0.0110, -0.0391],\n",
      "        [-0.0505,  0.0320, -0.0089, -0.0287],\n",
      "        [-0.0668,  0.0341, -0.0167, -0.0137],\n",
      "        [-0.0599,  0.0260,  0.0081, -0.0196],\n",
      "        [-0.0676,  0.0288,  0.0206, -0.0224],\n",
      "        [-0.0503,  0.0305,  0.0039, -0.0132],\n",
      "        [-0.0638,  0.0236,  0.0189, -0.0292],\n",
      "        [-0.0666,  0.0212,  0.0105, -0.0464],\n",
      "        [-0.0542,  0.0194, -0.0035, -0.0325],\n",
      "        [-0.0608,  0.0197,  0.0081, -0.0351],\n",
      "        [-0.0610,  0.0174, -0.0035, -0.0316],\n",
      "        [-0.0633,  0.0218,  0.0061, -0.0258],\n",
      "        [-0.0690,  0.0197,  0.0093, -0.0382],\n",
      "        [-0.0695,  0.0261,  0.0179, -0.0243],\n",
      "        [-0.0579,  0.0288,  0.0053, -0.0280],\n",
      "        [-0.0702,  0.0167,  0.0062, -0.0289],\n",
      "        [-0.0426,  0.0281,  0.0067, -0.0248],\n",
      "        [-0.0698,  0.0308,  0.0088, -0.0291],\n",
      "        [-0.0549,  0.0219,  0.0191, -0.0379],\n",
      "        [-0.0737,  0.0185, -0.0031, -0.0370],\n",
      "        [-0.0662,  0.0251, -0.0093, -0.0326],\n",
      "        [-0.0553,  0.0347, -0.0116, -0.0351],\n",
      "        [-0.0641,  0.0351, -0.0027, -0.0440],\n",
      "        [-0.0680,  0.0258,  0.0029, -0.0277],\n",
      "        [-0.0684,  0.0175,  0.0012, -0.0405],\n",
      "        [-0.0608,  0.0320,  0.0039, -0.0205],\n",
      "        [-0.0673,  0.0225,  0.0157, -0.0394],\n",
      "        [-0.0607,  0.0093, -0.0031, -0.0566],\n",
      "        [-0.0668,  0.0234, -0.0008, -0.0345],\n",
      "        [-0.0737,  0.0067, -0.0046, -0.0448],\n",
      "        [-0.0551,  0.0296, -0.0060, -0.0238],\n",
      "        [-0.0486,  0.0316, -0.0012, -0.0327],\n",
      "        [-0.0555,  0.0238, -0.0215, -0.0506],\n",
      "        [-0.0624,  0.0221, -0.0038, -0.0300],\n",
      "        [-0.0566,  0.0247, -0.0131, -0.0503],\n",
      "        [-0.0626,  0.0319, -0.0109, -0.0189],\n",
      "        [-0.0675,  0.0211,  0.0124, -0.0479],\n",
      "        [-0.0624,  0.0261,  0.0019, -0.0346],\n",
      "        [-0.0682,  0.0241,  0.0187, -0.0267],\n",
      "        [-0.0695,  0.0158,  0.0223, -0.0342],\n",
      "        [-0.0621,  0.0268,  0.0058, -0.0488],\n",
      "        [-0.0500,  0.0256, -0.0022, -0.0356],\n",
      "        [-0.0618,  0.0209, -0.0048, -0.0389],\n",
      "        [-0.0453,  0.0342, -0.0065, -0.0287],\n",
      "        [-0.0752,  0.0317, -0.0085, -0.0339],\n",
      "        [-0.0699,  0.0215,  0.0132, -0.0325],\n",
      "        [-0.0573,  0.0234,  0.0008, -0.0345],\n",
      "        [-0.0633,  0.0195,  0.0030, -0.0317],\n",
      "        [-0.0638,  0.0264,  0.0126, -0.0279],\n",
      "        [-0.0690,  0.0154, -0.0007, -0.0492],\n",
      "        [-0.0572,  0.0107, -0.0077, -0.0319],\n",
      "        [-0.0555,  0.0174,  0.0042, -0.0310],\n",
      "        [-0.0580,  0.0429,  0.0104, -0.0248],\n",
      "        [-0.0746,  0.0106,  0.0020, -0.0368],\n",
      "        [-0.0593,  0.0186,  0.0214, -0.0407],\n",
      "        [-0.0660,  0.0155, -0.0057, -0.0336],\n",
      "        [-0.0765,  0.0088, -0.0037, -0.0291],\n",
      "        [-0.0706,  0.0230,  0.0004, -0.0358],\n",
      "        [-0.0536,  0.0313, -0.0092, -0.0202],\n",
      "        [-0.0595,  0.0144, -0.0019, -0.0388]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<GatherBackward>)\n",
      "tensor([[-0.0573,  0.0315, -0.0041, -0.0375],\n",
      "        [-0.0541,  0.0249,  0.0157, -0.0084],\n",
      "        [-0.0746,  0.0171, -0.0090, -0.0288],\n",
      "        [-0.0602,  0.0242,  0.0198, -0.0345],\n",
      "        [-0.0653,  0.0247, -0.0110, -0.0391],\n",
      "        [-0.0505,  0.0320, -0.0089, -0.0287],\n",
      "        [-0.0668,  0.0341, -0.0167, -0.0137],\n",
      "        [-0.0599,  0.0260,  0.0081, -0.0196],\n",
      "        [-0.0676,  0.0288,  0.0206, -0.0224],\n",
      "        [-0.0503,  0.0305,  0.0039, -0.0132],\n",
      "        [-0.0638,  0.0236,  0.0189, -0.0292],\n",
      "        [-0.0666,  0.0212,  0.0105, -0.0464],\n",
      "        [-0.0542,  0.0194, -0.0035, -0.0325],\n",
      "        [-0.0608,  0.0197,  0.0081, -0.0351],\n",
      "        [-0.0610,  0.0174, -0.0035, -0.0316],\n",
      "        [-0.0633,  0.0218,  0.0061, -0.0258],\n",
      "        [-0.0690,  0.0197,  0.0093, -0.0382],\n",
      "        [-0.0695,  0.0261,  0.0179, -0.0243],\n",
      "        [-0.0579,  0.0288,  0.0053, -0.0280],\n",
      "        [-0.0702,  0.0167,  0.0062, -0.0289],\n",
      "        [-0.0426,  0.0281,  0.0067, -0.0248],\n",
      "        [-0.0698,  0.0308,  0.0088, -0.0291],\n",
      "        [-0.0549,  0.0219,  0.0191, -0.0379],\n",
      "        [-0.0737,  0.0185, -0.0031, -0.0370],\n",
      "        [-0.0662,  0.0251, -0.0093, -0.0326],\n",
      "        [-0.0553,  0.0347, -0.0116, -0.0351],\n",
      "        [-0.0641,  0.0351, -0.0027, -0.0440],\n",
      "        [-0.0680,  0.0258,  0.0029, -0.0277],\n",
      "        [-0.0684,  0.0175,  0.0012, -0.0405],\n",
      "        [-0.0608,  0.0320,  0.0039, -0.0205],\n",
      "        [-0.0673,  0.0225,  0.0157, -0.0394],\n",
      "        [-0.0607,  0.0093, -0.0031, -0.0566],\n",
      "        [-0.0668,  0.0234, -0.0008, -0.0345],\n",
      "        [-0.0737,  0.0067, -0.0046, -0.0448],\n",
      "        [-0.0551,  0.0296, -0.0060, -0.0238],\n",
      "        [-0.0486,  0.0316, -0.0012, -0.0327],\n",
      "        [-0.0555,  0.0238, -0.0215, -0.0506],\n",
      "        [-0.0624,  0.0221, -0.0038, -0.0300],\n",
      "        [-0.0566,  0.0247, -0.0131, -0.0503],\n",
      "        [-0.0626,  0.0319, -0.0109, -0.0189],\n",
      "        [-0.0675,  0.0211,  0.0124, -0.0479],\n",
      "        [-0.0624,  0.0261,  0.0019, -0.0346],\n",
      "        [-0.0682,  0.0241,  0.0187, -0.0267],\n",
      "        [-0.0695,  0.0158,  0.0223, -0.0342],\n",
      "        [-0.0621,  0.0268,  0.0058, -0.0488],\n",
      "        [-0.0500,  0.0256, -0.0022, -0.0356],\n",
      "        [-0.0618,  0.0209, -0.0048, -0.0389],\n",
      "        [-0.0453,  0.0342, -0.0065, -0.0287],\n",
      "        [-0.0752,  0.0317, -0.0085, -0.0339],\n",
      "        [-0.0699,  0.0215,  0.0132, -0.0325],\n",
      "        [-0.0573,  0.0234,  0.0008, -0.0345],\n",
      "        [-0.0633,  0.0195,  0.0030, -0.0317],\n",
      "        [-0.0638,  0.0264,  0.0126, -0.0279],\n",
      "        [-0.0690,  0.0154, -0.0007, -0.0492],\n",
      "        [-0.0572,  0.0107, -0.0077, -0.0319],\n",
      "        [-0.0555,  0.0174,  0.0042, -0.0310],\n",
      "        [-0.0580,  0.0429,  0.0104, -0.0248],\n",
      "        [-0.0746,  0.0106,  0.0020, -0.0368],\n",
      "        [-0.0593,  0.0186,  0.0214, -0.0407],\n",
      "        [-0.0660,  0.0155, -0.0057, -0.0336],\n",
      "        [-0.0765,  0.0088, -0.0037, -0.0291],\n",
      "        [-0.0706,  0.0230,  0.0004, -0.0358],\n",
      "        [-0.0536,  0.0313, -0.0092, -0.0202],\n",
      "        [-0.0595,  0.0144, -0.0019, -0.0388]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<GatherBackward>) tensor(122727.9844, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "torch.Size([64, 4]) torch.Size([64, 4])\n",
      "tensor([[441.8100, 163.1000, 638.9700, 459.3500],\n",
      "        [222.0000, 155.8000, 405.1700, 437.5200],\n",
      "        [287.1200,  41.3100, 491.3400, 145.7100],\n",
      "        [293.9800, 133.0700, 395.4200, 267.8300],\n",
      "        [356.6700,  94.9200, 427.0000, 396.9400],\n",
      "        [  4.3000, 143.5000, 248.2500, 357.3100],\n",
      "        [  0.0000, 148.6300, 306.3200, 426.9500],\n",
      "        [316.9400,  82.1900, 481.7000, 208.7200],\n",
      "        [148.0500,  31.4100, 265.1700, 169.6800],\n",
      "        [459.9900, 162.1100, 615.0000, 377.9200],\n",
      "        [ 60.5100,  59.9000, 177.5700, 209.3100],\n",
      "        [ 66.9900, 237.5800, 292.6800, 410.0900],\n",
      "        [ 88.1900, 158.8000, 203.6500, 351.0200],\n",
      "        [235.2300,  18.0900, 588.0800, 593.2500],\n",
      "        [349.3700, 125.0000, 429.8200, 314.3700],\n",
      "        [413.0200,  39.0100, 523.6900, 318.5400],\n",
      "        [347.4400, 220.8600, 640.0000, 480.0000],\n",
      "        [312.2700, 119.6000, 640.0000, 306.4700],\n",
      "        [ 83.4000, 384.6900, 224.2300, 623.4700],\n",
      "        [177.3600, 244.2600, 342.5700, 390.2000],\n",
      "        [286.0500,  26.8200, 534.1800, 304.2000],\n",
      "        [ 25.8300,   3.7300, 344.3900, 513.1500],\n",
      "        [305.1400,  62.3700, 458.6700, 398.2100],\n",
      "        [139.1500, 195.2400, 251.3300, 405.5700],\n",
      "        [309.9300,  46.1800, 638.8500, 410.0900],\n",
      "        [272.5000, 123.5000, 408.5000, 529.5000],\n",
      "        [  0.0000, 268.0200, 408.7500, 342.8800],\n",
      "        [ 29.9100,   0.0000, 311.3500, 238.9900],\n",
      "        [170.3800, 425.3700, 272.0400, 617.2300],\n",
      "        [ 34.1800,  42.0700, 198.0700, 307.6200],\n",
      "        [344.7700, 176.5200, 638.9700, 392.2600],\n",
      "        [302.8100, 324.7400, 427.2000, 511.8800],\n",
      "        [297.9200, 123.7300, 484.2000, 357.8800],\n",
      "        [468.1300,  72.8900, 596.3100, 273.5000],\n",
      "        [219.0000,  60.3800, 457.5000, 328.8800],\n",
      "        [169.8900,  45.6300, 421.8200, 333.8700],\n",
      "        [319.2300,   0.0000, 620.4400, 234.7300],\n",
      "        [510.2000, 145.2900, 635.3300, 474.2800],\n",
      "        [203.6400, 168.6200, 323.2100, 299.9500],\n",
      "        [421.5900, 129.3300, 498.0300, 269.6000],\n",
      "        [228.4200,  75.7000, 334.2200, 225.3100],\n",
      "        [ 79.0700,   5.4400, 427.0000, 497.6100],\n",
      "        [  0.0000, 127.0100,  79.6200, 277.7000],\n",
      "        [256.7000, 121.2500, 586.5300, 355.7900],\n",
      "        [373.0200, 202.4400, 639.7300, 346.4100],\n",
      "        [199.2800,   0.0000, 399.8000, 130.7000],\n",
      "        [  0.8800,  34.1400, 400.0000, 362.3600],\n",
      "        [ 50.6000, 167.4300, 163.4900, 295.5600],\n",
      "        [187.5800, 166.7000, 294.3800, 285.6000],\n",
      "        [521.0800, 160.8600, 640.0000, 472.2200],\n",
      "        [ 78.6500, 263.7000, 194.8700, 500.0000],\n",
      "        [131.8700, 104.3600, 316.3100, 218.4300],\n",
      "        [337.9300, 136.1600, 615.5200, 289.2500],\n",
      "        [304.9300,  59.8100, 608.1700, 358.0000],\n",
      "        [ 77.6400,  88.3000, 279.9400, 251.6100],\n",
      "        [ 28.7600, 205.6600,  96.3600, 463.1000],\n",
      "        [186.0500,  52.0100, 303.3500, 402.1000],\n",
      "        [331.8500, 120.8100, 481.2600, 278.9800],\n",
      "        [ 43.1500,  48.9000, 465.9800, 547.9600],\n",
      "        [448.1100, 157.3100, 638.0100, 474.1300],\n",
      "        [ 62.3700,  87.3200, 283.0700, 402.0500],\n",
      "        [400.3600, 281.9000, 583.1200, 371.9100],\n",
      "        [136.4000, 298.2300, 285.1700, 502.1900],\n",
      "        [102.9500, 200.6700, 342.3700, 335.4600]], device='cuda:0') tensor([[3.5312, 3.1270, 6.5703, 5.3398],\n",
      "        [3.5410, 3.1230, 6.5742, 5.3164],\n",
      "        [3.9238, 3.4805, 7.3164, 5.9219],\n",
      "        [4.3555, 3.8555, 8.1406, 6.5469],\n",
      "        [4.3555, 3.8613, 8.1016, 6.5039],\n",
      "        [3.7598, 3.3711, 7.0312, 5.6797],\n",
      "        [3.1230, 2.7930, 5.8750, 4.7539],\n",
      "        [3.9004, 3.4648, 7.2227, 5.8320],\n",
      "        [3.2891, 2.9375, 6.1641, 5.0078],\n",
      "        [3.9355, 3.4512, 7.2891, 5.8945],\n",
      "        [3.5410, 3.1855, 6.6523, 5.4023],\n",
      "        [3.5859, 3.2109, 6.7070, 5.4570],\n",
      "        [3.9023, 3.4336, 7.2617, 5.8203],\n",
      "        [3.5234, 3.1074, 6.5508, 5.2891],\n",
      "        [3.7422, 3.3496, 6.9844, 5.6641],\n",
      "        [3.3594, 2.9863, 6.2695, 5.0898],\n",
      "        [3.4004, 3.0176, 6.3438, 5.1484],\n",
      "        [3.3633, 3.0059, 6.2812, 5.0703],\n",
      "        [3.2441, 2.8984, 6.0742, 4.8906],\n",
      "        [3.1309, 2.8242, 5.8945, 4.7734],\n",
      "        [3.8848, 3.4570, 7.2227, 5.8398],\n",
      "        [3.9141, 3.4727, 7.2617, 5.8945],\n",
      "        [3.5273, 3.0879, 6.5195, 5.2500],\n",
      "        [3.1836, 2.8418, 5.9727, 4.7891],\n",
      "        [2.9043, 2.6523, 5.4805, 4.4883],\n",
      "        [3.8477, 3.3926, 7.1445, 5.8047],\n",
      "        [3.2715, 2.8750, 6.0859, 4.8633],\n",
      "        [3.3848, 3.0156, 6.3008, 5.1289],\n",
      "        [3.4473, 3.0684, 6.4219, 5.1680],\n",
      "        [3.1992, 2.8457, 5.9766, 4.8438],\n",
      "        [2.6660, 2.3887, 4.9922, 4.0078],\n",
      "        [2.9258, 2.6406, 5.5000, 4.4531],\n",
      "        [3.3516, 2.9688, 6.2656, 5.0352],\n",
      "        [4.1797, 3.6738, 7.7461, 6.2305],\n",
      "        [3.1152, 2.7812, 5.8242, 4.6992],\n",
      "        [2.8652, 2.5566, 5.3633, 4.3047],\n",
      "        [3.2832, 2.9492, 6.1562, 4.9883],\n",
      "        [3.5996, 3.1855, 6.6992, 5.4297],\n",
      "        [3.6738, 3.2637, 6.8672, 5.5664],\n",
      "        [2.8594, 2.5664, 5.3594, 4.2656],\n",
      "        [3.6484, 3.2188, 6.7734, 5.4844],\n",
      "        [3.7715, 3.3594, 7.0234, 5.6914],\n",
      "        [3.8418, 3.3945, 7.1523, 5.7188],\n",
      "        [3.5449, 3.1875, 6.6484, 5.4336],\n",
      "        [3.5781, 3.1875, 6.6719, 5.3867],\n",
      "        [3.4336, 3.0430, 6.4062, 5.1875],\n",
      "        [3.5742, 3.1758, 6.6836, 5.4102],\n",
      "        [3.9473, 3.5117, 7.3398, 5.9414],\n",
      "        [3.4551, 3.0684, 6.4531, 5.1875],\n",
      "        [3.4922, 3.0742, 6.4766, 5.2695],\n",
      "        [3.1660, 2.8164, 5.8633, 4.7500],\n",
      "        [2.8555, 2.5547, 5.3633, 4.3555],\n",
      "        [3.2285, 2.8750, 6.0391, 4.8672],\n",
      "        [2.7715, 2.4746, 5.1836, 4.1797],\n",
      "        [3.2617, 2.9297, 6.0820, 4.9883],\n",
      "        [3.8633, 3.3750, 7.1445, 5.7734],\n",
      "        [3.7324, 3.3320, 6.9453, 5.6406],\n",
      "        [3.5215, 3.1406, 6.6133, 5.3555],\n",
      "        [3.3418, 3.0098, 6.3281, 5.0547],\n",
      "        [3.6641, 3.2441, 6.8477, 5.4961],\n",
      "        [4.0156, 3.5566, 7.4570, 6.0391],\n",
      "        [3.4766, 3.1113, 6.5156, 5.2305],\n",
      "        [2.6699, 2.4414, 5.0469, 4.0742],\n",
      "        [3.1387, 2.8105, 5.9102, 4.7773]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<GatherBackward>)\n",
      "tensor([[3.5312, 3.1270, 6.5703, 5.3398],\n",
      "        [3.5410, 3.1230, 6.5742, 5.3164],\n",
      "        [3.9238, 3.4805, 7.3164, 5.9219],\n",
      "        [4.3555, 3.8555, 8.1406, 6.5469],\n",
      "        [4.3555, 3.8613, 8.1016, 6.5039],\n",
      "        [3.7598, 3.3711, 7.0312, 5.6797],\n",
      "        [3.1230, 2.7930, 5.8750, 4.7539],\n",
      "        [3.9004, 3.4648, 7.2227, 5.8320],\n",
      "        [3.2891, 2.9375, 6.1641, 5.0078],\n",
      "        [3.9355, 3.4512, 7.2891, 5.8945],\n",
      "        [3.5410, 3.1855, 6.6523, 5.4023],\n",
      "        [3.5859, 3.2109, 6.7070, 5.4570],\n",
      "        [3.9023, 3.4336, 7.2617, 5.8203],\n",
      "        [3.5234, 3.1074, 6.5508, 5.2891],\n",
      "        [3.7422, 3.3496, 6.9844, 5.6641],\n",
      "        [3.3594, 2.9863, 6.2695, 5.0898],\n",
      "        [3.4004, 3.0176, 6.3438, 5.1484],\n",
      "        [3.3633, 3.0059, 6.2812, 5.0703],\n",
      "        [3.2441, 2.8984, 6.0742, 4.8906],\n",
      "        [3.1309, 2.8242, 5.8945, 4.7734],\n",
      "        [3.8848, 3.4570, 7.2227, 5.8398],\n",
      "        [3.9141, 3.4727, 7.2617, 5.8945],\n",
      "        [3.5273, 3.0879, 6.5195, 5.2500],\n",
      "        [3.1836, 2.8418, 5.9727, 4.7891],\n",
      "        [2.9043, 2.6523, 5.4805, 4.4883],\n",
      "        [3.8477, 3.3926, 7.1445, 5.8047],\n",
      "        [3.2715, 2.8750, 6.0859, 4.8633],\n",
      "        [3.3848, 3.0156, 6.3008, 5.1289],\n",
      "        [3.4473, 3.0684, 6.4219, 5.1680],\n",
      "        [3.1992, 2.8457, 5.9766, 4.8438],\n",
      "        [2.6660, 2.3887, 4.9922, 4.0078],\n",
      "        [2.9258, 2.6406, 5.5000, 4.4531],\n",
      "        [3.3516, 2.9688, 6.2656, 5.0352],\n",
      "        [4.1797, 3.6738, 7.7461, 6.2305],\n",
      "        [3.1152, 2.7812, 5.8242, 4.6992],\n",
      "        [2.8652, 2.5566, 5.3633, 4.3047],\n",
      "        [3.2832, 2.9492, 6.1562, 4.9883],\n",
      "        [3.5996, 3.1855, 6.6992, 5.4297],\n",
      "        [3.6738, 3.2637, 6.8672, 5.5664],\n",
      "        [2.8594, 2.5664, 5.3594, 4.2656],\n",
      "        [3.6484, 3.2188, 6.7734, 5.4844],\n",
      "        [3.7715, 3.3594, 7.0234, 5.6914],\n",
      "        [3.8418, 3.3945, 7.1523, 5.7188],\n",
      "        [3.5449, 3.1875, 6.6484, 5.4336],\n",
      "        [3.5781, 3.1875, 6.6719, 5.3867],\n",
      "        [3.4336, 3.0430, 6.4062, 5.1875],\n",
      "        [3.5742, 3.1758, 6.6836, 5.4102],\n",
      "        [3.9473, 3.5117, 7.3398, 5.9414],\n",
      "        [3.4551, 3.0684, 6.4531, 5.1875],\n",
      "        [3.4922, 3.0742, 6.4766, 5.2695],\n",
      "        [3.1660, 2.8164, 5.8633, 4.7500],\n",
      "        [2.8555, 2.5547, 5.3633, 4.3555],\n",
      "        [3.2285, 2.8750, 6.0391, 4.8672],\n",
      "        [2.7715, 2.4746, 5.1836, 4.1797],\n",
      "        [3.2617, 2.9297, 6.0820, 4.9883],\n",
      "        [3.8633, 3.3750, 7.1445, 5.7734],\n",
      "        [3.7324, 3.3320, 6.9453, 5.6406],\n",
      "        [3.5215, 3.1406, 6.6133, 5.3555],\n",
      "        [3.3418, 3.0098, 6.3281, 5.0547],\n",
      "        [3.6641, 3.2441, 6.8477, 5.4961],\n",
      "        [4.0156, 3.5566, 7.4570, 6.0391],\n",
      "        [3.4766, 3.1113, 6.5156, 5.2305],\n",
      "        [2.6699, 2.4414, 5.0469, 4.0742],\n",
      "        [3.1387, 2.8105, 5.9102, 4.7773]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<GatherBackward>) tensor(107249.5781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "torch.Size([64, 4]) torch.Size([64, 4])\n",
      "tensor([[ 90.7900, 124.0400, 543.8200, 365.8400],\n",
      "        [144.5700,  66.8700, 361.4300, 338.8500],\n",
      "        [  2.8700,  21.4500, 401.7900, 629.8800],\n",
      "        [323.4200,  24.6800, 432.5300, 214.3100],\n",
      "        [282.5200,  14.4100, 399.2800, 304.1400],\n",
      "        [340.4600, 139.1100, 405.7000, 346.3700],\n",
      "        [399.6100, 342.8300, 561.5600, 478.4800],\n",
      "        [204.1900, 124.8400, 282.9700, 263.7000],\n",
      "        [142.4400, 195.8400, 260.5700, 478.3700],\n",
      "        [  1.2500,   0.0000, 639.5800, 473.7700],\n",
      "        [  0.0000, 101.5300, 386.0300, 382.2900],\n",
      "        [ 88.0600,  71.7900, 236.4300, 420.2100],\n",
      "        [ 70.6000, 107.1600, 219.3800, 379.0000],\n",
      "        [171.1800,   0.8300, 570.6100, 286.1300],\n",
      "        [206.9500,  18.1500, 640.0000, 474.3100],\n",
      "        [ 97.0700, 213.0400, 262.9800, 494.0700],\n",
      "        [  5.0800, 136.0800,  71.1100, 368.0200],\n",
      "        [  0.0000, 158.0000, 133.1600, 472.8400],\n",
      "        [ 89.3300, 499.8200, 479.5900, 640.0000],\n",
      "        [343.1100,  89.2000, 526.4800, 427.5100],\n",
      "        [272.4100,  61.8100, 433.1700, 352.1000],\n",
      "        [167.8700, 369.0900, 263.0000, 459.5300],\n",
      "        [  0.0000,   0.0000, 361.6100, 479.6400],\n",
      "        [341.6000, 219.5300, 524.3400, 343.5000],\n",
      "        [218.8500, 162.2700, 345.6700, 299.9900],\n",
      "        [104.5500, 226.4300, 250.3900, 574.1700],\n",
      "        [154.9100, 266.3800, 478.1000, 453.0000],\n",
      "        [237.1400,   1.9900, 342.4300, 165.1400],\n",
      "        [ 55.0100, 251.3300, 413.1200, 449.8000],\n",
      "        [352.3200, 214.3000, 523.3900, 374.6000],\n",
      "        [375.1700, 243.5400, 487.4200, 382.0800],\n",
      "        [286.9900, 209.3600, 602.2500, 432.8200],\n",
      "        [104.9800, 444.8900, 271.8200, 556.2400],\n",
      "        [441.1700, 189.8400, 582.4700, 474.6100],\n",
      "        [133.5100,  73.6500, 309.3800, 244.1600],\n",
      "        [298.7800, 143.8400, 482.7200, 283.7400],\n",
      "        [227.1800,  21.6800, 426.7200, 234.1700],\n",
      "        [229.8200, 145.4700, 407.3000, 380.5900],\n",
      "        [  0.9600,  32.4500, 344.2600, 418.0700],\n",
      "        [329.1300,  60.6900, 482.6500, 194.0700],\n",
      "        [142.8000, 182.4900, 228.1700, 352.2600],\n",
      "        [334.3600, 300.3600, 481.8700, 461.2300],\n",
      "        [  0.0000,  81.8500, 142.7900, 409.0600],\n",
      "        [  0.7500, 131.7000, 315.0400, 312.0500],\n",
      "        [119.5300, 255.6000, 346.6400, 436.8300],\n",
      "        [306.8100, 225.9000, 473.5800, 366.2400],\n",
      "        [134.1500,  28.0200, 252.9800, 394.0500],\n",
      "        [141.9700, 106.5900, 290.7000, 253.0700],\n",
      "        [361.6000,  64.4300, 463.5500, 418.3400],\n",
      "        [  0.0000,   0.0000, 131.6700, 193.6900],\n",
      "        [133.2100,   8.3800, 230.3200, 169.9800],\n",
      "        [205.5200, 114.6000, 378.5300, 637.7000],\n",
      "        [ 72.5800, 127.9800, 306.5700, 354.3300],\n",
      "        [185.5300, 103.5500, 385.4400, 632.8100],\n",
      "        [321.6600,   0.0000, 640.0000,  48.6200],\n",
      "        [150.8700,  56.1600, 356.8600, 307.3500],\n",
      "        [  1.9200, 189.9400, 512.5900, 407.2800],\n",
      "        [ 63.1700, 274.2900, 192.8300, 495.3800],\n",
      "        [  1.3800, 256.7600, 224.7900, 609.9700],\n",
      "        [156.6100, 170.0600, 446.4000, 371.5700],\n",
      "        [ 53.2800,  47.5200, 246.2400, 244.8000],\n",
      "        [180.5400,  83.2000, 253.1400, 280.9400],\n",
      "        [166.9600, 150.6500, 360.7900, 325.2900],\n",
      "        [ 68.7200,  30.2100, 310.6900, 578.5700]], device='cuda:0') tensor([[24.0000, 22.9688, 46.5625, 40.1875],\n",
      "        [17.4219, 16.7500, 33.8438, 29.2812],\n",
      "        [20.3906, 19.6406, 39.6875, 34.3750],\n",
      "        [23.0156, 22.0625, 44.6250, 38.5938],\n",
      "        [22.8281, 21.8594, 44.2500, 38.2812],\n",
      "        [23.0000, 22.0469, 44.6562, 38.5625],\n",
      "        [17.0312, 16.3750, 33.1250, 28.7031],\n",
      "        [25.4531, 24.4219, 49.3750, 42.7188],\n",
      "        [20.5000, 19.6250, 39.6875, 34.3750],\n",
      "        [22.9062, 21.9219, 44.3750, 38.3438],\n",
      "        [21.2188, 20.3438, 41.2500, 35.5938],\n",
      "        [19.8750, 19.0312, 38.5625, 33.3125],\n",
      "        [19.0469, 18.3125, 37.0000, 32.0625],\n",
      "        [19.5625, 18.7656, 38.0000, 32.7500],\n",
      "        [22.3594, 21.4062, 43.3750, 37.4688],\n",
      "        [26.2031, 25.0781, 50.8125, 43.9062],\n",
      "        [18.8125, 18.0312, 36.4688, 31.5000],\n",
      "        [23.2656, 22.3281, 45.2188, 39.0625],\n",
      "        [21.7031, 20.7656, 42.0312, 36.3750],\n",
      "        [19.9844, 19.2188, 38.8438, 33.6875],\n",
      "        [22.9844, 22.0938, 44.6875, 38.7188],\n",
      "        [22.5781, 21.5781, 43.7500, 37.7812],\n",
      "        [18.2812, 17.6250, 35.6250, 30.8281],\n",
      "        [24.9688, 23.9531, 48.4688, 41.9062],\n",
      "        [20.6875, 19.8438, 40.1250, 34.6562],\n",
      "        [20.7031, 19.8906, 40.2188, 34.8750],\n",
      "        [17.3750, 16.6562, 33.7188, 29.0938],\n",
      "        [24.0156, 23.0156, 46.5938, 40.2188],\n",
      "        [23.0469, 22.0938, 44.7500, 38.6562],\n",
      "        [20.6719, 19.8281, 40.1250, 34.6875],\n",
      "        [19.9062, 19.1250, 38.6562, 33.4375],\n",
      "        [20.5625, 19.6875, 39.8750, 34.4688],\n",
      "        [22.4219, 21.5000, 43.5312, 37.5938],\n",
      "        [21.5781, 20.6719, 41.8750, 36.1562],\n",
      "        [15.4453, 14.8281, 29.9844, 25.9375],\n",
      "        [20.8750, 20.0469, 40.5625, 35.0312],\n",
      "        [24.7812, 23.7188, 48.0312, 41.5312],\n",
      "        [23.6250, 22.6719, 45.8750, 39.6250],\n",
      "        [19.1875, 18.3906, 37.2188, 32.1562],\n",
      "        [17.3750, 16.6562, 33.7188, 29.1250],\n",
      "        [24.0469, 23.0625, 46.7500, 40.4062],\n",
      "        [19.2812, 18.5156, 37.4688, 32.4062],\n",
      "        [23.0625, 22.1094, 44.7500, 38.6562],\n",
      "        [21.9844, 21.0938, 42.6562, 36.8750],\n",
      "        [16.9688, 16.2969, 32.9688, 28.5000],\n",
      "        [21.9688, 21.0781, 42.6562, 36.9062],\n",
      "        [22.0781, 21.1562, 42.8438, 37.0625],\n",
      "        [19.6562, 18.8594, 38.1875, 33.0000],\n",
      "        [18.1250, 17.3594, 35.1250, 30.3750],\n",
      "        [19.1094, 18.3125, 37.0938, 32.0625],\n",
      "        [23.1406, 22.1562, 44.8750, 38.7812],\n",
      "        [22.7969, 21.8438, 44.2188, 38.2500],\n",
      "        [21.2188, 20.3594, 41.2188, 35.6562],\n",
      "        [23.0469, 22.0781, 44.7188, 38.6562],\n",
      "        [22.5469, 21.6094, 43.7500, 37.8125],\n",
      "        [22.0938, 21.1562, 42.8750, 37.0312],\n",
      "        [18.3906, 17.6562, 35.7188, 30.8750],\n",
      "        [20.3594, 19.4844, 39.5000, 34.1562],\n",
      "        [21.5938, 20.7031, 41.9688, 36.2812],\n",
      "        [22.1094, 21.2031, 42.9688, 37.1562],\n",
      "        [22.0469, 21.1406, 42.8125, 37.0312],\n",
      "        [16.0938, 15.4922, 31.2812, 27.0469],\n",
      "        [23.0938, 22.1250, 44.7812, 38.7188],\n",
      "        [19.0469, 18.3438, 37.0625, 32.1250]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<GatherBackward>)\n",
      "tensor([[24.0000, 22.9688, 46.5625, 40.1875],\n",
      "        [17.4219, 16.7500, 33.8438, 29.2812],\n",
      "        [20.3906, 19.6406, 39.6875, 34.3750],\n",
      "        [23.0156, 22.0625, 44.6250, 38.5938],\n",
      "        [22.8281, 21.8594, 44.2500, 38.2812],\n",
      "        [23.0000, 22.0469, 44.6562, 38.5625],\n",
      "        [17.0312, 16.3750, 33.1250, 28.7031],\n",
      "        [25.4531, 24.4219, 49.3750, 42.7188],\n",
      "        [20.5000, 19.6250, 39.6875, 34.3750],\n",
      "        [22.9062, 21.9219, 44.3750, 38.3438],\n",
      "        [21.2188, 20.3438, 41.2500, 35.5938],\n",
      "        [19.8750, 19.0312, 38.5625, 33.3125],\n",
      "        [19.0469, 18.3125, 37.0000, 32.0625],\n",
      "        [19.5625, 18.7656, 38.0000, 32.7500],\n",
      "        [22.3594, 21.4062, 43.3750, 37.4688],\n",
      "        [26.2031, 25.0781, 50.8125, 43.9062],\n",
      "        [18.8125, 18.0312, 36.4688, 31.5000],\n",
      "        [23.2656, 22.3281, 45.2188, 39.0625],\n",
      "        [21.7031, 20.7656, 42.0312, 36.3750],\n",
      "        [19.9844, 19.2188, 38.8438, 33.6875],\n",
      "        [22.9844, 22.0938, 44.6875, 38.7188],\n",
      "        [22.5781, 21.5781, 43.7500, 37.7812],\n",
      "        [18.2812, 17.6250, 35.6250, 30.8281],\n",
      "        [24.9688, 23.9531, 48.4688, 41.9062],\n",
      "        [20.6875, 19.8438, 40.1250, 34.6562],\n",
      "        [20.7031, 19.8906, 40.2188, 34.8750],\n",
      "        [17.3750, 16.6562, 33.7188, 29.0938],\n",
      "        [24.0156, 23.0156, 46.5938, 40.2188],\n",
      "        [23.0469, 22.0938, 44.7500, 38.6562],\n",
      "        [20.6719, 19.8281, 40.1250, 34.6875],\n",
      "        [19.9062, 19.1250, 38.6562, 33.4375],\n",
      "        [20.5625, 19.6875, 39.8750, 34.4688],\n",
      "        [22.4219, 21.5000, 43.5312, 37.5938],\n",
      "        [21.5781, 20.6719, 41.8750, 36.1562],\n",
      "        [15.4453, 14.8281, 29.9844, 25.9375],\n",
      "        [20.8750, 20.0469, 40.5625, 35.0312],\n",
      "        [24.7812, 23.7188, 48.0312, 41.5312],\n",
      "        [23.6250, 22.6719, 45.8750, 39.6250],\n",
      "        [19.1875, 18.3906, 37.2188, 32.1562],\n",
      "        [17.3750, 16.6562, 33.7188, 29.1250],\n",
      "        [24.0469, 23.0625, 46.7500, 40.4062],\n",
      "        [19.2812, 18.5156, 37.4688, 32.4062],\n",
      "        [23.0625, 22.1094, 44.7500, 38.6562],\n",
      "        [21.9844, 21.0938, 42.6562, 36.8750],\n",
      "        [16.9688, 16.2969, 32.9688, 28.5000],\n",
      "        [21.9688, 21.0781, 42.6562, 36.9062],\n",
      "        [22.0781, 21.1562, 42.8438, 37.0625],\n",
      "        [19.6562, 18.8594, 38.1875, 33.0000],\n",
      "        [18.1250, 17.3594, 35.1250, 30.3750],\n",
      "        [19.1094, 18.3125, 37.0938, 32.0625],\n",
      "        [23.1406, 22.1562, 44.8750, 38.7812],\n",
      "        [22.7969, 21.8438, 44.2188, 38.2500],\n",
      "        [21.2188, 20.3594, 41.2188, 35.6562],\n",
      "        [23.0469, 22.0781, 44.7188, 38.6562],\n",
      "        [22.5469, 21.6094, 43.7500, 37.8125],\n",
      "        [22.0938, 21.1562, 42.8750, 37.0312],\n",
      "        [18.3906, 17.6562, 35.7188, 30.8750],\n",
      "        [20.3594, 19.4844, 39.5000, 34.1562],\n",
      "        [21.5938, 20.7031, 41.9688, 36.2812],\n",
      "        [22.1094, 21.2031, 42.9688, 37.1562],\n",
      "        [22.0469, 21.1406, 42.8125, 37.0312],\n",
      "        [16.0938, 15.4922, 31.2812, 27.0469],\n",
      "        [23.0938, 22.1250, 44.7812, 38.7188],\n",
      "        [19.0469, 18.3438, 37.0625, 32.1250]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<GatherBackward>) tensor(83200.0547, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m cost_function \u001b[39m=\u001b[39m get_cost_function_multinet_model()\n\u001b[0;32m      4\u001b[0m \u001b[39m# And train\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train(multinet_model, train_loader, val_loader, test_loader, cost_function, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice, skip_initial_test_test\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, skip_initial_train_test\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, skip_initial_val_test\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[75], line 71\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, test_loader, cost_function, device, learning_rate, weight_decay, momentum, epochs, skip_initial_train_test, skip_initial_val_test, skip_initial_test_test)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m# Train for `epochs` steps\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 71\u001b[0m     train_loss, train_accuracy \u001b[39m=\u001b[39m training_step(net, train_loader, optimizer, cost_function, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     72\u001b[0m     val_loss, val_accuracy \u001b[39m=\u001b[39m test_step(net, val_loader, cost_function, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     74\u001b[0m     \u001b[39m# Log to TensorBoard\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[74], line 22\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m prompts \u001b[39m=\u001b[39m [prompt_list[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m prompt_list \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m     21\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m outputs \u001b[39m=\u001b[39m net(indices, images, prompts)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Ground truth\u001b[39;00m\n\u001b[0;32m     25\u001b[0m gt_bounding_boxes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(gt_bounding_boxes)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[1;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[1;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[0;32m     79\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[1;32m---> 81\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m], streams[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the cost function\n",
    "cost_function = get_cost_function_multinet_model()\n",
    "\n",
    "# And train\n",
    "train(multinet_model, train_loader, val_loader, test_loader, cost_function, epochs=30, device=device, skip_initial_test_test=True, skip_initial_train_test=True, skip_initial_val_test=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the model overfits very badly: the training accuracy continues to increase, up to 0.49, whereas the validation accuracy goes down after the first epochs, ending around 0.19. The same goes for the loss: on the training set, it continuously decreases, whilst on the validation fold it increases a lot.\n",
    "\n",
    "Regularization techniques may be introduced, however we think this is not the \"right\" way to address this task. A more complex and larger model is presented in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "| Backbone | Training Mean IoU | Validation Mean IoU | Test Mean IoU |\n",
    "| -------- | ----------------- | ------------------- | ------------- |\n",
    "| ViT-B/32  | 0.49             | 0.19              | 0.19          |\n",
    "\n",
    "We used the Mean Squared Error (MSE) loss and the Adam optimizer. We run for 30 epochs on the whole shuffled training set and evaluated on the whole training set, validation set and test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding a Fully-Connected Neural Network on top of CLIP\n",
    "\n",
    "We tried adding a fully-connected neural network on top of CLIP. Differently from the previous idea, which reduced the dimensionality at each layer, this one first enlarges it and then shrinks down to only four outputs (as the previous one).\n",
    "\n",
    "The architecture is as follows:\n",
    "\n",
    "<div align=\"center\"><img src=\"./asset/large-regressor-model.png\"/></div>\n",
    "\n",
    "This network was larger and we trained similarly to the previous one. We had to use the Mean Squared Error loss as the IoU loss was not stable enough. We also tried to combine it with MSE as:\n",
    "\n",
    "$\\mathcal{L} = \\lambda_1 \\textit{MSE} + \\lambda_2 \\textit{IoU}$\n",
    "\n",
    "where we set $\\lambda_1 = 0.7$ and $\\lambda_2 = 1.0$. In the end, the best one was MSE alone.\n",
    "\n",
    "Mean Squared Error, in this specific context, computes the average (squared) error between each pair of coordinates (predicted, ground truth). Therefore, if a bounding box is \"perfect\" (perfect overlap with the ground truth), it will have a MSE of 0. MSE penalizes small errors less than large errors, so it should allow the network to predict somewhat good values even if precision with respect to the ground truth may not be optimal. Nonetheless, MSE is the standard go-to loss function for regression problems, which is how we framed this problem.\n",
    "\n",
    "Intersection over Union focuses instead on maximizing the overlap between the two boxes. If the prediction is (x, y, x, y) (so it is just a line) and the golden truth is (x, y, ..., ...), then MSE would be quite low, since half the points perfectly match those of the ground truth. However, the goal is to make sure the output has an area and that the overlap with the ground truth is as large as possible. Thus, introducing the IoU may halp in serve this purpose.\n",
    "\n",
    "Training took approximately one hour when using the pre-computed embeddings. In this Notebook, where they are computed on-demand, it takes a lot longer - up to some hours. Indeed, to optimize computing time, we pre-computed the embeddings of all samples in the dataset and stored it on the file system, so that we could train several networks with different parameters much more quickly.\n",
    "\n",
    "> To speed training up, we preprocessed the whole dataset at the beginning by computing the embeddings for both images and textual prompts. Then we stored all those embeddings into a CSV file (approximate size of 1.2 GB for the training set). Eventually, we loaded the whole file containing all the embeddings of the training set into memory, thus cutting off CLIP to focus only on training the neural network. Otherwise, it would have taken much longer.\n",
    "\n",
    "We chose this architecture as we started from the one presented in the previous section and we expanded it by introducing also dropout layers, to try and limit overfit. The architecture first projects the input into a larger dimensional space, which is then reduced progressively down to 4 values, as in the previous architecture.\n",
    "\n",
    "The introduction of dropout layers with probability $p = 0.5$ should reduce at least partially the issues encountered with the previous model. Adding more linear layers should instead help in making it possible to learn a better, richer and more complex representation of the data.\n",
    "\n",
    "In the following, we present the code and architecture of this model. Some elements are shared with the previous architecture, therefore they are not repeated again. To successfully run the following code, then, it is necessary to run the cells in the previous section to get CLIP models loaded, the MultiNetModel architecture defined, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeRegressorModel(nn.Module):\n",
    "    def __init__(self, clip_out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_out_features = clip_out_features\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * self.clip_out_features, 4 * self.clip_out_features)\n",
    "        self.fc2 = nn.Linear(4 * self.clip_out_features, 2 * self.clip_out_features)\n",
    "        self.fc3 = nn.Linear(2 * self.clip_out_features, self.clip_out_features)\n",
    "        self.fc4 = nn.Linear(self.clip_out_features, self.clip_out_features // 2)\n",
    "        self.fc5 = nn.Linear(self.clip_out_features // 2, self.clip_out_features // 8)\n",
    "        self.fc6 = nn.Linear(self.clip_out_features // 8, 4)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a tensor of features representing CLIP's image features and\n",
    "        CLIP's text features concatenated into a tensor with length 1024 (512 + 512)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "large_regressor_model = LargeRegressorModel(512).to(device)\n",
    "multinet_large_model = MultiNetModel(models, preprocesses, large_regressor_model)\n",
    "\n",
    "# And make it work with multiple GPUs if they are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multinet_large_model = torch.nn.DataParallel(multinet_large_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "def loss_function_multinet_large_model(pred_bboxes, gt_bboxes):\n",
    "    lambda_1 = 0.7\n",
    "    lambda_2 = 1.0\n",
    "\n",
    "    return lambda_1 * mse(pred_bboxes, gt_bboxes) + lambda_2 * iou_loss(pred_bboxes, gt_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_function_multinet_large_model():\n",
    "    cost_function = loss_function_multinet_large_model\n",
    "    return cost_function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And eventually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "\tValidation loss 1203.30518, Validation accuracy 0.00\n",
      "\tTest loss 1204.80607, Test accuracy 0.00\n",
      "-----------------------------------------------------\n",
      "\n",
      "-- Starting training --\n",
      "Epoch: 1\n",
      "\tTraining loss 274.51980, Training accuracy 0.13\n",
      "\tValidation loss 200.15798, Validation accuracy 0.17\n",
      "-----------------------------------------------------\n",
      "After training:\n",
      "\tTraining loss 191.87883, Training accuracy 0.17\n",
      "\tValidation loss 200.15798, Validation accuracy 0.17\n",
      "\tTest loss 196.14407, Test accuracy 0.17\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the cost function\n",
    "cost_function = get_cost_function_multinet_large_model()\n",
    "\n",
    "# And train\n",
    "train(multinet_large_model, train_loader, val_loader, test_loader, cost_function, learning_rate=0.001, epochs=30, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GradCam and CLIPSeg\n",
    "\n",
    "We tried using GradCam on CLIP with Vision Transformer backbone to understand what CLIP is looking at. The idea was to compute bounding boxes starting from the heatmaps produced by GradCam itself. However, after manually examining the outputs to hundreds of inputs from the test set, we found it to be quite imprecise, so we decided not to further develop this idea.\n",
    "\n",
    "We then read the paper of CLIPSeg, a way of predicting the segmentation area of an object using CLIP. The overall architecture is quite simple, as it uses a frozen version of CLIP and a transformer is trained on the values of neurons sampled from the last layers of CLIP.\n",
    "\n",
    "We tested ClipSeg on hundreds of inputs as well, and we found it to be very good in producing a segmentation mask for the object referred to by the prompt. Computing a bounding box given the segmentation mask is straightforward.\n",
    "\n",
    "However, it was unable to discern spatial relationships, which was the issue we wanted to address. Therefore, we had to discard the idea of introducing ClipSeg in our architecture, even though it would have allowed us to fully remove YOLO and any other region proposals algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Language Understanding (not implemented, only investigated)\n",
    "\n",
    "Another idea we would like to propose is to work on the textual input to retrieve relevant information by parsing it using tools like [spaCy](https://spacy.io).\n",
    "\n",
    "By parsing the sentence, Noun Phrases (NP) can be extracted. Assuming each NP refers to an object in the image, it is possible to find a match between each NP and each object proposal by computing the embeddings of both of them and getting the object with the highest cosine similarity for each NP.\n",
    "\n",
    "Then, other structures (for instance, Verb Phrases (VP)) can be extracted, thus making it theoretically possible to relate NPs and finding the actual object referred to by the prompt.\n",
    "\n",
    "However, we did not implement this solution as it has at least two issues:\n",
    "1. If there are multiple objects which could be the \"right\" one and these only differ by their spatial position in the image (we found this condition to be quite common in the dataset), this would not work as image crops do not contain any spatial information\n",
    "2. The prompts in the dataset are rarely well-formed and complete sentences. Most of the times, they are simple descriptions which often lack verbs and sometimes present grammatical mistakes. Therefore, spaCy would struggle in dealing with them and more complex models, which introduce an additional overhead other than that of YOLO, would be needed.\n",
    "\n",
    "We also thought about using GPT-2 ([code](https://github.com/openai/gpt-2) and [paper](https://arxiv.org/abs/1908.09203)) to augment/engineer the prompts in an automatic way. More precisely, GPT-2 could be used to add additional information to the prompts based on their actual content. This could help the downstream pipeline/architecture better recognize the dominant/main object referred to by the textual piece of information. However, this technique can add noise to the input, thus leading to a degradation of performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graphs (partially implemented)\n",
    "\n",
    "Another idea we would like to propose it to construct a graph on the object proposals representing their spatial relationships. **In other terms, building a scene graph.** In the graph, each node would represent an object and each edge the spatial relationship between the two objects (nodes).\n",
    "\n",
    "The graph would be represented by an $m \\times m$ adjacency matrix, $m$ being the amount of object proposals.\n",
    "\n",
    "We implemented the construction of the graph in this way:\n",
    "1. Compute the centroid of each object\n",
    "2. Relate it with any other centroid and test whether it lies on its left, right, top, bottom. Transform this information into a numerical value to weight the edge\n",
    "\n",
    "The second step, which we decided not to implement due to computational resources and in order to focus on other strategies, would be to create a Graph Neural Network (GNN) to learn to select a node of the graph given CLIP's embedding of the prompt and, possibly, of each object proposal. This would reduce to a classification task (telling, for each node, whether it is the correct one or not), thus a Cross-Entropy loss may work fine.\n",
    "\n",
    "While this technique could work excellently in the task of spatial-references visual grounding, we believe that it may struggle with all the samples that do not contain any spatial information. We think it may perform as well as the baseline in these cases, since the graph information (which, as mentioned earlier, encodes the spatial relationships among objects through a scene graph) would be basically ignored thanks to the lack of spatial references in the textual prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Incorporating more information into the inputs\n",
    "\n",
    "We think that CLIP's extensive training set, consisting of approximately 400 millions (image, text) pairs, is so large that CLIP has to have learnt good enough representations for our task.\n",
    "\n",
    "The main problem, then, is to understand CLIP's representations and how to leverage them for visual grounding. Therefore, we focused on:\n",
    "* gathering knowledge on CLIP's encoders by extensive tests, and\n",
    "* augmenting the features in such a way to exploit CLIP's capabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried the following two approaches:\n",
    "* incorportaing additional knowledge into the visual prompt;\n",
    "* incorporating additional knowledge into the textual prompt.\n",
    "\n",
    "It turned out that the best performance can be achieved by combining the two. The following sections explain in detail two architectures that we devised.\n",
    "\n",
    "### 6.1 Encoding the position using colors\n",
    "\n",
    "The first idea we present is encoding the position (spatial) information using colors. We thought of this idea as during our preliminary extensive tests we noticed that CLIP was able to distinguish among objects of the same category whose only difference is color (e.g. two teddy bears, where one is light brown and one is dark brown).\n",
    "\n",
    "To achieve a successful incorporation of spatial information into the prompts, we tried two paths:\n",
    "1. encoding it in the background;\n",
    "2. encoding it on the whole image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Encoding position in the background\n",
    "\n",
    "For the first path, we did the following:\n",
    "1. Find region proposals\n",
    "1. Create a new image with the same size of the given one for each region proposal\n",
    "1. Set all the image content but the proposed region to a given color. The color codes for the position of the crop in the image. The position is computed using the centroid of the region proposal. More in detail, the centroid coordinates are normalized in the range $[0, 1] \\times [0, 1]$. Then, a color is assigned based on a Look-Up Table (LUT), which is defined as:\n",
    "\n",
    "| | Left | Center | Right |\n",
    "| - | - | - | -|\n",
    "| Top | Blue | Yellow | Red |\n",
    "| Center | Green | Magenta | Black |\n",
    "| Bottom | Brown | Cyan | White |\n",
    "\n",
    "9 overall regions were defined, coding for different spatial locations.\n",
    "\n",
    "4. Augment the prompt to make CLIP take into account this color information. Augmentation is performed as: `prompt + '. The image has a {color} background`, where `{color}` would take all the possible values on a given row or column based on the prompt itself. For instance, if the prompt contained the keyword \"left\", then it would be augmented generating three new prompts, one for each color in the left column of the table above.\n",
    "\n",
    "<div align=\"center\"><img src=\"./asset/colors-detail.png\"/></div>\n",
    "\n",
    "The following cells contain the highlights extracted from the Python code that we developed for this solution. We are not presenting the whole, runnable code as it is similar to the next section (for which instead we present the code) and we do not want to make it redundant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to perform textual prompt augmentation, where a prompt is checked for containing a spatial reference from the set $\\{\\textit{left}, \\textit{right}, \\textit{above}, \\textit{below}\\}$. If there is a match, three new prompts are created:\n",
    "* for left and right, three prompts corresponding, respectively, to the colors on the left or right column of the grid shown above;\n",
    "* for above and below, three prompts corresponding to the colors on the upper or lower rows of the grid shown above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for prompt in sample['image']['sentences']:\n",
    "    prompts.append(\n",
    "        prompt['sent']\n",
    "    )\n",
    "\n",
    "    if 'left' in prompt['tokens']:\n",
    "        for color in ['blue', 'green', 'brown']:\n",
    "            prompts.append(\n",
    "                f\"{prompt['sent']}. The image has a {color} background\"\n",
    "            )\n",
    "            prompts.append(\n",
    "                f\"{color}. {prompt['sent']}\"\n",
    "            )\n",
    "\n",
    "    if 'right' in prompt['tokens']:\n",
    "        for color in ['red', 'black', 'white']:\n",
    "            prompts.append(\n",
    "                f\"{prompt['sent']}. The image has a {color} background\"\n",
    "            )\n",
    "            prompts.append(\n",
    "                f\"{color}. {prompt['sent']}\"\n",
    "            )\n",
    "\n",
    "    if 'above' in prompt['tokens']:\n",
    "        updated = prompt['sent']\n",
    "        updated = updated[:updated.index('above')].strip()\n",
    "        \n",
    "        for color in ['blue', 'yellow', 'red']:\n",
    "            prompts.append(\n",
    "                f\"{updated}. The image has a {color} background\"\n",
    "            )\n",
    "            prompts.append(\n",
    "                f\"{color}. {updated}\"\n",
    "            )\n",
    "\n",
    "    if 'below' in prompt['tokens']:\n",
    "        updated = prompt['sent']\n",
    "        updated = updated[:updated.index('below')].strip()\n",
    "        \n",
    "        for color in ['brown', 'cyan', 'white']:\n",
    "            prompts.append(\n",
    "                f\"{updated}. The image has a {color} background\"\n",
    "            )\n",
    "            prompts.append(\n",
    "                f\"{color}. {updated}\"\n",
    "            )\n",
    "\n",
    "    return prompts\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to do something similar for the proposed regions, for which we have to encode the spatial position using the color mapping presented in the LUT above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_background_color(normalized_x, normalized_y):\n",
    "    color = [0, 0, 0]\n",
    "\n",
    "    # Left\n",
    "    if normalized_x <= 0.35:\n",
    "        # Top\n",
    "        if normalized_y <= 0.33:\n",
    "            color = [0, 0, 255] # Blue\n",
    "        # Center\n",
    "        elif normalized_y <= 0.67:\n",
    "            color = [0, 255, 0] #Green\n",
    "        # Bottom\n",
    "        else:\n",
    "            color = [139, 69, 19] #Brown\n",
    "        \n",
    "    # Center\n",
    "    elif normalized_x <= 0.65:\n",
    "        # Top\n",
    "        if normalized_y <= 0.33:\n",
    "            color = [255, 255, 0] # Yellow\n",
    "        # Center\n",
    "        elif normalized_y <= 0.67:\n",
    "            color = [255, 0, 255] #Magenta\n",
    "        # Bottom\n",
    "        else:\n",
    "            color = [0, 255, 255] #Cyan\n",
    "\n",
    "    # Right\n",
    "    else:\n",
    "        # Top\n",
    "        if normalized_y <= 0.33:\n",
    "            color = [255, 0, 0] # Red\n",
    "        # Center\n",
    "        elif normalized_y <= 0.67:\n",
    "            color = [0, 0, 0] #Black\n",
    "        # Bottom\n",
    "        else:\n",
    "            color = [255, 255, 255] #White\n",
    "    \n",
    "    #Just to check this never happens\n",
    "    return color\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, only one color is applied to each region proposal based on its location. Conversely, for each prompt containing spatial references three new prompts are generated, because there is the missing information of the other coordinate (_e.g._ left, but upper-left, center-left or bottom-left?)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This led to an overall increase in the performance, with an average IoU of 0.54.\n",
    "\n",
    "Nevertheless, this method had some issues with very crowded images or with samples whose prompt was too complicated. This is the reason why we tried to develop novel and further ideas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Encoding position in the whole image\n",
    "\n",
    "For this path, we did the following:\n",
    "1. Find region proposals\n",
    "1. Cropping each region proposal, thus generating $m$ new images\n",
    "1. Add a colored overlay to each image based on the object's position in the original image. Color is assigned using a Look-Up Table (LUT).\n",
    "1. Augment the prompt to make CLIP take into account this color information. Augmentation is performed as: `prompt + ' with a {color} overlay`, where `{color}` would be set according to the spatial keyword found in the prompt itself (e.g. \"left\", \"right\").\n",
    "\n",
    "The following cells present the implementation of our idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_class_weights_for_proposals = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload CLIP model since it was overridden in the previous sections where training happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip_backbones = ['RN50x16']\n",
    "\n",
    "models, preprocesses = {}, {}\n",
    "\n",
    "for clip_backbone in clip_backbones:\n",
    "    models[clip_backbone] = []\n",
    "    preprocesses[clip_backbone] = []\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            model, preprocess = clip.load(clip_backbone, device=f'cuda:{i}')\n",
    "            \n",
    "            models[clip_backbone].append(model)\n",
    "            preprocesses[clip_backbone].append(preprocess)\n",
    "    else:\n",
    "        model, preprocess = clip.load(clip_backbone, device=device)\n",
    "        models[clip_backbone].append(model)\n",
    "        preprocesses[clip_backbone].append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use_for_colors_architecture = 'RN50x16'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create class prompts in the form `A photo of a {class}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "classes = {id: class_name for id, class_name in yolo_models[0].names.items()}\n",
    "class_prompts = {id: f'A photo of a {class_name}' for id, class_name in yolo_models[0].names.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tensor with one row per class\n",
    "    prompts_tensor = clip.tokenize(class_prompts.values()).to(device)\n",
    "\n",
    "    # Tensor with one row per class and 512 columns (embeddings), normalized\n",
    "    class_prompts_embeddings = models[model_to_use_for_colors_architecture][0].encode_text(prompts_tensor)\n",
    "    class_prompts_embeddings /= class_prompts_embeddings.norm(dim=-1, keepdim=True)\n",
    "    class_prompts_embeddings = class_prompts_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "class ColorsOverlayModel(nn.Module):\n",
    "    def __init__(self, device=None, models=None, preprocesses=None, yolo_models=None, classes=None, class_embeddings=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a ColorsOverlay model.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        if not models or not preprocesses:\n",
    "            raise ValueError('Models and preprocesses for CLIP model should be provided')\n",
    "\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "        \n",
    "        if not yolo_models:\n",
    "            raise ValueError('Models for YOLO should be provided')\n",
    "        self.yolo_models = yolo_models\n",
    "\n",
    "        if classes is None or class_embeddings is None:\n",
    "            raise ValueError('Classes and class embeddings shoulld be provided')\n",
    "        self.classes = classes\n",
    "        self.class_embeddings = class_embeddings\n",
    "\n",
    "    def forward(self, indices, images, prompts_list):\n",
    "        \"\"\"\n",
    "        Forward call\n",
    "\n",
    "        `indices` represents a list of indices for the images and prompts lists.\n",
    "        `indices` is automatically split in a multi-GPU setting, therefore it allows\n",
    "        for extracting only the inputs that should be processed by the single GPU.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        # -- Getting the right data and moving it to the correct device --\n",
    "\n",
    "        # Images remain on the CPU because they are PIL Images, not Tensors\n",
    "        # Converting to Tensors leads to errors with YOLO\n",
    "        images = [images[i] for i in indices]\n",
    "\n",
    "        prompts_list = [prompts_list[i] for i in indices]\n",
    "        prompts_list = self.update_prompts(prompts_list)\n",
    "        prompts_tensor = [clip.tokenize(prompt_list).to(self.device) for prompt_list in prompts_list]\n",
    "\n",
    "        # -- Actual processing --\n",
    "\n",
    "        bounding_boxes = self.get_bounding_boxes(images)\n",
    "\n",
    "        # It contains the predicted bounding box for each image for each prompt\n",
    "        # Then, it is a list of length len(images) and for each entry there is a\n",
    "        # list with len(prompts[i]), where i is the i-th image \n",
    "        overall_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, prompts_tensor_for_sample in enumerate(prompts_tensor):\n",
    "                # Image crops\n",
    "                image_crops = self.get_cropped_bounding_boxes(images[idx], bounding_boxes.pred[idx])\n",
    "\n",
    "                preprocessed_image_crops = torch.stack([self.preprocesses[self.device_index](image).to(self.device) for image in image_crops])\n",
    "\n",
    "                crop_features = self.models[self.device_index].encode_image(preprocessed_image_crops)\n",
    "                crop_features /= crop_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                text_features = self.models[self.device_index].encode_text(prompts_tensor_for_sample)\n",
    "\n",
    "                text_similarity = cosine_similarity(self.class_embeddings.to(self.device), text_features).float()\n",
    "                prompt_categories_p = (100 * text_similarity).softmax(dim=-1)\n",
    "                \n",
    "                similarity = cosine_similarity(crop_features, text_features).float().to(self.device)\n",
    "                \n",
    "                if use_class_weights_for_proposals:\n",
    "                    # Compute an a-priori score for each crop. These scores are based on the crop's category as\n",
    "                    # predicted by YOLO\n",
    "                    weights_for_crops = torch.zeros((prompt_categories_p.shape[0], len(image_crops))).to(self.device)\n",
    "                    for prompt_idx, t_s in enumerate(text_similarity):\n",
    "                        for weight_idx, crop in enumerate(bounding_boxes.pred[idx]):\n",
    "                            weights_for_crops[prompt_idx, weight_idx] = t_s[int(crop[-1])]\n",
    "                    \n",
    "                    similarity *= weights_for_crops\n",
    "                    \n",
    "                texts_p = (100 * similarity).softmax(dim=-1)\n",
    "\n",
    "                _, max_indices = texts_p.max(dim=1)\n",
    "                try:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor(bounding_boxes.xyxy[idx][max_idx, 0:4]).to(self.device)\n",
    "                        )\n",
    "                except:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor((0, 0, images[idx].size[0], images[idx].size[1])).to(self.device)\n",
    "                        )\n",
    "\n",
    "        return torch.stack(overall_outputs)\n",
    "\n",
    "    def update_prompts(self, prompts):\n",
    "        \"\"\"\n",
    "        Update the prompts by introucing a color reference based on spatial\n",
    "        references.\n",
    "        \"\"\"\n",
    "        \n",
    "        updated_prompts = []\n",
    "\n",
    "        for sample in prompts:\n",
    "            sample_prompts = []\n",
    "            for prompt in sample:\n",
    "                if 'left' in prompt:\n",
    "                    prompt += ' with a red overlay'\n",
    "                elif 'right' in prompt:\n",
    "                    prompt += ' with a green overlay'\n",
    "                sample_prompts.append(prompt)\n",
    "            updated_prompts.append(sample_prompts)\n",
    "\n",
    "        return updated_prompts\n",
    "\n",
    "    def get_bounding_boxes(self, pil_images):\n",
    "        bounding_boxes = self.yolo_models[self.device_index](pil_images)\n",
    "        return bounding_boxes\n",
    "    \n",
    "    def get_cropped_bounding_boxes(self, image, bounding_boxes):\n",
    "        \"\"\"\n",
    "        Bounding boxes in the form:\n",
    "        [top left x, top left y, bottom right x, bottom right y, confidence, categoy]\n",
    "        \"\"\"\n",
    "\n",
    "        cropped_bounding_boxes = []\n",
    "\n",
    "        image_width, image_height = image.size\n",
    "        \n",
    "        for bbox_idx, bounding_box in enumerate(bounding_boxes):\n",
    "            cropped_img = image.crop((bounding_box[0].item(), bounding_box[1].item(), bounding_box[2].item(), bounding_box[3].item()))\n",
    "\n",
    "            # Centroid: (min + (max - min) / 2) / dimension\n",
    "            crop_centroid_normalized = (\n",
    "                ((bounding_box[2].item() + bounding_box[0].item()) / 2) / image_width,\n",
    "                ((bounding_box[3].item() + bounding_box[1].item()) / 2 ) / image_height\n",
    "            )\n",
    "\n",
    "            # Assign a color overlay based on centroid position\n",
    "            if crop_centroid_normalized[0] < 0.5:\n",
    "                overlay = Image.new('RGBA', cropped_img.size, overlay_colors[0])\n",
    "            elif crop_centroid_normalized[0] > 0.5:\n",
    "                overlay = Image.new('RGBA', cropped_img.size, overlay_colors[1])\n",
    "            else:\n",
    "                overlay = Image.new('RGBA', cropped_img.size, overlay_colors[-1])\n",
    "            blended = Image.alpha_composite(cropped_img.convert('RGBA'), overlay)\n",
    "            cropped_bounding_boxes.append(blended)\n",
    "\n",
    "        if len(cropped_bounding_boxes) == 0:\n",
    "            cropped_bounding_boxes.append(image)\n",
    "                \n",
    "        return cropped_bounding_boxes\n",
    "\n",
    "overlay_colors = [\n",
    "    (255, 0, 0, 128),   # Red, alpha = 0.5\n",
    "    (0, 255, 0, 128),   # Green, alpha = 0.5\n",
    "    (0, 0, 255, 128),   # Blue, alpha = 0.5\n",
    "    (0, 0, 0, 0),       # None\n",
    "]\n",
    "\n",
    "colors_overlay_model = ColorsOverlayModel(models=models[model_to_use_for_colors_architecture], preprocesses=preprocesses[model_to_use_for_colors_architecture], yolo_models=yolo_models, classes=classes, class_embeddings=class_prompts_embeddings)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    colors_overlay_model = torch.nn.DataParallel(colors_overlay_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the evaluator with the same CLIP backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_model = Evaluator(models=models, preprocesses=preprocesses)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    evaluator_model = torch.nn.DataParallel(evaluator_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 0 --\n",
      "-- Batch index: 1 --\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_model(test_loader, colors_overlay_model, evaluator_model, device, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[26], line 11\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(data_loader, model, evaluator_model, device, verbose)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Run the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(images))))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[39m=\u001b[39m model(indices, images, prompts)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Group the outputs by sample, since each sample may have multiple\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# prompts which were treated independently of one another, thus\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# generating multiple bounding boxes.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m outputs_grouped_by_sample \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[1;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[1;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[0;32m     79\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[1;32m---> 81\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m], streams[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\operatore\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_model(test_loader, colors_overlay_model, evaluator_model, device, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This led to an overall increase in the performance with respect to the baseline.\n",
    "\n",
    "| Backbone | Mean Intersection over Union | Accuracy | Cosine Similarity |\n",
    "| -------- | ---------------------------- | -------- | ----------------- |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deviating CLIP's \"attention\" to relevant parts (our final proposal)\n",
    "\n",
    "We decided to try and make CLIP focus on some specific areas of the input image while feeding it all to it in order not to lose spatial information and context: indeed, we believe CLIP has the ability to understand language and, if properly guided, it can be more accurate.\n",
    "\n",
    "Therefore, we pushed even further the prompt engineering task we had already done for the previously presented architectures. Some key elements factored in into this final pipeline which we present as our project:\n",
    "1. Attention engineering to make CLIP focus on the relevant object in the image. By introducing masking operators and visual annotations (e.g. rectangles, ellipses) and background blurring to reduce distractions, we found that CLIP is able to shift its attention to che highlighted block. Note that \"attention\" does not only refer to the concept of \"attention\" in Transformer-based models: it refers to the general ability to focus and pay attention on a given portion of the image. Our method indeed works well even with ResNet-based models.\n",
    "1. Prompt adapatation to unbalance even more CLIP's focus onto every object proposal. We have tried several textual prompt engineering techniques, for instance:\n",
    "    * `prompt + ' with a {color} {shape} around it`, where: `{color}` was either fixed for all region proposals, independently of their position, or dependant on their position; `{shape}` depends on the shape actually used for highlighting the region proposal and it can be \"rectangle\", \"ellipse\" or \"circle\".\n",
    "    * `This is + prompt`.\n",
    "\n",
    "We discovered that this idea could work \"by accident\", when, due to a wrong implementation, we drew a rectangle instead of cropping the image on the region proposals. Combined with the aforementioned ideas, we though it would be interesting, so we fully developed it through thorough investigations on the outcomes it produced.\n",
    "\n",
    "<div align=\"center\"><img src=\"./asset/visual-prompting-detail.png\"/></div>\n",
    "\n",
    "The main issue is making sure that spatial structure is preserved while not deviating CLIP's attention to the wrong objects. In the beginning, we were only using visual markers to shift CLIP's focus, but it was not enough for overly crowded scenes. Therefore, we decided to smooth things out by blurring the background and keeping the rectangular window around a region proposal sharp. (We also tried other window shapes, for instance ellipses and rounded-corner rectangles, but we found straight rectangles to work the best.)\n",
    "\n",
    "This allows, as it can be seen in the previous image, to leverage both CLIP's spatial reasoning abilities and natural language understanding capabilities.\n",
    "\n",
    "In addition, we found that generating two distinct proposals for each region proposal helps improving the overall performance. More precisely, we tried the following modifications:\n",
    "1. Adding a visual marker\n",
    "2. Adding a visual marker and blurring\n",
    "3. Adding a visual marker and gray-scaling the rest of the image\n",
    "4. Adding a visual marker and gray-scaling and blurring the rest of the image\n",
    "\n",
    "As mentioned earlier, we found that background blurring plays a key role in improving the performance, therefore we decided to only use modifications 2 and 4.\n",
    "\n",
    "**This resembles the way our vision system works: we can perceive the whole image, but we can truly focus only on a small region.**\n",
    "\n",
    "To summarize, if there are $m$ object proposals, we have to send through CLIP $2 \\times m$ images, corresponding to the augmented prompts.\n",
    "\n",
    "After extensive tests, we found the following parameters to work the best:\n",
    "* using a stroke color of red for visual markers;\n",
    "* using a stroke width of 3 px for visual markers;\n",
    "* using a blur radius of 20 px, with a Gaussian blur being applied;\n",
    "* masking using rectangles and using ellipses as visual markers.\n",
    "\n",
    "We also tried the following hyperparameters:\n",
    "* Stroke color: red, orange, purple, yello, green, blue, none (transparent, to hide it).\n",
    "* Stroke width: 1 to 10 px, with all intermediate integer values.\n",
    "* Blur radius: 0 px (no blur), 1 px, 1.5 px, 2 px, 5 px, 10 px, 20 px, 25 px and 50 px.\n",
    "\n",
    "Testing all these hyperparameters combinations is challenging. Considering that there is also the backbone model playing a crucial role and that it could interfere with some settings, we had to try them in a controlled way to understand what combination works the best. Therefore, we sampled approximately 100 samples from the dataset and we tested about fifty distinct combinations on them. We manually examined all the outcomes to understand how the model was behaving. In the end, after all these thorough tests, we decided to use the configuration we proposed earlier.\n",
    "\n",
    "---\n",
    "\n",
    "To further improve the performance, we wanted to find a way of discouraging proposals which are unlikely to be the correct ones. We thought of the following two ideas, which we implemented and tested on the whole test test:\n",
    "* Disallowing region proposals whose area is larger than $a\\%$ of the area of the given image. We set $a = 80\\%$, thus prohibiting region proposals spanning at least $80\\%$ of the given image. This is a thoughtful idea as most of the objects referred to by prompts are actually a lot smaller than the overall image they are contained in. At the same time, region proposal algorithms/models (such as the YOLO we are using) sometimes find almos the whole image as a proposal: for instance, if the photo depicts a table with some objects on it, YOLO would also find the whole table, thus generating a proposal that contains almost all the original image and which will likely get the highest score for this reason. This approach may lead to wrong results in some edge cases, when the actual object to be found is (almost) the whole image. Nevertheless, the overall loss due to this fact is negligible considering how rare it is to find such a situation. However, we found this technique not to significantly improve the overall performance, so we decided *not* to leave it in the final proposal. Nevertheless, the lines of code that deas with it can be uncommented to reintroduce it.\n",
    "* Weighting the region proposals by their content. That is, we used the label provided by YOLO (_e.g._ 'person') to create novel prompts (_i.e._, 'A picture of a {category}'), which we will refer to as \"category prompts\". We then encoded all these prompts (80 in total, as the number of categories recognized by YOLO) using the CLIP encoder. When a region was proposed, we weighted it by the similarity between the category prompt and the given prompt. For instance, the prompt \"An orange in the bowl\" will have a higher similarity with the category prompt \"A picture of a orange\" than to \"A picture of a cat\". Therefore, if the image contains both an orange and a cat, the cat region will be assigned a lower likelihood. In this way, we are basically computing a prior probability of a region to be correct one.\n",
    "\n",
    "---\n",
    "\n",
    "All this information has to be combined in order to predict a single bounding box for the given pair (image, prompt). We tried different \"ensembling\" methods, however we found the following to work best:\n",
    "* computing the mean between the cosine similarities of the visual prompts referring to the same proposal. We also tried getting the max or the median (when we tried with 3+ visual prompts for each proposal);\n",
    "* extracting the maximum cosine similarity value across all proposals to find the best-matching one.\n",
    "\n",
    "In addition we found evidence that CLIP is biased towards certain categories of objects, perhaps because it was exposed more to them during pre-training time. This conjecture/speculation is also supported by some of the papers we have studied. To overcome this issue and gaining some more accuracy, a \"de-biasing\" technique can be applied. This speculation is also supported by some papers, which proposed some techniques to address it.\n",
    "\n",
    "We decided to sample $l$ (in the end, $l = 5000$) random prompts from the trainig set and encoding them using CLIP's encoder. Then, during inference time, we compute the cosine similarity between all region proposals, the given prompt(s) for the image and all these \"de-biasing\" prompts. We therefore get a huge matrix, whose size is $(\\#\\textit{prompts} + \\#\\textit{de-biasing prompts}) \\times \\#\\textit{visual prompts}$. We compute the average on a per-column basis, so we get $\\#\\textit{visual prompts}$ average values. We drop all the rows (corresponding to the prompts) but the first $\\#\\textit{prompts}$ one, which are those referring to the actually given prompts. Then, to each of these remaining rows, we subtract the average computed earlier. This helps de-biasing the result.\n",
    "\n",
    "To speed up computations, we also tried to pre-compute the value of the de-biasing terms and even to learn them, but it did not to satisfactory results: the best practice, from a mere metric performance result, is achieved by doing as explained above. However, depending on the application, we suggest to take into account the idea of pre-computing these values or to train a network to predict them. In either case - and even in the one we have decided to use - we consider it to be a sort of training: we are exploiting data and information to improve performance on unseen data.\n",
    "\n",
    "To summarize, these are the main components of our architecture:\n",
    "* visual prompt editing to guide CLIP towards our goal;\n",
    "* textual prompt editing to help CLIP find better matches;\n",
    "* using prior knowledge to improve the performance on novel, unseen data.\n",
    "\n",
    "The following cell contains the code of our implementation. We present a general framework which we then adapated to work both with YOLOv5 and the recently-released YOLOv8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "\n",
    "class CirclesModel(nn.Module):\n",
    "    def __init__(self, device=None, models=None, preprocesses=None, yolo_models=None, text_features_bias=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a CirclesModel. Note that it does not actually work, as it acts as an abstract class, which\n",
    "        has to be subclassed in order to work properly with different YOLO versions.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        if not models or not preprocesses:\n",
    "            raise ValueError('Models and preprocesses for CLIP model should be provided')\n",
    "\n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "\n",
    "        self.clip_backbones = list(self.models.keys())\n",
    "        \n",
    "        if not yolo_models:\n",
    "            raise ValueError('Models for YOLO should be provided')\n",
    "        self.yolo_models = yolo_models\n",
    "\n",
    "        # De-biasing features\n",
    "        if not text_features_bias:\n",
    "            self.text_features_bias = {}\n",
    "            for backbone in self.clip_backbones:\n",
    "                text_features_bias[backbone] = None\n",
    "        else:\n",
    "            self.text_features_bias = text_features_bias\n",
    "\n",
    "        # How many visual prompts are generated for each image\n",
    "        self.visual_augmentation = 2\n",
    "\n",
    "    def forward(self, indices, images, prompts_list):\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        # -- Getting the right data and moving it to the correct device --\n",
    "\n",
    "        # Images remain on the CPU because they are PIL Images, not Tensors\n",
    "        # Converting to Tensors leads to errors with YOLO\n",
    "        images = [images[i] for i in indices]\n",
    "\n",
    "        prompts_list = [prompts_list[i] for i in indices]\n",
    "        prompts_list = self.update_prompts_with_this_is(prompts_list)\n",
    "        prompts_tensor = [clip.tokenize(prompt_list).to(self.device) for prompt_list in prompts_list]\n",
    "\n",
    "        # -- Actual processing --\n",
    "\n",
    "        bounding_boxes = self.get_bounding_boxes(images)\n",
    "\n",
    "        # It contains the predicted bounding box for each image for each prompt\n",
    "        # Then, it is a list of length len(images) and for each entry there is a\n",
    "        # list with len(prompts[i]), where i is the i-th image \n",
    "        overall_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, prompts_tensor_for_sample in enumerate(prompts_tensor):\n",
    "                # Image crops\n",
    "                image_crops, bounding_boxes[idx] = self.get_visual_prompts(images[idx], bounding_boxes[idx])\n",
    "\n",
    "                preprocessed_image_crops = {}\n",
    "                for backbone in self.clip_backbones:\n",
    "                    preprocessed_image_crops[backbone] = torch.stack([self.preprocesses[backbone][self.device_index](image) for image in image_crops]).to(self.device)\n",
    "\n",
    "                similarities = {}\n",
    "                for backbone in self.clip_backbones:\n",
    "                    pic_batches = np.array_split(range(len(preprocessed_image_crops[backbone])), len(preprocessed_image_crops[backbone]) // self.visual_augmentation)\n",
    "                    \n",
    "                    visual_features = []\n",
    "                    for pic_batch in pic_batches:\n",
    "                        visual_features.append(self.models[backbone][self.device_index].encode_image(preprocessed_image_crops[backbone][pic_batch]))\n",
    "                    visual_features = torch.cat(visual_features)\n",
    "\n",
    "                    text_features = self.models[backbone][self.device_index].encode_text(prompts_tensor_for_sample)\n",
    "\n",
    "                    if self.text_features_bias[backbone] is not None:\n",
    "                        text_features = torch.cat([text_features, self.text_features_bias[backbone].to(self.device)])\n",
    "\n",
    "                    similarities[backbone] = cosine_similarity(visual_features, text_features)\n",
    "\n",
    "\n",
    "                similarity = torch.empty_like(similarities[self.clip_backbones[0]])\n",
    "                for prompt_idx in range(similarity.shape[0]):\n",
    "                    for proposal_idx in range(similarity.shape[1]):\n",
    "                        similarity[prompt_idx, proposal_idx] = torch.mean(torch.stack([\n",
    "                            similarities[backbone][prompt_idx, proposal_idx] for backbone in self.clip_backbones\n",
    "                        ]))\n",
    "\n",
    "\n",
    "                average = similarity.mean(dim=0)\n",
    "                scores = (similarity - average)[range(len(prompts_tensor_for_sample))]\n",
    "\n",
    "                final_scores = torch.empty((scores.shape[0], scores.shape[1] // self.visual_augmentation))\n",
    "\n",
    "                for prompt_idx in range(final_scores.shape[0]):\n",
    "                    for final_score_idx, proposal_idx in enumerate(range(0, scores.shape[1], self.visual_augmentation)):\n",
    "                        final_scores[prompt_idx, final_score_idx] = torch.max(torch.stack([\n",
    "                            scores[prompt_idx, proposal_idx + i] for i in range(self.visual_augmentation)\n",
    "                        ]))\n",
    "\n",
    "                \n",
    "                _, max_indices = final_scores.max(dim=-1)\n",
    "                try:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            (bounding_boxes[idx][max_idx, 0:4]).to(self.device)\n",
    "                        )\n",
    "                except:\n",
    "                    for max_idx in max_indices:\n",
    "                        overall_outputs.append(\n",
    "                            torch.tensor((0, 0, images[idx].size[0], images[idx].size[1])).to(self.device)\n",
    "                        )\n",
    "\n",
    "        return torch.stack(overall_outputs)\n",
    "\n",
    "    def update_prompts_with_this_is(self, prompts):\n",
    "        \"\"\"\n",
    "        Update the textual prompts by adding 'This is' at the beginning.\n",
    "        This seems to help in guiding CLIP to focus on the correct part\n",
    "        of the visual prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        return [['This is ' + prompt for prompt in sample] for sample in prompts]\n",
    "\n",
    "    def get_bounding_boxes(self, pil_images):\n",
    "        pass\n",
    "\n",
    "    def get_image_with_marker(self, image, bbox, stroke_color='red', stroke_width=1):\n",
    "        \"\"\"\n",
    "        Add a visual marker to the image at the position specified by the\n",
    "        bounding box (bbox), which is expected to be in the format\n",
    "        (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n",
    "        \"\"\"\n",
    "        \n",
    "        result = image.copy()\n",
    "        draw = ImageDraw.Draw(result)\n",
    "        draw.ellipse(bbox, outline=stroke_color, width=stroke_width)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_image_with_marker_and_blur(self, image, bbox, stroke_color='red', stroke_width=1, blur_radius=1):\n",
    "        \"\"\"\n",
    "        Add a visual marker to the image at the position specified by the\n",
    "        bounding box (bbox), which is expected to be in the format\n",
    "        (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n",
    "        The background is then blurred.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = image.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n",
    "        mask = Image.new('L', image.size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.rectangle(bbox, fill=255)\n",
    "        result.paste(image, mask=mask)\n",
    "        draw = ImageDraw.Draw(result)\n",
    "        draw.ellipse(bbox, outline=stroke_color, width=stroke_width)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def get_image_with_marker_and_grayscale(self, image, bbox, stroke_color='red', stroke_width=1):\n",
    "        \"\"\"\n",
    "        Add a visual marker to the image at the position specified by the\n",
    "        bounding box (bbox), which is expected to be in the format\n",
    "        (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n",
    "        The background is grayscaled.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = image.convert('L').convert('RGB')\n",
    "        mask = Image.new('L', image.size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.rectangle(bbox, fill=255)\n",
    "        result.paste(image, mask=mask)\n",
    "        draw = ImageDraw.Draw(result)\n",
    "        draw.ellipse(bbox, outline=stroke_color, width=stroke_width)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_image_with_marker_and_blur_grayscale(self, image, bbox, stroke_color='red', stroke_width=1, blur_radius=1):\n",
    "        \"\"\"\n",
    "        Add a visual marker to the image at the position specified by the\n",
    "        bounding box (bbox), which is expected to be in the format\n",
    "        (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n",
    "        The background is both grayscaled and blurred.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = image.filter(ImageFilter.GaussianBlur(radius=blur_radius)).convert('L').convert('RGB')\n",
    "        mask = Image.new('L', image.size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.rectangle(bbox, fill=255)\n",
    "        result.paste(image, mask=mask)\n",
    "        draw = ImageDraw.Draw(result)\n",
    "        draw.ellipse(bbox, outline=stroke_color, width=stroke_width)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_visual_prompts(self, image, bounding_boxes):\n",
    "        self.visual_augmentation = 2\n",
    "        visual_prompts = []\n",
    "        keep_bbox = []\n",
    "\n",
    "        if bounding_boxes is None:\n",
    "            return [image] * self.visual_augmentation, bounding_boxes\n",
    "\n",
    "        # Setting the parameters for the visual markers\n",
    "        stroke_color = 'red'\n",
    "        stroke_width = 3\n",
    "        blur_radius = 20\n",
    "\n",
    "        for idx, bounding_box in enumerate(bounding_boxes if bounding_boxes is not None else []):\n",
    "            bounding_box = (bounding_box[0].item(), bounding_box[1].item(), bounding_box[2].item(), bounding_box[3].item())\n",
    "\n",
    "            # For the following line to work correctly bounding boxes should actually be removed from\n",
    "            # YOLO's results, as that's what is actually used in the end\n",
    "            # if (bounding_box[2] - bounding_box[0]) * (bounding_box[3] - bounding_box[1]) < (image.size[0] * image.size[1]) * 0.8:\n",
    "            #     continue\n",
    "\n",
    "            # If the previous condition was uncommented, this line would not execute\n",
    "            # for boxes covering more than 80% of the area of the image, thus they\n",
    "            # would be removed from the results\n",
    "            keep_bbox += [idx]\n",
    "\n",
    "            # Uncomment the following lines to add or remove visual markers.\n",
    "            # Remember to update `self.visual_augmentation` to match the number\n",
    "            # of visual prompts that are generated for each region proposal.\n",
    "            bbox_visual_prompts = [\n",
    "                # self.get_image_with_marker(image, bounding_box, stroke_color=stroke_color, stroke_width=stroke_width),\n",
    "                self.get_image_with_marker_and_blur(image, bounding_box, stroke_color=stroke_color, stroke_width=stroke_width, blur_radius=blur_radius),\n",
    "                # self.get_image_with_marker_and_grayscale(image, bounding_box, stroke_color=stroke_color, stroke_width=stroke_width),\n",
    "                self.get_image_with_marker_and_blur_grayscale(image, bounding_box, stroke_color=stroke_color, stroke_width=stroke_width, blur_radius=blur_radius),\n",
    "            ]\n",
    "\n",
    "            for el in bbox_visual_prompts:\n",
    "                visual_prompts.append(el)\n",
    "\n",
    "        # if self.device_index == 0:\n",
    "        #     print('From', len(bounding_boxes))\n",
    "        bounding_boxes = bounding_boxes[keep_bbox]\n",
    "        # if self.device.index == 0:\n",
    "        #     print('To', len(bounding_boxes))\n",
    "\n",
    "        if len(visual_prompts) == 0:\n",
    "            # If no region proposal, return the whole image.\n",
    "            # It is inserted as many times as each region would\n",
    "            # be augmented to ensure consistency in the algorithm\n",
    "            for _ in range(self.visual_augmentation):\n",
    "                visual_prompts.append(image)\n",
    "                \n",
    "        return visual_prompts, bounding_boxes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we define two concrete implementation of the CirclesModel which rely on different YOLO versions, namely YOLOv5 and YOLOv8.\n",
    "\n",
    "We initially utilized YOLOv5, however we decided to move to YOLOv8 as we noticed improved performance. YOLOv8 is based on the latest research papers and is thus able to work much better than YOLOv5. In particular, we noticed that in some cases YOLOv5 was *not* able to find the correct region proposals nor was it always accurate. YOLOv8, on the other hand, performs much better thanks to the improvements made to its architecture and training phase.\n",
    "\n",
    "Replacing YOLOv5 with YOLOv8 allows us to get an average $+4\\%$ on our task.\n",
    "\n",
    "To make fair comparisons and to avoid unfair unbalances, we used the \"small\" version of YOLO in either case (that is, YOLOv5s and YOLOv8s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CirclesModelYOLOv5(CirclesModel):\n",
    "    def get_bounding_boxes(self, pil_images):\n",
    "        bounding_boxes = self.yolo_models[self.device_index](pil_images)\n",
    "        return bounding_boxes.pred\n",
    "\n",
    "class CirclesModelYOLOv8(CirclesModel):\n",
    "    def get_bounding_boxes(self, pil_images):\n",
    "        bounding_boxes = self.yolo_models[self.device_index].predict(pil_images, verbose=False)\n",
    "        bounding_boxes = [torch.cat([box.xyxy for box in res.boxes]) if res.boxes else None for res in bounding_boxes]\n",
    "        return bounding_boxes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we have to load YOLOv8. We also check whether YOLOv5 was already loaded and, if necessary, we load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    yolo_models\n",
    "except:\n",
    "    if torch.cuda.is_available():\n",
    "        yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    yolo_v8_models = [YOLO('yolov8s.pt') for i in range(torch.cuda.device_count())]\n",
    "else:\n",
    "    yolo_v8_models = [YOLO('yolov8s.pt')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we can load CLIP model(s). We have decided to stick to the ResNet backbone (RN50x16) as the assignment asked to use it, if possible. Nevertheless, we tried our architecture with different backbones and the next section presents the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "# The following list is used to choose which CLIP versions to load.\n",
    "# If multiple backbones are chosen, they are all loaded and the models\n",
    "# will use all of them as an ensemble.\n",
    "# While this may improve the metric performance by a tiny fraction, it\n",
    "# dramatically slows down the computation, as all the calculations have\n",
    "# to be repeated for all backbones and eventually ensembled.\n",
    "clip_backbones = [\n",
    "    'RN50x16',\n",
    "    # 'RN50x64',\n",
    "    # 'ViT-B/16',\n",
    "    # 'ViT-L/14@336px',\n",
    "    # 'ViT-B/32',\n",
    "]\n",
    "\n",
    "models, preprocesses = {}, {}\n",
    "\n",
    "for clip_backbone in clip_backbones:\n",
    "    models[clip_backbone] = []\n",
    "    preprocesses[clip_backbone] = []\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            model, preprocess = clip.load(clip_backbone, device=f'cuda:{i}')\n",
    "            \n",
    "            models[clip_backbone].append(model)\n",
    "            preprocesses[clip_backbone].append(preprocess)\n",
    "    else:\n",
    "        model, preprocess = clip.load(clip_backbone, device=device)\n",
    "        models[clip_backbone].append(model)\n",
    "        preprocesses[clip_backbone].append(preprocess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before instantiating the models, we have to load debiasing features. This is what the following cell does.\n",
    "\n",
    "Debiasing features are prompts which are randomly sampled from the training set. The more, the better - in general. The more, the higher the compute time, therefore a good tradeoff has to be found. We decided to use 5000 randomly sampled prompts.\n",
    "\n",
    "As exaplined earlier, the core idea is to compute the cosine similarity between all the region proposals and the text prompts (which are both the ones given for the image and the debiasing ones). Then, the average is computed column-wise (columns contain region proposals). All the rows but the ones corresponding to the actually given prompts are discarded, thus reducing greatly the dimensionality of the cosine similarity matrix, and the average computed earlier is subtracted to each row to get the aforementioned debias. This idea was proposed by Shtedritski et al. (see the References section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -- Step 1 --\n",
    "# Loading the training set\n",
    "all_prompts = []\n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(train_loader):\n",
    "    refined_prompts = ['This is ' + prompt for sample in prompts for prompt in sample]\n",
    "\n",
    "    for prompt in refined_prompts:\n",
    "        all_prompts.append(prompt)\n",
    "\n",
    "# -- Step 2 --\n",
    "# Choosing 5000 random prompts\n",
    "# Note that even setting the random seed does not ensure to get the very\n",
    "# same prompts, since the DataLoader for the training set shuffles the data,\n",
    "# thus loading it in different ways every time.\n",
    "np.random.seed = 42\n",
    "randomly_sampled_examples = np.random.choice(all_prompts, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Step 3 --\n",
    "# Encoding the prompts using CLIP\n",
    "text_features_full = {}\n",
    "\n",
    "text = clip.tokenize(randomly_sampled_examples).to(device)\n",
    "\n",
    "for clip_backbone in clip_backbones:\n",
    "    encoder = models[clip_backbone][0]\n",
    "\n",
    "    # All the prompts are split into 10 mini-batches, as loading all\n",
    "    # them onto the GPU at the same time may cause out-of-memory errors.\n",
    "    # Splitting into 10 smaller batches should prevent this problem\n",
    "    # for reasonably sized debiasing sets.\n",
    "    # If there are still issues due to low-capacity GPUs, the number of\n",
    "    # batches shall be increased\n",
    "    indices = range(text.shape[0])\n",
    "    batches = np.array_split(indices, 10)\n",
    "\n",
    "    text_features_full[clip_backbone] = []\n",
    "\n",
    "    for batch in batches:\n",
    "        with torch.no_grad():\n",
    "            text_features_full[clip_backbone].append(encoder.encode_text(text[batch]))\n",
    "\n",
    "# -- Step 4 --\n",
    "# Concatenate all prompt features to form a matrix with as many rows as the\n",
    "# number of randomly sampled prompts and as many columns as the embedding\n",
    "# dimensionality\n",
    "for clip_backbone in clip_backbones:\n",
    "    text_features_full[clip_backbone] = torch.cat(text_features_full[clip_backbone])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_model_yolo_v5 = CirclesModelYOLOv5(models=models, preprocesses=preprocesses, yolo_models=yolo_models, text_features_bias=text_features_full)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    circles_model_yolo_v5 = torch.nn.DataParallel(circles_model_yolo_v5)\n",
    "\n",
    "circles_model_yolo_v8 = CirclesModelYOLOv8(models=models, preprocesses=preprocesses, yolo_models=yolo_v8_models, text_features_bias=text_features_full)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    circles_model_yolo_v8 = torch.nn.DataParallel(circles_model_yolo_v8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the evaluator with the same CLIP backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_model = Evaluator(models=models, preprocesses=preprocesses)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    evaluator_model = torch.nn.DataParallel(evaluator_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can eventually test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch index: 0 --\n",
      "-- Batch index: 1 --\n",
      "-- Batch index: 2 --\n",
      "-- Batch index: 3 --\n",
      "-- Batch index: 4 --\n",
      "-- Batch index: 5 --\n",
      "-- Batch index: 6 --\n",
      "-- Batch index: 7 --\n",
      "-- Batch index: 8 --\n",
      "-- Batch index: 9 --\n",
      "-- Batch index: 10 --\n",
      "-- Batch index: 11 --\n",
      "-- Batch index: 12 --\n",
      "-- Batch index: 13 --\n",
      "-- Batch index: 14 --\n",
      "-- Batch index: 15 --\n",
      "-- Batch index: 16 --\n",
      "-- Batch index: 17 --\n",
      "-- Batch index: 18 --\n",
      "-- Batch index: 19 --\n",
      "-- Batch index: 20 --\n",
      "-- Batch index: 21 --\n",
      "-- Batch index: 22 --\n",
      "-- Batch index: 23 --\n",
      "-- Batch index: 24 --\n",
      "-- Batch index: 25 --\n",
      "-- Batch index: 26 --\n",
      "-- Batch index: 27 --\n",
      "-- Batch index: 28 --\n",
      "-- Batch index: 29 --\n",
      "-- Batch index: 30 --\n",
      "-- Batch index: 31 --\n",
      "-- Batch index: 32 --\n",
      "-- Batch index: 33 --\n",
      "-- Batch index: 34 --\n",
      "-- Batch index: 35 --\n",
      "-- Batch index: 36 --\n",
      "-- Batch index: 37 --\n",
      "-- Batch index: 38 --\n",
      "-- Batch index: 39 --\n",
      "-- Batch index: 40 --\n",
      "-- Batch index: 41 --\n",
      "-- Batch index: 42 --\n",
      "-- Batch index: 43 --\n",
      "-- Batch index: 44 --\n",
      "-- Batch index: 45 --\n",
      "-- Batch index: 46 --\n",
      "-- Batch index: 47 --\n",
      "-- Batch index: 48 --\n",
      "-- Batch index: 49 --\n",
      "-- Batch index: 50 --\n",
      "-- Batch index: 51 --\n",
      "-- Batch index: 52 --\n",
      "-- Batch index: 53 --\n",
      "-- Batch index: 54 --\n",
      "-- Batch index: 55 --\n",
      "-- Batch index: 56 --\n",
      "-- Batch index: 57 --\n",
      "-- Batch index: 58 --\n",
      "-- Batch index: 59 --\n",
      "-- Batch index: 60 --\n",
      "-- Batch index: 61 --\n",
      "-- Batch index: 62 --\n",
      "-- Batch index: 63 --\n",
      "-- Batch index: 64 --\n",
      "-- Batch index: 65 --\n",
      "-- Batch index: 66 --\n",
      "-- Batch index: 67 --\n",
      "-- Batch index: 68 --\n",
      "-- Batch index: 69 --\n",
      "-- Batch index: 70 --\n",
      "-- Batch index: 71 --\n",
      "-- Batch index: 72 --\n",
      "-- Batch index: 73 --\n",
      "-- Batch index: 74 --\n",
      "-- Batch index: 75 --\n",
      "-- Batch index: 76 --\n",
      "-- Batch index: 77 --\n",
      "-- Batch index: 78 --\n",
      "--- Metrics ---\n",
      "Mean Intersection over Union (mIoU): 0.5064360232455498\n",
      "Accuracy: 0.4830508474576271\n",
      "Mean Cosine Similarity: 0.8853325278072034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5064360232455498, 0.4830508474576271, 0.8853325278072034)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(test_loader, circles_model_yolo_v8, evaluator_model, device, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We present, as the main results of this project, the outcomes that we obtained from the latest model/architecture that we have introduced.\n",
    "\n",
    "| Backbone | Mean Intersection over Union | Accuracy | Cosine Similarity |\n",
    "| -------- | ---------------------------- | -------- | ----------------- |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## 1\n",
    "\n",
    "We implemented the baseline and all the models using YOLOv5, downloaded from TorcHub. In the end, when the final architecture was completed, we decided to try YOLOv8, the latest iteration of the YOLO family which should perform better. Indeed, we noticed that in some cases YOLOv5 was *not* able to propose any (meaningful) regions.\n",
    "\n",
    "We therefore switched from YOLOv5 (small model) to YOLOv8 (small model, for comparable results). We found that it provides an overall +4% boost, both on the baseline and on the latter architecture we have proposed.\n",
    "\n",
    "We also thought of using other region-proposal methods, such as Fast R-CNN, Faster R-CNN and the Detectron2 suite by FAIR. However, we decided to stick to YOLO for the following reasons:\n",
    "* results are comparable across different architectures;\n",
    "* YOLO, in the small version we are using, is fast enough for the task;\n",
    "* YOLO was trained on the Coco dataset, thus being very accurate in proposing regions.\n",
    "\n",
    "Nonetheless, other solutions as mentioned above may be employed to allow for faster inference times or not to rely on YOLO.\n",
    "\n",
    "## 2\n",
    "\n",
    "In the end, we mostly proposed training-free architectures. We did so for the following reasons:\n",
    "1. CLIP was trained on a huge dataset, therefore it should have a good representation of concepts both in the visual and the textual domain (which are then brought to the same latent space). We wanted to discover whether this assertion is accurate and how much and we did not want to exacerbate or \"ruin\" CLIP itself: instead, we wanted to leverage its capabilities.\n",
    "2. Training requires lots of resources. We would not have been able to train large models, for instance the Graph Neural Network one, as we intended. We could only train relatively small models (see previous sections) just to test some more basic ideas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)\n",
    "* [CLIP-Lite: Information Efficient Visual Representation Learning with Language Supervision](https://arxiv.org/abs/2112.07133)\n",
    "* [Adapting CLIP For Phrase Localization Without Further Training](https://arxiv.org/pdf/2204.03647.pdf)\n",
    "* [Zero-shot Referring Image Segmentation with Global-Local Context Features](https://arxiv.org/pdf/2303.17811.pdf)\n",
    "* [Hierarchical Local-Global Transformer for Temporal Sentence Grounding](https://arxiv.org/pdf/2208.14882.pdf)\n",
    "* [[CLS] Token is All You Need for Zero-Shot Semantic Segmentation](https://arxiv.org/pdf/2304.06212.pdf)\n",
    "* [What does CLIP know about a red circle? Visual prompt engineering for VLMs](https://arxiv.org/pdf/2304.06712.pdf)\n",
    "* [CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching](https://arxiv.org/pdf/2303.13076.pdf)\n",
    "* [ActBERT: Learning Global-Local Video-Text Representations](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_ActBERT_Learning_Global-Local_Video-Text_Representations_CVPR_2020_paper.pdf)\n",
    "* [A Local-to-Global Approach to Multi-modal Movie Scene Segmentation](https://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_A_Local-to-Global_Approach_to_Multi-Modal_Movie_Scene_Segmentation_CVPR_2020_paper.pdf)\n",
    "* [CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings](https://www.jair.org/index.php/jair/article/view/13689/26825)\n",
    "* [Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs](https://arxiv.org/pdf/2212.00785.pdf)\n",
    "* [CLIP-Event: Connecting Text and Images with Event Structures](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf)\n",
    "* [STAIR: Learning Sparse Text and Image Representation in Grounded Tokens](https://arxiv.org/pdf/2301.13081.pdf)\n",
    "* [Learning Generalized Zero-Shot Learners for Open-Domain Image Geolocalization](https://arxiv.org/pdf/2302.00275.pdf)\n",
    "* [CRIS: CLIP-Driven Referring Image Segmentation](https://arxiv.org/pdf/2111.15174.pdf)\n",
    "* [LAVT: Language-Aware Vision Transformer for Referring Image Segmentation](https://arxiv.org/pdf/2112.02244.pdf)\n",
    "* [Weakly-supervised segmentation of referring expressions](https://arxiv.org/pdf/2205.04725.pdf)\n",
    "* [Focusing on Targets for Improving Weakly Supervised Visual Grounding](https://arxiv.org/pdf/2302.11252v1.pdf)\n",
    "* [Fine-tuned CLIP Models are Efficient Video Learners](https://arxiv.org/pdf/2212.03640.pdf)\n",
    "* [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/pdf/2111.09734)\n",
    "* [Simple but Effective: CLIP Embeddings for Embodied AI](https://openaccess.thecvf.com/content/CVPR2022/papers/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.pdf)\n",
    "* [What does CLIP know about a red circle? Visual prompt engineering for VLMs](https://arxiv.org/pdf/2304.06712.pdf)\n",
    "* [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "* [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n",
    "* [Detectron2](https://ai.facebook.com/tools/detectron2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
