{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class RefCocoG_Dataset(Dataset):\n",
    "    full_annotations = None\n",
    "\n",
    "    def __init__(self, root_dir, annotations_f, instances_f, split='train', transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations_f = annotations_f\n",
    "        self.instances_f = instances_f\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.get_annotations()\n",
    "        self.image_names = list([\n",
    "            self.annotations[id]['image']['actual_file_name']\n",
    "            for id in self.annotations\n",
    "        ])\n",
    "\n",
    "    def get_annotations(self):\n",
    "        if RefCocoG_Dataset.full_annotations:\n",
    "            self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, RefCocoG_Dataset.full_annotations.items()))\n",
    "            return\n",
    "\n",
    "        # Load pickle data\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.annotations_f), 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "\n",
    "        # Load instances\n",
    "        with open(os.path.join(self.root_dir, 'annotations', self.instances_f), 'rb') as file:\n",
    "            self.instances = json.load(file)\n",
    "\n",
    "        # Match data between the two files and build the actual dataset\n",
    "        self.annotations = {}\n",
    "\n",
    "        images_actual_file_names = {}\n",
    "        for image in self.instances['images']:\n",
    "            images_actual_file_names[image['id']] = image['file_name']\n",
    "\n",
    "        for image in self.data:\n",
    "            if image['ann_id'] not in self.annotations:\n",
    "                self.annotations[image['ann_id']] = {}\n",
    "\n",
    "            self.annotations[image['ann_id']]['image'] = image\n",
    "            self.annotations[image['ann_id']]['image']['actual_file_name'] = images_actual_file_names[image['image_id']]\n",
    "\n",
    "        for annotation in self.instances['annotations']:\n",
    "            if annotation['id'] not in self.annotations:\n",
    "                continue\n",
    "\n",
    "            self.annotations[annotation['id']]['annotation'] = annotation\n",
    "\n",
    "        # Keep only samples from the given split\n",
    "        RefCocoG_Dataset.full_annotations = self.annotations\n",
    "        self.annotations = dict(filter(lambda match: match[1]['image']['split'] == self.split, self.annotations.items()))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def corner_size_to_corners(self, bounding_box):\n",
    "        \"\"\"\n",
    "        Transform (top_left_x, top_left_y, width, height) bounding box representation\n",
    "        into (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            bounding_box[0],\n",
    "            bounding_box[1],\n",
    "            bounding_box[0] + bounding_box[2],\n",
    "            bounding_box[1] + bounding_box[3]\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image name at the given index\n",
    "        image_name = self.image_names[idx]\n",
    "\n",
    "        # Load the image file as a PIL image\n",
    "        image = Image.open(os.path.join(self.root_dir, 'images', image_name)).convert('RGB')\n",
    "        # image = read_image(os.path.join(self.root_dir, 'images', image_name), ImageReadMode.RGB)\n",
    "        \n",
    "        image_id = list(self.annotations)[idx]\n",
    "\n",
    "        # print(image_id)\n",
    "\n",
    "        # Get the caption for the image\n",
    "        prompts = [\n",
    "            prompt['sent'] for prompt in self.annotations[image_id]['image']['sentences']\n",
    "        ]\n",
    "\n",
    "        # Get the bounding box for the prompts for the image\n",
    "        bounding_box = self.corner_size_to_corners(self.annotations[image_id]['annotation']['bbox'])\n",
    "\n",
    "        # Apply the transform if given\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = [\n",
    "            image,\n",
    "            bounding_box,\n",
    "            prompts,\n",
    "        ]\n",
    "\n",
    "        # Return the sample as a list\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='train')\n",
    "dataset_val = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='val')\n",
    "dataset_test = RefCocoG_Dataset('refcocog', 'refs(umd).p', 'instances.json', split='test')\n",
    "\n",
    "dataset_splits = [\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RefCocoG_Dataset.full_annotations), len(dataset_train.annotations), len(dataset_val.annotations), len(dataset_test.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class RefCocoG_Dataset_Preprocessed(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split='train') -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.data = pd.read_csv(os.path.join(self.root_dir, f'{self.split}_data.csv'))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :-4]), torch.tensor(self.data.iloc[idx, -4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed_train = RefCocoG_Dataset_Preprocessed('preprocessed', split='train')\n",
    "dataset_preprocessed_test = RefCocoG_Dataset_Preprocessed('preprocessed', split='test')\n",
    "\n",
    "dataset_splits = [\n",
    "    dataset_preprocessed_train,\n",
    "    dataset_preprocessed_test,\n",
    "    dataset_preprocessed_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_differently_sized_prompts(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    bboxes = [item[1] for item in batch]\n",
    "    prompts = [item[2] for item in batch]\n",
    "    \n",
    "    return list(images), list(bboxes), list(prompts)\n",
    "\n",
    "def get_data(dataset_splits, batch_size=64, test_batch_size=256, num_workers=0, collate_fn=None):\n",
    "    training_data = dataset_splits[0]\n",
    "    validation_data = dataset_splits[1]\n",
    "    test_data = dataset_splits[2]\n",
    "\n",
    "    # Change shuffle to True for train\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data(dataset_splits, batch_size=64, test_batch_size=512, num_workers=0)#, collate_fn=collate_differently_sized_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # First GPU\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "# else:\n",
    "#     yolo_models = [torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "models, preprocesses = [], []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        model, preprocess = clip.load(\"ViT-B/32\", device=f'cuda:{i}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preprocesses.append(preprocess)\n",
    "else:\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    models.append(model)\n",
    "    preprocesses.append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(models[0].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Cosine Similarity\n",
    "\n",
    "    Normalizes both tensors a and b. Returns <b, a.T> (inner product).\n",
    "    \"\"\"\n",
    "\n",
    "    a_norm = a / a.norm(dim=-1, keepdim=True)\n",
    "    b_norm = b / b.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (b_norm @ a_norm.T)\n",
    "\n",
    "    return similarity.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualise_scores(scores: torch.Tensor, images, texts: list[str]):\n",
    "    for t_idx, text in enumerate(texts):\n",
    "        for i_idx, image in enumerate(images):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f'Score: {scores[t_idx, i_idx]} / Prompt: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class RegressorModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.clip_out_features = 512\n",
    "\n",
    "#         self.fc1 = nn.Linear(2 * self.clip_out_features, 4 * self.clip_out_features, bias=False).double()\n",
    "#         self.fc2 = nn.Linear(4 * self.clip_out_features, 8 * self.clip_out_features, bias=False).double()\n",
    "#         self.fc3 = nn.Linear(8 * self.clip_out_features, 2 * self.clip_out_features, bias=False).double()\n",
    "#         self.fc4 = nn.Linear(2 * self.clip_out_features, 4, bias=False).double()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x is a tensor of features representing CLIP's image features and\n",
    "#         CLIP's text features concatenated into a tensor with length 1024 (512 + 512)\n",
    "#         \"\"\"\n",
    "\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         x = self.fc3(x)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         x = self.fc4(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_out_features = 512\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * self.clip_out_features, 4 * self.clip_out_features)\n",
    "        self.fc2 = nn.Linear(4 * self.clip_out_features, 2 * self.clip_out_features)\n",
    "        self.fc3 = nn.Linear(2 * self.clip_out_features, self.clip_out_features)\n",
    "        self.fc4 = nn.Linear(self.clip_out_features, self.clip_out_features // 2)\n",
    "        self.fc5 = nn.Linear(self.clip_out_features // 2, self.clip_out_features // 8)\n",
    "        self.fc6 = nn.Linear(self.clip_out_features // 8, 4)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a tensor of features representing CLIP's image features and\n",
    "        CLIP's text features concatenated into a tensor with length 1024 (512 + 512)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoundingBoxModel(nn.Module):\n",
    "    def __init__(self, models, preprocess):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.models = models\n",
    "        self.preprocesses = preprocesses\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 4096)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4096, 512 * 6)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(6 * 512, 4)\n",
    "\n",
    "    def forward(self, indices: torch.Tensor, images: torch.Tensor, prompts) -> torch.Tensor:\n",
    "        self.device = indices.device\n",
    "        if indices.is_cuda:\n",
    "            self.device_index = int(str(self.device)[-1])\n",
    "        else:\n",
    "            self.device_index = 0\n",
    "\n",
    "        model, preprocess = self.models[self.device_index], self.preprocesses[self.device_index]\n",
    "\n",
    "        images = [images[i] for i in indices]\n",
    "        prompts = [prompts[i] for i in indices]\n",
    "\n",
    "        preprocessed_images = torch.stack([\n",
    "            preprocess(image) for image in images\n",
    "        ]).to(self.device)\n",
    "        preprocessed_prompts = torch.cat([\n",
    "            clip.tokenize(prompt) for prompt in prompts\n",
    "        ]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images_features = model.encode_image(preprocessed_images).float()\n",
    "            texts_features = model.encode_text(preprocessed_prompts).float()\n",
    "\n",
    "        # print(f'{images_features.shape}, {texts_features.shape}')\n",
    "\n",
    "        features = torch.cat([images_features, texts_features], dim=1)\n",
    "        x = self.fc1(features)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "boundingbox_model = BoundingBoxModel(models, preprocesses).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    boundingbox_model = torch.nn.DataParallel(boundingbox_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(boundingbox_model.module.models[1].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def iou_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Localization Accuracy Metric\n",
    "\n",
    "    Intersection over Union (IoU) is a common metric measure for localization accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).unsqueeze(0).to(device)\n",
    "    # ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).to(device)\n",
    "\n",
    "    # print(bounding_boxes.shape, ground_truth_bounding_boxes.shape)\n",
    "\n",
    "    return box_iou(bounding_boxes, ground_truth_bounding_boxes)\n",
    "\n",
    "def cosine_similarity_metric(bounding_boxes, ground_truth_bounding_boxes):\n",
    "    \"\"\"\n",
    "    Cosine Similarity Metric\n",
    "\n",
    "    Cosine similarity is a common metric measure for semantic similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_bounding_boxes = torch.tensor(ground_truth_bounding_boxes).to(device)\n",
    "    \n",
    "    return cosine_similarity(bounding_boxes, ground_truth_bounding_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum):\n",
    "  try:\n",
    "    actual_model = model.module\n",
    "  except AttributeError:\n",
    "    actual_model = model\n",
    "\n",
    "  optimizer = torch.optim.SGD([\n",
    "      {'params': actual_model.parameters(), 'lr': lr}\n",
    "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
    "  \n",
    "  return optimizer\n",
    "\n",
    "def get_cost_function():\n",
    "  # cost_function = torch.nn.MSELoss()\n",
    "  cost_function = torchvision.ops.distance_box_iou_loss\n",
    "  return cost_function\n",
    "\n",
    "def training_step_full(net, data_loader, optimizer, cost_function, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  # set the network to training mode\n",
    "  net.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # iterate over the training set\n",
    "  for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "    if batch_idx % 25 == 0:\n",
    "      print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    prompts = [prompt_list[0] for prompt_list in prompts]\n",
    "      \n",
    "    # forward pass\n",
    "    outputs = net(indices, images, prompts)\n",
    "\n",
    "    # full_gt_bounding_boxes = []\n",
    "    # for sample_idx, prompt_list in enumerate(prompts):\n",
    "    #     for _ in range(len(prompt_list)):\n",
    "    #         full_gt_bounding_boxes.append(torch.tensor(gt_bounding_boxes[sample_idx]))\n",
    "    # full_gt_bounding_boxes = torch.stack(full_gt_bounding_boxes).to(device)\n",
    "    gt_bounding_boxes = torch.tensor(gt_bounding_boxes).to(device)\n",
    "\n",
    "    # pred_bounding_boxes_by_image = [[] for _ in range(len(prompts))]\n",
    "    # for sample_idx, prompt_list in enumerate(prompts):\n",
    "    #     for _ in range(len(prompt_list)):\n",
    "    #         pred_bounding_boxes_by_image[sample_idx].append(outputs[sample_idx])\n",
    "    #     pred_bounding_boxes_by_image[sample_idx] = torch.stack(pred_bounding_boxes_by_image[sample_idx])\n",
    "\n",
    "    # loss computation\n",
    "    loss = cost_function(outputs, gt_bounding_boxes, reduction='sum')\n",
    "    print(loss)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # parameters update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # gradients reset\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # fetch prediction and loss value\n",
    "    samples += float(outputs.shape[0])\n",
    "    cumulative_loss += float(loss.item())\n",
    "\n",
    "    # compute accuracy\n",
    "    for output_bbox, gt_bbox in zip(outputs, gt_bounding_boxes):\n",
    "          cumulative_accuracy += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(output_bbox.unsqueeze(0), gt_bbox)]))\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy / samples\n",
    "\n",
    "def test_step_full(net, data_loader, cost_function, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  # set the network to evaluation mode\n",
    "  net.eval() \n",
    "\n",
    "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "  with torch.no_grad():\n",
    "    # iterate over the test set\n",
    "    for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(data_loader):\n",
    "        if batch_idx % 25 == 0:\n",
    "          print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "        indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "        prompts = [prompt_list[0] for prompt_list in prompts]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = net(indices, images, prompts)\n",
    "        \n",
    "        # full_gt_bounding_boxes = []\n",
    "        # for sample_idx, prompt_list in enumerate(prompts):\n",
    "        #     for _ in range(len(prompt_list)):\n",
    "        #         full_gt_bounding_boxes.append(torch.tensor(gt_bounding_boxes[sample_idx]))\n",
    "        # full_gt_bounding_boxes = torch.stack(full_gt_bounding_boxes).to(device)\n",
    "        gt_bounding_boxes = torch.tensor(gt_bounding_boxes).to(device)\n",
    "\n",
    "        # pred_bounding_boxes_by_image = [[] for _ in range(len(prompts))]\n",
    "        # for sample_idx, prompt_list in enumerate(prompts):\n",
    "        #     for _ in range(len(prompt_list)):\n",
    "        #         pred_bounding_boxes_by_image[sample_idx].append(outputs[sample_idx])\n",
    "        #     pred_bounding_boxes_by_image[sample_idx] = torch.stack(pred_bounding_boxes_by_image[sample_idx])\n",
    "\n",
    "        # loss computation\n",
    "        loss = cost_function(outputs, gt_bounding_boxes, reduction='sum')\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        samples += float(outputs.shape[0])\n",
    "        cumulative_loss += float(loss.item())\n",
    "\n",
    "        # compute accuracy\n",
    "        # print(iou_metric(outputs, full_gt_bounding_boxes).shape)\n",
    "        # for gt_bbox, pred_bbox in zip(full_gt_bounding_boxes, outputs):\n",
    "        # for sample_idx, pred_bounding_boxes_for_image in enumerate(pred_bounding_boxes_by_image):\n",
    "        #   cumulative_accuracy += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(pred_bounding_boxes_for_image, gt_bounding_boxes[sample_idx])]))\n",
    "        for output_bbox, gt_bbox in zip(outputs, gt_bounding_boxes):\n",
    "          cumulative_accuracy += np.nansum(np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in iou_metric(output_bbox.unsqueeze(0), gt_bbox)]))\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum):\n",
    "  try:\n",
    "    actual_model = model.module\n",
    "  except AttributeError:\n",
    "    actual_model = model\n",
    "\n",
    "  # optimizer = torch.optim.SGD([\n",
    "  #     {'params': actual_model.parameters(), 'lr': lr}\n",
    "  # ], lr=lr / 100, weight_decay=wd, momentum=momentum)\n",
    "  \n",
    "  optimizer = torch.optim.Adam(actual_model.parameters(), lr=lr)\n",
    "\n",
    "  return optimizer\n",
    "\n",
    "# def iou_loss(pred_bboxes, gt_bboxes):\n",
    "#     sum = torch.tensor([0.0]).to(device)\n",
    "#     for pred, gt in zip(pred_bboxes, gt_bboxes):\n",
    "#         sum += iou_metric(pred.unsqueeze(0), gt).squeeze().to(device)\n",
    "#         # print(iou_metric(pred.unsqueeze(0), gt).squeeze())\n",
    "    \n",
    "#     return 1 - (sum / float(pred_bboxes.shape[0]))\n",
    "\n",
    "def iou_loss(pred_bboxes, gt_bboxes):\n",
    "  x1 = torch.max(pred_bboxes[:, 0], gt_bboxes[:, 0])\n",
    "  y1 = torch.max(pred_bboxes[:, 1], gt_bboxes[:, 1])\n",
    "  x2 = torch.max(pred_bboxes[:, 2], gt_bboxes[:, 2])\n",
    "  y2 = torch.max(pred_bboxes[:, 3], gt_bboxes[:, 3])\n",
    "\n",
    "  intersection_area = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "  predicted_area = (pred_bboxes[:, 2] - pred_bboxes[:, 0]) * (pred_bboxes[:, 3] - pred_bboxes[:, 1])\n",
    "  target_area = (gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (gt_bboxes[:, 3] - gt_bboxes[:, 1])\n",
    "\n",
    "  union_area = predicted_area + target_area - intersection_area\n",
    "\n",
    "  iou_loss = 1.0 - (intersection_area / (union_area + 1e-6))\n",
    "\n",
    "  print(f'iou loss: {iou_loss}')\n",
    "\n",
    "  # mean = torch.clamp(iou_loss.nanmean(), min=0, max=1.0)\n",
    "\n",
    "  # if torch.isnan(mean):\n",
    "  #   return torch.tensor(1e-6)\n",
    "  \n",
    "  return iou_loss.nanmean()\n",
    "\n",
    "def get_cost_function():\n",
    "  cost_function = torch.nn.MSELoss()\n",
    "  # cost_function = iou_loss\n",
    "  return cost_function\n",
    "\n",
    "def training_step_with_embeddings(net, data_loader, optimizer, cost_function, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  # set the network to training mode\n",
    "  net.train()\n",
    "\n",
    "  # iterate over the training set\n",
    "  for batch_idx, (embeddings, gt_bbox) in enumerate(data_loader):\n",
    "    if batch_idx % 25 == 0:\n",
    "      print(f'-- Batch index: {batch_idx}, size: {len(embeddings)} --')\n",
    "\n",
    "    embeddings = embeddings.to(device).float()\n",
    "    gt_bbox = gt_bbox.to(device).float()\n",
    "\n",
    "    # gradients reset\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = net(embeddings)\n",
    "\n",
    "    # loss computation\n",
    "    loss = 0.7 * cost_function(outputs, gt_bbox) + 1.0 * iou_loss(outputs, gt_bbox)#, reduction='sum')\n",
    "    print(outputs.shape, gt_bbox.shape)\n",
    "    print(loss)\n",
    "    \n",
    "    # loss = iou_loss(outputs, gt_bbox)\n",
    "    print(\"Loss only IoU:\")\n",
    "    print(outputs, loss)\n",
    "    # print(f'a: {cost_function(outputs, gt_bbox)}')\n",
    "    # print(f'b: {iou_loss(outputs, gt_bbox)}')\n",
    "\n",
    "\n",
    "    # if batch_idx % 25 == 0:\n",
    "    #   print(f'\\t- Loss: {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # parameters update\n",
    "    optimizer.step()\n",
    "\n",
    "    # fetch prediction and loss value\n",
    "    samples += float(outputs.shape[0])\n",
    "    cumulative_loss += float(loss.item())\n",
    "\n",
    "    # compute accuracy\n",
    "    # cumulative_accuracy += cumula\n",
    "\n",
    "  cumulative_accuracy = cumulative_loss\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy / samples\n",
    "\n",
    "def test_step_with_embeddings(net, data_loader, cost_function, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  IoUs = []\n",
    "  cosine_similarities = []\n",
    "\n",
    "  # set the network to evaluation mode\n",
    "  net.eval() \n",
    "\n",
    "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "  with torch.no_grad():\n",
    "    # iterate over the test set\n",
    "    for batch_idx, (embeddings, gt_bbox) in enumerate(data_loader):\n",
    "        if batch_idx % 25 == 0:\n",
    "          print(f'-- Batch index: {batch_idx}, size: {len(embeddings)} --')\n",
    "          \n",
    "        embeddings = embeddings.to(device).float()\n",
    "        gt_bbox = gt_bbox.to(device).float()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = net(embeddings)\n",
    "\n",
    "        # loss computation\n",
    "        # loss = cost_function(outputs, gt_bbox)#, reduction='sum')\n",
    "        loss = 0.7 * cost_function(outputs, gt_bbox) + 1.0 * iou_loss(outputs, gt_bbox)#, reduction='sum')\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        samples += float(outputs.shape[0])\n",
    "        cumulative_loss += float(loss.item())\n",
    "\n",
    "        for idx, pred_bbox in enumerate(outputs):\n",
    "          IoUs.append(iou_metric(pred_bbox.unsqueeze(0), gt_bbox[idx]))\n",
    "          cosine_similarities.append(cosine_similarity_metric(pred_bbox, gt_bbox[idx]))\n",
    "\n",
    "        # compute accuracy\n",
    "        # cumulative_accuracy += float(predicted.eq(gt_bbox).sum().item())\n",
    "\n",
    "    cumulative_accuracy = cumulative_loss\n",
    "\n",
    "    IoUs_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in IoUs])\n",
    "    mIoU = np.nanmean(IoUs_to_cpu)\n",
    "\n",
    "    cosine_similarities_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in cosine_similarities])\n",
    "    m_cos_sim = np.nanmean(cosine_similarities_to_cpu)\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy / samples, mIoU, m_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_function = get_cost_function()\n",
    "# optimizer = get_optimizer(bbox_model, 0.01, 0.000001, 0.9)\n",
    "\n",
    "# s = 0.0\n",
    "# cl = 0.0\n",
    "\n",
    "# training_step_with_embeddings(bbox_model, train_loader, optimizer, cost_function, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "# main funcition\n",
    "def train(\n",
    "      # dataset_name=\"cifar10\",\n",
    "      # batch_size=128,\n",
    "      # num_classes=10,\n",
    "      model,\n",
    "      train_loader, val_loader, test_loader,\n",
    "      device='cuda:0',\n",
    "      learning_rate=0.01,\n",
    "      weight_decay=0.000001,\n",
    "      momentum=0.9,\n",
    "      epochs=10,\n",
    "    ):\n",
    "  # create a logger for the experiment\n",
    "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "  # get dataloaders\n",
    "  # train_loader, val_loader, test_loader = get_data(\n",
    "  #   dataset_name, transform=preprocess, batch_size=batch_size,\n",
    "  # )\n",
    "  \n",
    "  # instantiate the network and move it to the chosen device (GPU)\n",
    "  net = model\n",
    "  \n",
    "  # instantiate the optimizer\n",
    "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "  \n",
    "  # define the cost function\n",
    "  cost_function = get_cost_function()\n",
    "\n",
    "  # computes evaluation results before training\n",
    "  print('Before training:')\n",
    "  # train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "  # val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "  # test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "  # log to TensorBoard\n",
    "  # log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "  # log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "  # log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "  # print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  # print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  # print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "\n",
    "  print('\\n-- Starting training --')\n",
    "\n",
    "  # for each epoch, train the network and then compute evaluation results\n",
    "  for e in range(epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = training_step_with_embeddings(net, train_loader, optimizer, cost_function, device=device)\n",
    "    val_loss, val_accuracy, val_mIoU, val_m_cos_sim = test_step_with_embeddings(net, val_loader, cost_function, device=device)\n",
    "    # val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "    # logs to TensorBoard\n",
    "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "    print('Epoch: {:d}'.format(e+1))\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}, mean IoU {:.2f}, mean cos sim {:.2f}'.format(val_loss, val_accuracy, val_mIoU, val_m_cos_sim))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "  # compute final evaluation results\n",
    "  print('After training:')\n",
    "  train_loss, train_accuracy, train_mIoU, train_m_cos_sim = test_step_with_embeddings(net, train_loader, cost_function, device=device)\n",
    "  val_loss, val_accuracy, mIoU, m_cos_sim = test_step_with_embeddings(net, val_loader, cost_function, device=device)\n",
    "  test_loss, test_accuracy, test_mIoU, test_m_cos_sim = test_step_with_embeddings(net, test_loader, cost_function=device)\n",
    "\n",
    "  # log to TensorBoard\n",
    "  log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "  log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "  log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}, mean IoU {:.2f}, mean cos sim {:.2f}'.format(test_loss, test_accuracy, test_mIoU, test_m_cos_sim))\n",
    "  print('-----------------------------------------------------')\n",
    "\n",
    "  # closes the logger\n",
    "  writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_model = RegressorModel().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    bbox_model = torch.nn.DataParallel(bbox_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "train(bbox_model, train_loader, val_loader, test_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_step_with_embeddings(bbox_model, test_loader, get_cost_function())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training boundingbox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(boundingbox_model, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(boundingbox_model.state_dict(), 'models/boundingbox_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 2\n",
    "\n",
    "image_test = [dataset_test[sample_idx][0]]\n",
    "prompt_test = [dataset_test[sample_idx][2][0]]\n",
    "gt_bbox_test = torch.tensor([dataset_test[sample_idx][1]]).to(device)\n",
    "indices = torch.tensor(list(range(len(image_test)))).to(device)\n",
    "\n",
    "pre_image = torch.stack([preprocesses[0](image_test[0])]).to(device)\n",
    "pre_prompt = clip.tokenize(prompt_test[0]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = models[0].encode_image(pre_image)\n",
    "    text_features = models[0].encode_text(pre_prompt)\n",
    "\n",
    "features = torch.cat([image_features, text_features], dim=1).double()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_bbox = bbox_model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_bbox_test, pred_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# output_idx = 0\n",
    "\n",
    "# Loading the image\n",
    "img = image_test[0]\n",
    "\n",
    "# Preparing the output\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "# Create a Rectangle patch\n",
    "for bbox, color in zip([gt_bbox_test, pred_bbox], colors):\n",
    "    bounding_box_coordinates = bbox.cpu()[0]\n",
    "    top_left_x, top_left_y = bounding_box_coordinates[0], bounding_box_coordinates[1]\n",
    "    width, height = bounding_box_coordinates[2]- top_left_x, bounding_box_coordinates[3] - top_left_y\n",
    "\n",
    "    # Parameters: (x, y), width, height\n",
    "    rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=1, edgecolor=color, facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.set_title(prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_bbox_test, pred_bbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute average cosine similarity between embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_outputs = []\n",
    "\n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(test_loader):\n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    # images_tensor = torch.stack([transform(image) for image in images]).to(device)\n",
    "    \n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    prompts = [prompt_list[0] for prompt_list in prompts]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = boundingbox_model(indices, images, prompts)\n",
    "\n",
    "    # print(len(outputs))\n",
    "\n",
    "    overall_outputs.append(outputs)\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_outputs[0][0], gt_bounding_boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_cpu = []\n",
    "for out in overall_outputs:\n",
    "    for cos_sim_val in out:\n",
    "        cos_sim_cpu.append(cos_sim_val.item())\n",
    "cos_sim_cpu = np.array(cos_sim_cpu)\n",
    "np.nanmean(cos_sim_cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import boxes as box_ops\n",
    "\n",
    "IoUs = []\n",
    "cosine_similarities = []\n",
    "  \n",
    "for batch_idx, (images, gt_bounding_boxes, prompts) in enumerate(test_loader):\n",
    "    print(f'-- Batch index: {batch_idx} --')\n",
    "\n",
    "    prompts_tensor = [clip.tokenize(prompt_list) for prompt_list in prompts]\n",
    "    \n",
    "    indices = torch.tensor(list(range(len(images)))).to(device)\n",
    "    outputs = baseline_model(indices, images, prompts)\n",
    "\n",
    "    outputs_grouped_by_sample = []\n",
    "    outputs_idx = 0\n",
    "    prompts_idx = 0\n",
    "    while True:\n",
    "        if not prompts_idx < len(images):\n",
    "            break\n",
    "\n",
    "        outputs_grouped_by_sample.append(\n",
    "            outputs[outputs_idx : outputs_idx + len(prompts[prompts_idx])]\n",
    "        )\n",
    "\n",
    "        outputs_idx += len(prompts[prompts_idx])\n",
    "        prompts_idx += 1\n",
    "\n",
    "    for output_bboxes, gt_bboxes in zip(outputs_grouped_by_sample, gt_bounding_boxes):\n",
    "        \"\"\"\n",
    "        There is one output bounding box for each prompt given in input.\n",
    "        Note that each prompt for a given input is actually a list of prompts,\n",
    "        therefore it can contain an arbitrary number of promps. Hence, there is\n",
    "        a bounding box for each one of them.\n",
    "        \"\"\"\n",
    "\n",
    "        result_ious = iou_metric(output_bboxes, gt_bboxes)\n",
    "        result_cosine_similarity = cosine_similarity_metric(output_bboxes, gt_bboxes)\n",
    "\n",
    "        for iou in result_ious:\n",
    "            IoUs.append(iou)\n",
    "\n",
    "        for cs in result_cosine_similarity:\n",
    "            cosine_similarities.append(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoUs_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in IoUs])\n",
    "mIoU = np.nanmean(IoUs_to_cpu)\n",
    "\n",
    "cosine_similarities_to_cpu = np.array([tensor.item() if torch.is_tensor(tensor) else 0 for tensor in cosine_similarities])\n",
    "m_cos_sim = np.nanmean(cosine_similarities_to_cpu)\n",
    "\n",
    "print('--- Metrics ---')\n",
    "print(f'Mean Intersection over Union (mIoU): {mIoU}')\n",
    "print(f'Mean Cosine Similarity: {m_cos_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "output_idx = 0\n",
    "\n",
    "# Loading the image\n",
    "img = images[output_idx]\n",
    "\n",
    "# Preparing the output\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "# Create a Rectangle patch\n",
    "for bbox, color in zip(outputs_grouped_by_sample[output_idx][1:2], colors):\n",
    "    bounding_box_coordinates = bbox.cpu()\n",
    "    top_left_x, top_left_y = bounding_box_coordinates[0], bounding_box_coordinates[1]\n",
    "    width, height = bounding_box_coordinates[2]- top_left_x, bounding_box_coordinates[3] - top_left_y\n",
    "\n",
    "    # Parameters: (x, y), width, height\n",
    "    rect = patches.Rectangle((top_left_x, top_left_y), width, height, linewidth=1, edgecolor=color, facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.set_title(prompts[output_idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
